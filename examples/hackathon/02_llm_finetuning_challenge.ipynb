{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üèÜ Challenge 2: End-to-End LLM Fine-tuning\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê (Advanced) | **Time**: 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By completing this challenge, you will:\n",
    "1. Generate training datasets from clinical data\n",
    "2. Configure and understand LoRA/QLoRA parameters\n",
    "3. Fine-tune an LLM for medical forecasting\n",
    "4. Run inference and evaluate model predictions\n",
    "\n",
    "## ‚ö†Ô∏è Prerequisites\n",
    "- Complete Challenge 1 first!\n",
    "- GPU with at least 30GB memory\n",
    "- Install: `pip install twinweaver[fine-tuning-example]`\n",
    "\n",
    "## üìã Rules\n",
    "- Complete all `# TODO:` sections\n",
    "- Answer quiz questions before proceeding\n",
    "- Make predictions about hyperparameter effects BEFORE running experiments\n",
    "- **No peeking at the original tutorial!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "\n",
    "from twinweaver import (\n",
    "    DataManager,\n",
    "    Config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Decide on Model and Context Length\n",
    "\n",
    "Before starting, you need to make important decisions about your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### ‚ùì Quiz 1: Model Selection\n",
    "\n",
    "**Q1.1**: Why might you choose a smaller model (e.g., Phi-4-mini) over a larger one (e.g., Llama-70B) for this task?\n",
    "\n",
    "**Q1.2**: What is the trade-off between context length and memory usage?\n",
    "\n",
    "**Q1.3**: Why do we use an \"instruction-tuned\" base model instead of a base model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**Your Answers:**\n",
    "\n",
    "Q1.1: \n",
    "\n",
    "Q1.2: \n",
    "\n",
    "Q1.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose your base model\n",
    "# Options to consider:\n",
    "# - \"microsoft/Phi-4-mini-instruct\" (small, fast)\n",
    "# - \"meta-llama/Llama-3.2-3B-Instruct\" (medium)\n",
    "# - \"mistralai/Mistral-7B-Instruct-v0.3\" (larger)\n",
    "\n",
    "BASE_MODEL = None  # Choose your model\n",
    "\n",
    "# TODO: Choose your context length\n",
    "# Consider: Longer = more patient history, but more memory\n",
    "# Reasonable range: 2048 - 16384\n",
    "\n",
    "MAX_CONTEXT_LENGTH = None  # Choose your context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Generate Training Data\n",
    "\n",
    "Using what you learned in Challenge 1, set up the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_events = pd.read_csv(\"../example_data/events.csv\")\n",
    "df_constant = pd.read_csv(\"../example_data/constant.csv\")\n",
    "df_constant_description = pd.read_csv(\"../example_data/constant_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure the pipeline (you did this in Challenge 1!)\n",
    "# Set up:\n",
    "# - split_event_category\n",
    "# - event_category_forecast\n",
    "# - data_splitter_events_variables_category_mapping\n",
    "# - constant_columns_to_use\n",
    "# - constant_birthdate_column\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Your configuration here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize DataManager and all splitters\n",
    "# This should be familiar from Challenge 1\n",
    "\n",
    "dm = DataManager(config=config)\n",
    "# ... complete the setup\n",
    "\n",
    "# Initialize splitters\n",
    "# ...\n",
    "\n",
    "# Initialize converter with YOUR chosen context length\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### üîß Exercise 2.1: Implement Dataset Generator\n",
    "\n",
    "Write a function to generate the training dataset. This is a key skill!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patient IDs for each split\n",
    "training_patientids = dm.get_all_patientids_in_split(config.train_split_name)\n",
    "validation_patientids = dm.get_all_patientids_in_split(config.validation_split_name)\n",
    "\n",
    "print(f\"Training patients: {len(training_patientids)}\")\n",
    "print(f\"Validation patients: {len(validation_patientids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the dataset generation function\n",
    "# This function should:\n",
    "# 1. Iterate through each patient\n",
    "# 2. Get splits for each patient\n",
    "# 3. Convert each split to instruction format\n",
    "# 4. Return a DataFrame with 'prompt' and 'completion' columns\n",
    "\n",
    "\n",
    "def generate_transformers_df(patientids_list):\n",
    "    \"\"\"\n",
    "    Generate training data from a list of patient IDs.\n",
    "\n",
    "    Args:\n",
    "        patientids_list: List of patient IDs to process\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with columns: prompt, completion, patientid\n",
    "    \"\"\"\n",
    "    df = []\n",
    "\n",
    "    # TODO: Implement the function\n",
    "    # HINT: Use dm.get_patient_data(), data_splitter.get_splits_from_patient_with_target(),\n",
    "    #       and converter.forward_conversion()\n",
    "\n",
    "    pass\n",
    "\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets\n",
    "df_train = generate_transformers_df(training_patientids)\n",
    "df_validation = generate_transformers_df(validation_patientids)\n",
    "\n",
    "print(f\"Training examples: {len(df_train)}\")\n",
    "print(f\"Validation examples: {len(df_validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### üèÅ Checkpoint 2.1: Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the generated dataset\n",
    "def validate_dataset(df, name):\n",
    "    errors = []\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        errors.append(f\"‚ùå {name} is empty\")\n",
    "    elif \"prompt\" not in df.columns:\n",
    "        errors.append(f\"‚ùå {name} missing 'prompt' column\")\n",
    "    elif \"completion\" not in df.columns:\n",
    "        errors.append(f\"‚ùå {name} missing 'completion' column\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {name}: {len(df)} examples\")\n",
    "        print(f\"   Avg prompt length: {df['prompt'].str.len().mean():.0f} chars\")\n",
    "        print(f\"   Avg completion length: {df['completion'].str.len().mean():.0f} chars\")\n",
    "        return True\n",
    "\n",
    "    for e in errors:\n",
    "        print(e)\n",
    "    return False\n",
    "\n",
    "\n",
    "train_valid = validate_dataset(df_train, \"Training set\")\n",
    "val_valid = validate_dataset(df_validation, \"Validation set\")\n",
    "\n",
    "if train_valid and val_valid:\n",
    "    print(\"\\nüéâ Datasets ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Tokenizer and Data Formatting\n",
    "\n",
    "LLMs expect data in a specific chat format. Let's set this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the tokenizer for your chosen model\n",
    "tokenizer = None  # Load tokenizer\n",
    "\n",
    "# TODO: Set the padding token\n",
    "# HINT: A common approach is to use the EOS token as the padding token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### ‚ùì Quiz 2: Chat Templates\n",
    "\n",
    "**Q2.1**: What is a \"chat template\" and why do instruction-tuned models need it?\n",
    "\n",
    "**Q2.2**: What roles are typically used in a chat format?\n",
    "\n",
    "**Q2.3**: Why do we set `completion_only_loss=True` during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "**Your Answers:**\n",
    "\n",
    "Q2.1: \n",
    "\n",
    "Q2.2: \n",
    "\n",
    "Q2.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the chat formatting function\n",
    "# Convert raw prompt/completion to chat message format\n",
    "\n",
    "\n",
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Convert a single example to chat format.\n",
    "\n",
    "    Args:\n",
    "        example: Dict with 'prompt' and 'completion' keys\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'prompt' as list of user messages and 'completion' as list of assistant messages\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    # HINT: Return format should be:\n",
    "    # {\"prompt\": [{\"role\": \"user\", \"content\": ...}],\n",
    "    #  \"completion\": [{\"role\": \"assistant\", \"content\": ...}]}\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace datasets and apply formatting\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "validation_dataset = Dataset.from_pandas(df_validation)\n",
    "\n",
    "train_dataset = train_dataset.map(format_chat_template)\n",
    "validation_dataset = validation_dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Configure Quantization (QLoRA)\n",
    "\n",
    "Quantization allows training large models on limited hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### ‚ùì Quiz 3: Quantization Understanding\n",
    "\n",
    "**Q3.1**: What does \"4-bit quantization\" mean? What are we quantizing?\n",
    "\n",
    "**Q3.2**: What is the trade-off between quantization level (4-bit vs 8-bit vs full precision)?\n",
    "\n",
    "**Q3.3**: What does \"nf4\" (NormalFloat4) quantization type do differently than regular int4?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**Your Answers:**\n",
    "\n",
    "Q3.1: \n",
    "\n",
    "Q3.2: \n",
    "\n",
    "Q3.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure 4-bit quantization\n",
    "# Parameters to set:\n",
    "# - load_in_4bit: Enable 4-bit loading\n",
    "# - bnb_4bit_quant_type: Use \"nf4\" for better quality\n",
    "# - bnb_4bit_compute_dtype: Use torch.bfloat16 for modern GPUs\n",
    "# - bnb_4bit_use_double_quant: Enable for additional memory savings\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # TODO: Fill in the parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Configure LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) enables efficient fine-tuning by training only a small number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### üß™ Experiment 5.1: LoRA Hyperparameter Impact\n",
    "\n",
    "Before configuring, make predictions about how these hyperparameters affect training:\n",
    "\n",
    "| Parameter | Your Prediction: What happens if we INCREASE it? |\n",
    "|-----------|---------------------------------------------------|\n",
    "| `r` (rank) | |\n",
    "| `lora_alpha` | |\n",
    "| `lora_dropout` | |\n",
    "| Number of target modules | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "**Your Predictions:**\n",
    "\n",
    "- `r` (rank): \n",
    "- `lora_alpha`: \n",
    "- `lora_dropout`: \n",
    "- Number of target modules: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure LoRA\n",
    "# Make deliberate choices for each parameter and justify them!\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    # TODO: Set lora_alpha (scaling factor, common values: 8, 16, 32)\n",
    "    lora_alpha=None,\n",
    "    # TODO: Set lora_dropout (regularization, common values: 0.05-0.2)\n",
    "    lora_dropout=None,\n",
    "    # TODO: Set r (rank - higher = more parameters, common values: 4, 8, 16, 32)\n",
    "    r=None,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # TODO: Choose which modules to target\n",
    "    # Options:\n",
    "    # - Minimal: [\"q_proj\", \"v_proj\"] - faster but less expressive\n",
    "    # - Full attention: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    # - All linear: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    target_modules=None,\n",
    ")\n",
    "\n",
    "# Document your reasoning:\n",
    "print(\"My LoRA configuration choices:\")\n",
    "print(f\"  r={peft_config.r}: [Your reasoning here]\")\n",
    "print(f\"  lora_alpha={peft_config.lora_alpha}: [Your reasoning here]\")\n",
    "print(f\"  target_modules={peft_config.target_modules}: [Your reasoning here]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Training Configuration\n",
    "\n",
    "Configure the training hyperparameters. Each choice matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### üß™ Experiment 6.1: Learning Rate Analysis\n",
    "\n",
    "**Q6.1**: The tutorial uses `learning_rate=1e-4`. This is higher than typical full fine-tuning (1e-5 to 5e-5) we have used. Why might PEFT methods benefit from higher learning rates?\n",
    "\n",
    "**Q6.2**: What problems might you see if the learning rate is:\n",
    "- Too high?\n",
    "- Too low?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "**Your Answers:**\n",
    "\n",
    "Q6.1: \n",
    "\n",
    "Q6.2 (too high): \n",
    "\n",
    "Q6.2 (too low): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure training arguments\n",
    "# Think carefully about each parameter!\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"./results_challenge\",\n",
    "    # TODO: Set number of training epochs (consider: small dataset = more epochs OK)\n",
    "    num_train_epochs=None,\n",
    "    # TODO: Set batch size (limited by GPU memory)\n",
    "    per_device_train_batch_size=None,\n",
    "    # TODO: Set gradient accumulation (effective_batch = batch_size * grad_accum)\n",
    "    gradient_accumulation_steps=None,\n",
    "    # Optimizer settings\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    # Logging and evaluation\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    per_device_eval_batch_size=1,\n",
    "    # TODO: Set learning rate (PEFT typically uses higher LR than full fine-tuning)\n",
    "    learning_rate=None,\n",
    "    # Precision settings\n",
    "    fp16=False,  # Set True for older GPUs (V100, T4)\n",
    "    bf16=True,  # Set True for newer GPUs (A100, 3090, 4090)\n",
    "    # Regularization\n",
    "    max_grad_norm=1.0,\n",
    "    # TODO: Set warmup ratio (what fraction of training for warmup?)\n",
    "    warmup_ratio=None,\n",
    "    group_by_length=True,\n",
    "    save_total_limit=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_length=MAX_CONTEXT_LENGTH,\n",
    "    packing=False,\n",
    "    completion_only_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Load Model and Train\n",
    "\n",
    "Time to put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the base model with quantization\n",
    "# Parameters needed:\n",
    "# - Model name (BASE_MODEL)\n",
    "# - quantization_config (bnb_config)\n",
    "# - device_map=\"auto\"\n",
    "# - trust_remote_code=False\n",
    "\n",
    "model = None  # Load the model\n",
    "\n",
    "# Don't forget: Disable cache for training\n",
    "# model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the SFTTrainer\n",
    "# Parameters needed:\n",
    "# - model\n",
    "# - train_dataset\n",
    "# - processing_class (tokenizer)\n",
    "# - args (training_arguments)\n",
    "# - eval_dataset (validation_dataset)\n",
    "# - peft_config\n",
    "\n",
    "trainer = None  # Create trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### üß™ Experiment 7.1: Training Observation\n",
    "\n",
    "Before running training, predict what you expect to see:\n",
    "\n",
    "**Q7.1**: What should happen to the training loss over time?\n",
    "\n",
    "**Q7.2**: What might it mean if validation loss increases while training loss decreases?\n",
    "\n",
    "**Q7.3**: How long do you expect training to take? (Make a guess!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "**Your Predictions:**\n",
    "\n",
    "Q7.1: \n",
    "\n",
    "Q7.2: \n",
    "\n",
    "Q7.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training!\n",
    "# Note: This takes ~5 minutes on a good GPU\n",
    "# Watch the training loss and validation loss as it runs\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### üìä Post-Training Analysis\n",
    "\n",
    "After training completes, analyze what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze the training results\n",
    "# Questions to answer:\n",
    "# 1. What was the final training loss?\n",
    "# 2. What was the final validation loss?\n",
    "# 3. Did validation loss ever increase? (sign of overfitting)\n",
    "# 4. Did your predictions match reality?\n",
    "\n",
    "print(\"Training Analysis:\")\n",
    "print(\"==================\")\n",
    "# Your analysis here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adapter\n",
    "adapter_path = \"./results_challenge/final_adapter\"\n",
    "trainer.save_model(adapter_path)\n",
    "print(f\"Adapter saved to {adapter_path}\")\n",
    "\n",
    "# Clean up\n",
    "del trainer\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Inference Challenge\n",
    "\n",
    "Now let's test the fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test patient\n",
    "test_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0]\n",
    "patient_data = dm.get_patient_data(test_patientid)\n",
    "\n",
    "# Get the date of first line of therapy\n",
    "df_events_patient = patient_data[\"events\"].copy()\n",
    "date_of_first_lot = df_events_patient.loc[\n",
    "    df_events_patient[\"event_category\"] == config.event_category_lot, \"date\"\n",
    "].min()\n",
    "\n",
    "print(f\"Test patient: {test_patientid}\")\n",
    "print(f\"First LoT date: {date_of_first_lot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### üîß Exercise 8.1: Design Your Prediction Task\n",
    "\n",
    "Choose what you want to predict for this patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design your forecasting task\n",
    "# What variable do you want to forecast? At what time points?\n",
    "\n",
    "# Example structure:\n",
    "# forecasting_times_to_predict = {\n",
    "#     \"variable_name\": [week1, week2, week3, ...]\n",
    "# }\n",
    "\n",
    "forecasting_times_to_predict = {\n",
    "    # TODO: Fill in - choose a lab value and time points\n",
    "}\n",
    "\n",
    "# Get inference splits\n",
    "forecast_split, events_split = data_splitter.get_splits_from_patient_inference(\n",
    "    patient_data,\n",
    "    inference_type=\"both\",\n",
    "    # TODO: Set the variable(s) to predict\n",
    "    forecasting_override_variables_to_predict=None,  # List of variable names\n",
    "    # TODO: Set the event to predict\n",
    "    events_override_category=None,  # e.g., \"death\"\n",
    "    events_override_observation_time_delta=pd.Timedelta(days=52 * 7),  # 1 year\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert to instruction format for inference\n",
    "# Use converter.forward_conversion_inference()\n",
    "\n",
    "converted = None  # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the prompt to see what we're asking the model\n",
    "print(\"=\" * 50)\n",
    "print(\"INFERENCE PROMPT:\")\n",
    "print(\"=\" * 50)\n",
    "print(converted[\"instruction\"][:2000])  # First 2000 chars\n",
    "print(\"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### üß™ Experiment 8.2: Prediction Before Running\n",
    "\n",
    "Based on the patient's history in the prompt, make your own predictions:\n",
    "\n",
    "**Q8.1**: What do you predict for the forecasted values?\n",
    "\n",
    "**Q8.2**: What do you predict for the time-to-event?\n",
    "\n",
    "**Q8.3**: How confident are you in these predictions? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "**Your Predictions:**\n",
    "\n",
    "Q8.1: \n",
    "\n",
    "Q8.2: \n",
    "\n",
    "Q8.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the base model and adapter for inference\n",
    "\n",
    "# 1. Load base model with quantization\n",
    "base_model_inference = None  # Your code\n",
    "\n",
    "# 2. Load the saved adapter\n",
    "inference_model = None  # Use PeftModel.from_pretrained()\n",
    "\n",
    "# 3. Set to evaluation mode\n",
    "# inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create text generation pipeline\n",
    "inference_model.config.use_cache = True\n",
    "\n",
    "text_gen_pipeline = None  # Create pipeline(\"text-generation\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate prediction\n",
    "# Use the pipeline with appropriate generation parameters:\n",
    "# - max_new_tokens: 128 is usually enough\n",
    "# - return_full_text: False (we only want the generated part)\n",
    "# - do_sample: True (for nucleus sampling)\n",
    "# - temperature: 0.7 (controls randomness)\n",
    "# - top_p: 0.9 (nucleus sampling threshold)\n",
    "\n",
    "generated_answer = None  # Your generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the generated answer\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL PREDICTION:\")\n",
    "print(\"=\" * 50)\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reverse convert to structured data\n",
    "return_list = None  # Use converter.reverse_conversion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display structured results\n",
    "for i, result in enumerate(return_list):\n",
    "    print(f\"\\nTask {i + 1}:\")\n",
    "    print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### üìä Final Analysis\n",
    "\n",
    "Compare the model's predictions to your own and reflect on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your analysis\n",
    "# 1. How did the model's predictions compare to yours?\n",
    "# 2. Are the predictions clinically reasonable?\n",
    "# 3. What improvements would you suggest?\n",
    "\n",
    "print(\"\"\"\n",
    "Final Analysis:\n",
    "===============\n",
    "\n",
    "1. Comparison to my predictions:\n",
    "   [Your analysis here]\n",
    "\n",
    "2. Clinical reasonableness:\n",
    "   [Your analysis here]\n",
    "\n",
    "3. Suggested improvements:\n",
    "   [Your analysis here]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "---\n",
    "## üåü Bonus Challenge 1: Hyperparameter Experiment\n",
    "\n",
    "**+20 points**\n",
    "\n",
    "Train two more models with different LoRA configurations:\n",
    "1. Low rank (r=4) with minimal target modules\n",
    "2. High rank (r=32) with all linear modules\n",
    "\n",
    "Compare their performance on the same test patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Implement your hyperparameter experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "---\n",
    "## üåü Bonus Challenge 2: Multi-Sample Inference\n",
    "\n",
    "**+15 points**\n",
    "\n",
    "Generate multiple predictions (N=5) for the same patient using different random seeds. Analyze the variance in predictions. What does this tell you about model confidence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Implement multi-sample inference and analyze variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "---\n",
    "## üåü Bonus Challenge 3: Evaluation Framework\n",
    "\n",
    "**+25 points**\n",
    "\n",
    "Build an evaluation framework that:\n",
    "1. Runs inference on all test patients\n",
    "2. Compares predictions to ground truth\n",
    "3. Computes metrics (MAE for forecasting, accuracy for events)\n",
    "4. Generates a summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Implement the evaluation framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "---\n",
    "## üèÜ Challenge Complete!\n",
    "\n",
    "Congratulations on completing the advanced challenge! You've learned to:\n",
    "\n",
    "- ‚úÖ Generate training datasets from clinical data\n",
    "- ‚úÖ Configure quantization for memory-efficient training\n",
    "- ‚úÖ Design and justify LoRA hyperparameter choices\n",
    "- ‚úÖ Fine-tune an LLM for medical forecasting\n",
    "- ‚úÖ Run inference and analyze predictions\n",
    "- ‚úÖ Convert model outputs back to structured data\n",
    "\n",
    "### üìù Reflection Questions\n",
    "\n",
    "Take a moment to reflect on what you learned:\n",
    "\n",
    "1. What was the most challenging part of this challenge?\n",
    "2. What would you do differently if you had more compute resources?\n",
    "3. How would you adapt this pipeline for a different clinical task?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
