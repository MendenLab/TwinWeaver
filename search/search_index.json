{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"TwinWeaver","text":"<p>TwinWeaver is a longitudinal framework for LLM-based Patient Digital Twins. It serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models (LLMs). This framework transforms structured patient history\u2014including demographics, labs, treatments, and genetics\u2014into a single, human-readable text prompt, enabling LLMs to jointly forecast continuous biomarkers and predict discrete clinical events.</p>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li> <p> Installation</p> <p>Install TwinWeaver with pip in seconds</p> <p> Installation Guide</p> </li> <li> <p> Tutorials</p> <p>Step-by-step notebooks to learn TwinWeaver</p> <p> View Tutorials</p> </li> <li> <p> Quick Start</p> <p>Minimal code example for experienced users</p> <p> Quick Start</p> </li> <li> <p> Dataset Format</p> <p>Understand the expected data structure</p> <p> Dataset Format</p> </li> <li> <p> Pro Tips</p> <p>Debugging, scaling, and optimization advice</p> <p> Pro Tips</p> </li> <li> <p> API Index</p> <p>Quick reference to all classes and functions</p> <p> API Index</p> </li> </ul>"},{"location":"#why-twinweaver","title":"Why TwinWeaver?","text":"<p>TwinWeaver addresses the challenge of modeling sparse, multi-modal clinical time series by leveraging the generative capabilities of LLMs:</p> <ul> <li>Text Serialization: Transforms multi-modal inputs into structured textual representations</li> <li>Unified Tasks: Supports both time-series forecasting and landmark event prediction</li> <li>Flexible Horizons: Avoids overfitting to specific canonical time points</li> <li>MEDS Integration: Easily integrate existing MEDS datasets</li> </ul> <p> Learn more about the framework</p>"},{"location":"#featured-genie-digital-twin-gdt","title":"Featured: Genie Digital Twin (GDT)","text":"<p>GDT is a pan-cancer model instantiated using TwinWeaver, trained on over 93,000 patients across 20 cancer types. It achieves a median MASE of 0.87 for forecasting and an average C-index of 0.703 for risk stratification.</p> <p> GDT Repository</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use TwinWeaver in your research, please cite our paper: <pre><code>@misc{makarov2026twinweaver,\n      title={TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins},\n      author={Nikita Makarov and Maria Bordukova and Lena Voith von Voithenberg and Estrella Pivel-Villanueva and Sabrina Mielke and Jonathan Wickes and Hanchen Wang and Mingyu Derek Ma and Keunwoo Choi and Kyunghyun Cho and Stephen Ra and Raul Rodriguez-Esteban and Fabian Schmich and Michael Menden},\n      year={2026},\n      eprint={2601.20906},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2601.20906},\n}\n</code></pre></p>"},{"location":"api-index/","title":"API Index","text":"<p>This page provides a complete reference to all classes, methods, and functions available in TwinWeaver. Click on any item to view its detailed documentation.</p>"},{"location":"api-index/#quick-import","title":"Quick Import","text":"<p>All main components can be imported directly from <code>twinweaver</code>:</p> <pre><code>from twinweaver import (\n    # Core\n    Config,\n    DataManager,\n\n    # Instruction Tuning\n    ConverterInstruction,\n    DataSplitter,\n    DataSplitterForecasting,\n    DataSplitterEvents,\n\n    # Pretraining\n    ConverterPretrain,\n\n    # Utilities\n    convert_meds_to_dtc,\n    identify_constant_and_changing_columns,\n    aggregate_events_to_weeks,\n)\n</code></pre>"},{"location":"api-index/#common-module","title":"Common Module","text":""},{"location":"api-index/#config","title":"Config","text":"<p>Configuration manager for TwinWeaver settings.</p> Member Type Description <code>Config</code> Class Centralized configuration repository <code>Config.set_delta_time_unit</code> Method Set the time unit for delta calculations <code>Config.seed</code> Property Random seed for reproducibility"},{"location":"api-index/#datamanager","title":"DataManager","text":"<p>Handles data loading and management.</p> Member Type Description <code>DataManager</code> Class Manages data loading, processing, and splitting <code>DataManager.load_indication_data</code> Method Load data tables for a specific indication <code>DataManager.process_indication_data</code> Method Process loaded indication data <code>DataManager.setup_unique_mapping_of_events</code> Method Create unique mapping for all events <code>DataManager.setup_dataset_splits</code> Method Split data into train/val/test sets <code>DataManager.get_all_patientids_in_split</code> Method Get all patient IDs in a specific split <code>DataManager.get_patient_split</code> Method Get the split assignment for a patient <code>DataManager.get_patient_data</code> Method Retrieve all data for a specific patient <code>DataManager.infer_var_types</code> Method Infer variable types (numeric/categorical)"},{"location":"api-index/#converterbase","title":"ConverterBase","text":"<p>Base class for all converters.</p> Member Type Description <code>ConverterBase</code> Class Base class for data-to-text conversion <code>round_and_strip</code> Function Format numbers with precision control <code>ConverterBase.get_difference_in_event_dataframes</code> Method Compare two event DataFrames <code>ConverterBase.forward_conversion_inference</code> Method Abstract method for inference conversion <code>ConverterBase.generate_target_manual</code> Method Abstract method to generate target text <code>ConverterBase.aggregate_multiple_responses</code> Method Abstract method to aggregate responses"},{"location":"api-index/#instruction-module","title":"Instruction Module","text":""},{"location":"api-index/#converterinstruction","title":"ConverterInstruction","text":"<p>Main converter for instruction-tuning data.</p> Member Type Description <code>ConverterInstruction</code> Class Converter combining forecasting and events <code>ConverterInstruction.set_custom_summarized_row_fn</code> Method Set custom function for row summarization <code>ConverterInstruction.get_nr_tokens</code> Method Count tokens in a string <code>ConverterInstruction.forward_conversion</code> Method Convert patient data to text (training) <code>ConverterInstruction.forward_conversion_inference</code> Method Convert patient data to text (inference) <code>ConverterInstruction.generate_target_manual</code> Method Generate target text from reverse conversion <code>ConverterInstruction.aggregate_multiple_responses</code> Method Aggregate multiple LLM responses <code>ConverterInstruction.reverse_conversion</code> Method Convert text back to structured data <code>ConverterInstruction.get_difference_in_event_dataframes</code> Method Compare predicted vs actual events"},{"location":"api-index/#converterforecasting","title":"ConverterForecasting","text":"<p>Converter for forecasting tasks.</p> Member Type Description <code>ConverterForecasting</code> Class Converter for time-series forecasting <code>ConverterForecasting.forward_conversion</code> Method Convert patient split to text (training) <code>ConverterForecasting.forward_conversion_inference</code> Method Convert patient split to text (inference) <code>ConverterForecasting.generate_target_manual</code> Method Generate forecasting target text <code>ConverterForecasting.aggregate_multiple_responses</code> Method Aggregate multiple forecasting responses <code>ConverterForecasting.reverse_conversion</code> Method Parse text back to structured forecasts"},{"location":"api-index/#converterforecastingqa","title":"ConverterForecastingQA","text":"<p>Converter for forecasting with Q&amp;A format.</p> Member Type Description <code>ConverterForecastingQA</code> Class Q&amp;A format forecasting converter <code>ConverterForecastingQA.forward_conversion</code> Method Convert to Q&amp;A format text"},{"location":"api-index/#converterevents","title":"ConverterEvents","text":"<p>Converter for event prediction tasks.</p> Member Type Description <code>ConverterEvents</code> Class Converter for clinical event prediction <code>ConverterEvents.forward_conversion</code> Method Convert patient split to text (training) <code>ConverterEvents.forward_conversion_inference</code> Method Convert patient split to text (inference) <code>ConverterEvents.generate_target_manual</code> Method Generate event prediction target <code>ConverterEvents.reverse_conversion</code> Method Parse text back to event predictions <code>ConverterEvents.get_difference_in_event_dataframes</code> Method Compare predicted vs actual events <code>ConverterEvents.aggregate_multiple_responses</code> Method Aggregate multiple event responses"},{"location":"api-index/#datasplitter","title":"DataSplitter","text":"<p>Main data splitter combining forecasting and events.</p> Member Type Description <code>DataSplitter</code> Class Combined data splitter <code>DataSplitter.get_splits_from_patient_with_target</code> Method Get splits with target data (training) <code>DataSplitter.get_splits_from_patient_inference</code> Method Get splits for inference"},{"location":"api-index/#datasplitterforecasting","title":"DataSplitterForecasting","text":"<p>Splitter for forecasting data.</p> Member Type Description <code>DataSplitterForecasting</code> Class Forecasting-specific data splitter <code>DataSplitterForecasting.setup_statistics</code> Method Compute statistics for normalization <code>DataSplitterForecasting.get_splits_from_patient</code> Method Generate splits for a patient <code>DataSplitterForecastingOption</code> Class Configuration for forecasting splits <code>DataSplitterForecastingGroup</code> Class Grouping for forecasting options <code>DataSplitterForecastingGroup.append</code> Method Add option to group"},{"location":"api-index/#datasplitterevents","title":"DataSplitterEvents","text":"<p>Splitter for event data.</p> Member Type Description <code>DataSplitterEvents</code> Class Event-specific data splitter <code>DataSplitterEvents.setup_variables</code> Method Setup event variables <code>DataSplitterEvents.get_splits_from_patient</code> Method Generate splits for a patient <code>DataSplitterEventsOption</code> Class Configuration for event splits <code>DataSplitterEventsGroup</code> Class Grouping for event options <code>DataSplitterEventsGroup.append</code> Method Add option to group"},{"location":"api-index/#basedatasplitter","title":"BaseDataSplitter","text":"<p>Base class for all data splitters.</p> Member Type Description <code>BaseDataSplitter</code> Class Abstract base for splitters <code>BaseDataSplitter.select_random_splits</code> Method Randomly select split points <code>BaseDataSplitter.drop_duplicates_except_na_for_date_col</code> Method Remove duplicates preserving NA dates"},{"location":"api-index/#pretrain-module","title":"Pretrain Module","text":""},{"location":"api-index/#converterpretrain","title":"ConverterPretrain","text":"<p>Converter for pretraining data.</p> Member Type Description <code>ConverterPretrain</code> Class Converter for pretraining data preparation <code>ConverterPretrain.forward_conversion</code> Method Convert patient data to pretraining text <code>ConverterPretrain.reverse_conversion</code> Method Parse pretraining text back to data"},{"location":"api-index/#utils-module","title":"Utils Module","text":"<p>Utility functions for data preprocessing and integration.</p> Name Type Description <code>convert_meds_to_dtc</code> Function Convert MEDS format data to TwinWeaver format <code>identify_constant_and_changing_columns</code> Function Identify constant vs. time-varying columns <code>aggregate_events_to_weeks</code> Function Aggregate event data to weekly intervals"},{"location":"api-index/#module-reference","title":"Module Reference","text":"<p>For complete module documentation with full parameter details, see the API Reference section in the navigation.</p> Module Description <code>twinweaver.common</code> Core configuration and base classes <code>twinweaver.instruction</code> Instruction-tuning converters and splitters <code>twinweaver.pretrain</code> Pretraining data converters <code>twinweaver.utils</code> Utility functions and integrations"},{"location":"citation/","title":"Citation","text":""},{"location":"citation/#paper","title":"Paper","text":"<p>The paper can be found on Arxiv.</p>"},{"location":"citation/#authors","title":"Authors","text":"<p>The core authors are: Nikita Makarov, Maria Bordukova, Lena Voith von Voithenberg, Estrella Villanueva Pivel, Sabrina Mielke, Jonathan Wickes, Hanchen Wang, Derek Ma, Keunwoo Choi, Kyunghyun Cho, Stephen Ra, Raul Rodriguez-Esteban, Fabian Schmich, Michael Menden</p>"},{"location":"citation/#bibtex","title":"BibTeX","text":"<p>If you use the package, please cite:</p> <pre><code>@misc{makarov2026twinweaver,\n      title={TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins},\n      author={Nikita Makarov and Maria Bordukova and Lena Voith von Voithenberg and Estrella Pivel-Villanueva and Sabrina Mielke and Jonathan Wickes and Hanchen Wang and Mingyu Derek Ma and Keunwoo Choi and Kyunghyun Cho and Stephen Ra and Raul Rodriguez-Esteban and Fabian Schmich and Michael Menden},\n      year={2026},\n      eprint={2601.20906},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2601.20906},\n}\n</code></pre>"},{"location":"citation/#logo","title":"Logo","text":"<p>The logo was generated with Nano Banana Pro.</p>"},{"location":"citation/#license","title":"License","text":"<p>TwinWeaver is licensed under the Apache License 2.0. See LICENSE for details.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to TwinWeaver! Please follow these steps to contribute.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository and install dependencies:</p> <pre><code>git clone https://github.com/MendenLab/TwinWeaver\ncd twinweaver\npip install -e .\npip install -r examples/requirements.txt\npip install pre-commit pytest pytest-cov\npip install -r docs/requirements.txt\n</code></pre> </li> <li> <p>Install pre-commit hooks:</p> <p>We use <code>pre-commit</code> to ensure code formatting and quality checks run before you commit.</p> <pre><code>pre-commit install\n</code></pre> </li> </ol>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>We use <code>pytest</code> for testing. To run the full test suite:</p> <pre><code>pytest tests/\n</code></pre> <p>With coverage reporting:</p> <pre><code>pytest tests/ --cov=twinweaver --cov-report=html\n</code></pre>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<p>The documentation is built with <code>mkdocs</code>. To preview it locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then open http://127.0.0.1:8000 in your browser.</p>"},{"location":"contributing/#contribution-workflow","title":"Contribution Workflow","text":"<ol> <li> <p>Create a New Branch: Always create a new branch for your feature or fix.</p> <pre><code>git checkout -b feature/my-new-feature\n</code></pre> </li> <li> <p>Make Changes: Implement your feature or fix.</p> </li> <li> <p>Run Tests &amp; Linting: Ensure your code passes all tests and pre-commit hooks.</p> <pre><code>pytest tests/\npre-commit run --all-files\n</code></pre> </li> <li> <p>Submit a Merge Request:</p> <ul> <li>Push your branch to the repository.</li> <li>Open a Merge Request (Pull Request) against the <code>main</code> branch.</li> <li>Describe your changes clearly in the MR description.</li> </ul> </li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>We follow PEP 8 for Python code style</li> <li>Use type hints where appropriate</li> <li>Write docstrings in NumPy format</li> <li>Add tests for new functionality</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you find a bug or have a feature request, please open an issue on GitHub.</p>"},{"location":"dataset-format/","title":"Dataset Format","text":"<p>TwinWeaver expects three primary dataframes (or CSV files) as input. Example files can be found in <code>examples/example_data/</code>.</p>"},{"location":"dataset-format/#1-longitudinal-events-eventscsv","title":"1. Longitudinal Events (<code>events.csv</code>)","text":"<p>Contains time-varying clinical data where each row represents a single event.</p> Column Description <code>patientid</code> Unique identifier for the patient <code>date</code> Date of the event (processable by pandas.to_datetime) <code>event_descriptive_name</code> Human-readable name used in the text output <code>event_category</code> (Optional) Category (e.g., <code>lab</code>, <code>drug</code>), used for determining splits &amp; tasks <code>event_name</code> (Optional) Specific event identifier <code>event_value</code> Value associated with the event <code>meta_data</code> (Optional) Additional metadata <code>source</code> (Optional) Modality of data - default to \"events\", alternatively \"genetic\" <p>Example:</p> <pre><code>patientid,date,event_descriptive_name,event_category,event_name,event_value,meta_data,source\npatient_001,2024-01-15,Hemoglobin,lab,HGB,12.5,,clinical\npatient_001,2024-01-15,White Blood Cells,lab,WBC,7.2,,clinical\npatient_001,2024-02-01,Chemotherapy Started,treatment,CHEMO,1,,clinical\n</code></pre>"},{"location":"dataset-format/#2-patient-constants-constantcsv","title":"2. Patient Constants (<code>constant.csv</code>)","text":"<p>Contains static patient information (demographics, baseline characteristics). One row per patient.</p> Column Description <code>patientid</code> Unique identifier for the patient <code>birthyear</code> (example) Patient's year of birth <code>gender</code> (example) Patient's gender <code>...</code> Any other static patient attributes <p>Example:</p> <pre><code>patientid,birthyear,gender,diagnosis_stage\npatient_001,1965,Female,Stage II\npatient_002,1978,Male,Stage III\n</code></pre>"},{"location":"dataset-format/#3-constant-descriptions-constant_descriptioncsv","title":"3. Constant Descriptions (<code>constant_description.csv</code>)","text":"<p>Maps columns in the <code>constant</code> table to human-readable descriptions for the text prompt.</p> Column Description <code>variable</code> Name of the column in the constant table <code>comment</code> Description of the variable for the text prompt <p>Example:</p> <pre><code>variable,comment\nbirthyear,Year of birth\ngender,Patient gender\ndiagnosis_stage,Cancer stage at diagnosis\n</code></pre>"},{"location":"dataset-format/#conceptual-overview-of-text-transformation","title":"Conceptual Overview of Text Transformation","text":"<p>The <code>Converter</code> transforms the structured data splits into natural language:</p>"},{"location":"dataset-format/#constants-demographics-text","title":"Constants \u2192 Demographics Text","text":"<p>Static patient information is converted into readable sentences:</p> Input (DataFrame) Output (Text) <code>birthyear: 1965</code> \"Year of birth is 1965\" <code>gender: Female</code> \"Patient gender is female\""},{"location":"dataset-format/#events-temporal-narrative","title":"Events \u2192 Temporal Narrative","text":"<p>Longitudinal events are organized by visit date and converted to natural language with relative time references:</p> <p>Input (Events DataFrame): <pre><code>date        | event_descriptive_name | event_value\n2024-01-15  | Hemoglobin            | 12.5\n2024-01-15  | White Blood Cells     | 7.2\n2024-01-29  | Hemoglobin            | 11.8\n</code></pre></p> <p>Output (Text): <pre><code>On the first visit, the patient experienced the following:\n    Hemoglobin is 12.5,\n    White Blood Cells is 7.2.\n\n2 weeks later, the patient visited and experienced the following:\n    Hemoglobin is 11.8.\n</code></pre></p>"},{"location":"dataset-format/#final-output-structure","title":"Final Output Structure","text":"<p>For training, TwinWeaver produces input-target pairs:</p> <pre><code>Input:\n    [Preamble explaining data structure]\n    [Demographics section]\n    [Chronological event narrative]\n    [Task-specific prompt]\n\nTarget:\n    [Expected model response - predicted values or outcomes]\n</code></pre> <p>For inference, only the input portion is generated, and the model produces the target predictions.</p>"},{"location":"dataset-format/#best-practices-for-data-processing","title":"Best Practices for Data Processing","text":"<p>When transforming raw clinical data into TwinWeaver format, following these principles will help you get the most out of your data.</p>"},{"location":"dataset-format/#1-prefer-events-over-constants","title":"1. Prefer Events Over Constants","text":"<p>Key principle: Put as much data as possible into the events table. Only truly immutable patient characteristics should go into constants.</p> <p>Even data that appears \"constant\" is often better represented as events because:</p> <ul> <li>It has a specific date when it was measured (e.g., biomarker test date)</li> <li>It could change over time (e.g., acquired resistance mutations, re-staging)</li> <li>Temporal context matters clinically (when was this information known?)</li> </ul> <p>Examples: | Data Type | Recommended Table | Rationale | |-----------|------------------|-----------| | Birth year, biological sex | <code>constant</code> | Truly immutable | | Biomarker results (EGFR, ALK, PD-L1) | <code>events</code> | Has test date, could change | | Cancer stage | <code>events</code> | Stage at diagnosis date, may be re-staged | | Diagnosis information | <code>events</code> | Occurred at a specific date | | Lab values, vitals | <code>events</code> | Longitudinal measurements | | Treatment administrations | <code>events</code> | Time-varying interventions | | Death, progression | <code>events</code> | Time-to-event outcomes |</p>"},{"location":"dataset-format/#2-include-all-available-data-first","title":"2. Include All Available Data First","text":"<p>Start by including everything, then trim during data generation if needed:</p> <ul> <li>The <code>ConverterInstruction</code> token budget automatically controls output length</li> <li>The framework prioritizes recent and relevant events</li> <li>You can always exclude data later, but you can't include what wasn't captured</li> </ul>"},{"location":"dataset-format/#3-use-consistent-event-naming","title":"3. Use Consistent Event Naming","text":"<p>Standardize your event names and categories:</p> <pre><code># Good: Consistent naming convention\nevent_name = \"hemoglobin_-_718-7\"  # Includes LOINC code for clarity\nevent_descriptive_name = \"hemoglobin - 718-7\"  # Human-readable version\n\n# Avoid: Inconsistent naming\nevent_name = \"Hgb\"  # One record\nevent_name = \"hemoglobin\"  # Another record\nevent_name = \"HGB\"  # Yet another\n</code></pre>"},{"location":"dataset-format/#4-structure-event-categories-meaningfully","title":"4. Structure Event Categories Meaningfully","text":"<p>Choose event categories that align with your modeling objectives:</p> Category Description Example Events <code>lab</code> Laboratory test results hemoglobin, platelets, creatinine <code>drug</code> Drug administrations pembrolizumab, carboplatin <code>lot</code> Line of therapy markers treatment start, line number <code>death</code> Mortality events death <code>response</code> Treatment response RECIST response, progression <code>staging</code> Cancer staging stage, TNM classification <code>basic_biomarker</code> Molecular markers EGFR, ALK, KRAS"},{"location":"dataset-format/#5-use-preprocessing-helper-functions","title":"5. Use Preprocessing Helper Functions","text":"<p>TwinWeaver provides helper functions to analyze and prepare your data:</p> <pre><code>from twinweaver import (\n    identify_constant_and_changing_columns,\n    aggregate_events_to_weeks,\n)\n\n# Identify which columns are truly constant vs. changing over time\nconstant_cols, changing_cols = identify_constant_and_changing_columns(\n    df, date_column=\"visit_date\", patientid_column=\"patient_id\"\n)\n\n# Aggregate frequent measurements to reduce noise\ndf_aggregated = aggregate_events_to_weeks(\n    df_events,\n    patientid_column=\"patientid\",\n    date_column=\"date\",\n    event_name_column=\"event_name\",\n    event_value_column=\"event_value\",\n)\n</code></pre>"},{"location":"dataset-format/#6-validate-your-data-before-training","title":"6. Validate Your Data Before Training","text":"<p>Always validate your data format before proceeding:</p> <pre><code>def validate_twinweaver_format(df_events, df_constant, df_constant_description):\n    \"\"\"Validate that dataframes conform to TwinWeaver requirements.\"\"\"\n    issues = []\n\n    # Check required columns\n    events_required = [\"patientid\", \"date\", \"event_category\", \"event_name\",\n                       \"event_value\", \"event_descriptive_name\"]\n    for col in events_required:\n        if col not in df_events.columns:\n            issues.append(f\"df_events missing column: {col}\")\n\n    # Check patient ID consistency\n    events_patients = set(df_events[\"patientid\"].unique())\n    constant_patients = set(df_constant[\"patientid\"].unique())\n    if events_patients != constant_patients:\n        issues.append(\"Patient IDs don't match between events and constants\")\n\n    return len(issues) == 0, issues\n</code></pre>"},{"location":"dataset-format/#7-handle-time-to-event-outcomes-properly","title":"7. Handle Time-to-Event Outcomes Properly","text":"<p>Death, progression, and other time-to-event outcomes should be represented as events with a specific date:</p> <pre><code># Death event\n{\n    \"patientid\": \"PT001\",\n    \"date\": \"2021-02-10\",  # Date of death\n    \"event_category\": \"death\",\n    \"event_name\": \"death\",\n    \"event_value\": \"Yes\",\n    \"event_descriptive_name\": \"Death\",\n}\n</code></pre> <p>Censored Patients</p> <p>For patients who are alive (censored), simply don't include a death event. The absence of a death event indicates the patient was alive at last follow-up.</p>"},{"location":"dataset-format/#loading-data","title":"Loading Data","text":"<p>Data can be loaded as pandas DataFrames directly:</p> <pre><code>import pandas as pd\nfrom twinweaver import DataManager, Config\n\n# Load your data\ndf_events = pd.read_csv(\"events.csv\")\ndf_constant = pd.read_csv(\"constant.csv\")\ndf_constant_description = pd.read_csv(\"constant_description.csv\")\n\n# Initialize configuration\nconfig = Config()\n\n# &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt;\n# 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot')\n# Has to be set for all instruction tasks\nconfig.split_event_category = \"lot\"\n\n# 2. List of event categories we want to forecast (e.g., forecasting 'lab' values)\n# Only needs to be set if you want to forecast variables\nconfig.event_category_forecast = [\"lab\"]\n\n# 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression')\n# Only needs to be set if you want to do time to event prediction\nconfig.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",  # Custom name in prompt\n}\n\n# Initialize DataManager\ndm = DataManager(config=config)\ndm.load_indication_data(\n    df_events=df_events,\n    df_constant=df_constant,\n    df_constant_description=df_constant_description\n)\n</code></pre> <p>Configuration Parameters</p> <ul> <li><code>split_event_category</code>: The event category used to anchor split points for generating training samples (required for instruction tuning)</li> <li><code>event_category_forecast</code>: Which event categories to forecast as time-series values</li> <li><code>data_splitter_events_variables_category_mapping</code>: Maps event names to prediction tasks (e.g., survival, progression)</li> </ul> <p>See the Raw Data Preprocessing Tutorial for transforming raw clinical data into TwinWeaver format, or the Data Preparation Tutorial for a complete walkthrough of instruction-tuning data generation.</p>"},{"location":"framework/","title":"Framework Overview","text":"<p>TwinWeaver addresses the challenge of modeling sparse, multi-modal clinical time series by leveraging the generative capabilities of LLMs.</p>"},{"location":"framework/#core-components","title":"Core Components","text":""},{"location":"framework/#1-text-serialization","title":"1. Text Serialization","text":"<p>Transforms multi-modal inputs (diagnoses, laboratory measurements, genetic mutation panels) into a structured textual representation of longitudinal patient trajectories.</p>"},{"location":"framework/#2-unified-task-support","title":"2. Unified Task Support","text":"<p>TwinWeaver provides a unified framework for multiple clinical prediction tasks:</p> <ul> <li>Time-Series Forecasting: Forecasting frequently measured values such as blood biomarkers or vital signs.</li> <li>Landmark Event Prediction: Predicting patient event status (e.g., survival, disease progression) at future time points using a landmarking framework.</li> </ul>"},{"location":"framework/#3-flexible-horizon","title":"3. Flexible Horizon","text":"<p>Supports sampling split times and prediction horizons to avoid overfitting to specific canonical time points.</p>"},{"location":"framework/#dataset-types-instruction-vs-pretraining","title":"Dataset Types: Instruction vs. Pretraining","text":"<p>TwinWeaver supports two primary data formats, each serving a distinct stage in the model training pipeline:</p>"},{"location":"framework/#pretraining-data","title":"Pretraining Data","text":"Aspect Details Purpose Continued Pretraining (CPT) to adapt a general-purpose LLM to the clinical domain Format Narrative-style serialization of the entire patient history Goal Enables the model to learn medical terminology, clinical relationships, and temporal dynamics in an unsupervised manner (next-token prediction) Converter <code>twinweaver.pretrain.converter_manual_template.ConverterPretrain</code>"},{"location":"framework/#instruction-data","title":"Instruction Data","text":"Aspect Details Purpose Supervised Fine-Tuning (SFT) to teach the model to perform specific clinical tasks Format Structured into \"Input\" (Prompt) and \"Target\" (Completion) pairs Goal Optimizes the model for specific downstream applications like forecasting and risk stratification Converter <code>twinweaver.instruction.converter_manual_instruction.ConverterInstruction</code> <p>Input/Target Structure:</p> <ul> <li>Input: Patient history up to a specific time point + a list of specific questions (e.g., \"Forecast the next 3 weeks of hemoglobin values\")</li> <li>Target: The ground truth answers to those questions</li> </ul> <p>Configuration Required</p> <p>Instruction tuning requires explicit configuration of splitting and prediction variables:</p> <ul> <li><code>config.split_event_category</code>: Event category used to anchor split points (e.g., <code>\"lot\"</code> for line of therapy)</li> <li><code>config.event_category_forecast</code>: List of event categories to forecast (e.g., <code>[\"lab\"]</code>)</li> <li><code>config.data_splitter_events_variables_category_mapping</code>: Mapping of events to prediction tasks (e.g., death, progression)</li> </ul> <p>See the Quick Start or Data Preparation Tutorial for examples.</p>"},{"location":"framework/#genie-digital-twin-gdt","title":"Genie Digital Twin (GDT)","text":"<p>Note</p> <p>The specific implementation, training, and evaluation code for the GDT model mentioned in the TwinWeaver paper is located in MendenLab/GDT. This repository contains the core <code>twinweaver</code> framework.</p> <p>GDT is a pan-cancer model instantiated using TwinWeaver, trained on over 93,000 patients across 20 cancer types.</p>"},{"location":"framework/#performance","title":"Performance","text":"<p>GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for strong time-series baselines. Furthermore, it improves risk stratification, achieving an average C-index of 0.703 across survival, progression, and therapy switching tasks. GDT also demonstrates capabilities in zero-shot generalization to out-of-distribution clinical trials and supports an interpretable clinical reasoning extension.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>Core dependencies: <code>pandas</code>, <code>numpy</code>, <code>transformers</code>, <code>scikit-learn</code></li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from PyPi","text":"<p>To install the package:</p> <pre><code>pip install twinweaver\n</code></pre>"},{"location":"installation/#install-with-fine-tuning-support","title":"Install with Fine-Tuning Support","text":"<p>For running the end-to-end fine-tuning example, install with additional dependencies:</p> <pre><code>pip install twinweaver[fine-tuning-example]\n</code></pre> <p>Note</p> <p>The torch CUDA version might need to be adapted to your system.</p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute or modify TwinWeaver:</p> <pre><code>git clone https://github.com/MendenLab/TwinWeaver\ncd twinweaver\npip install -e .\npip install -r examples/requirements.txt\npip install pre-commit pytest pytest-cov\npip install -r docs/requirements.txt\n</code></pre> <p>See the Contributing guide for more details.</p>"},{"location":"pro-tips/","title":"Pro Tips","text":"<p>This page contains practical advice for debugging, scaling, and optimizing your TwinWeaver workflows.</p>"},{"location":"pro-tips/#debugging-development","title":"Debugging &amp; Development","text":""},{"location":"pro-tips/#use-a-subset-of-patients","title":"Use a Subset of Patients","text":"<p>When debugging or developing new features, use a subset of patients rather than limiting the number of events. This approach:</p> <ul> <li>Maintains realistic patient trajectories with complete event sequences</li> <li>Reduces data loading and processing time significantly</li> <li>Helps identify issues with edge cases in patient histories</li> </ul> <pre><code># Example: Filter to a subset of patients for debugging\npatient_subset = df[\"patientid\"].unique()[:100]  # First 100 patients\ndf_debug = df[df[\"patientid\"].isin(patient_subset)]\n</code></pre>"},{"location":"pro-tips/#use-simpler-interface-libraries","title":"Use Simpler Interface Libraries","text":"<p>For debugging and prototyping, use Hugging Face Transformers directly for both training and inference:</p> <ul> <li>Easier to debug with clear error messages</li> <li>More flexible for experimentation</li> <li>Well-documented with extensive community support</li> </ul>"},{"location":"pro-tips/#scaling-up","title":"Scaling Up","text":""},{"location":"pro-tips/#enable-flash-attention","title":"Enable Flash Attention","text":"<p>For larger-scale training and inference, ensure Flash Attention is enabled for significant memory and speed improvements:</p> <p>Requirements</p> <p>Flash Attention requires compatible hardware (Ampere GPUs or newer) and the <code>flash-attn</code> package: <pre><code>pip install flash-attn --no-build-isolation\n</code></pre></p>"},{"location":"pro-tips/#use-specialized-deployment-libraries","title":"Use Specialized Deployment Libraries","text":"<p>For production-scale inference, consider using vLLM with prefix caching enabled:</p> <ul> <li>vLLM: High-throughput inference with optimized memory management</li> <li>Prefix Caching: Reuses computed KV cache for shared prompt prefixes, ideal for patient history prompts</li> <li>Enable further parallelization via OpenAI server: By launching as a separate server instance, this allows requests to run more parallel.</li> </ul> <p>GDT Examples</p> <p>For real-world examples of vLLM deployment with prefix caching, see the GDT (Genie Digital Twin) repository.</p>"},{"location":"pro-tips/#experiment-tracking-with-weights-biases","title":"Experiment Tracking with Weights &amp; Biases","text":"<p>Use Weights &amp; Biases (W&amp;B) for comprehensive experiment tracking at scale:</p> <ul> <li>Track hyperparameters, metrics, and model artifacts</li> <li>Compare runs and identify optimal configurations</li> <li>Collaborate with team members on experiment analysis</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>This page provides a minimal code example to get you started with TwinWeaver. For detailed explanations, see the Tutorials.</p> <p>Recommended Path</p> <p>If you're new to TwinWeaver and have raw clinical data, start with the Raw Data Preprocessing Tutorial to learn how to transform your data into TwinWeaver format. Then proceed to the Data Preparation Tutorial for instruction-tuning data generation.</p>"},{"location":"quickstart/#minimal-example","title":"Minimal Example","text":"<pre><code>import pandas as pd\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n    DataSplitterForecasting,\n    DataSplitterEvents,\n    ConverterInstruction,\n    DataSplitter,\n)\n\n# Initialize configuration\nconfig = Config()\n\n# &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt;\n# 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot')\n# Has to be set for all instruction tasks\nconfig.split_event_category = \"lot\"\n\n# 2. List of event categories we want to forecast (e.g., forecasting 'lab' values)\n# Only needs to be set if you want to forecast variables\nconfig.event_category_forecast = [\"lab\"]\n\n# 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression')\n# Only needs to be set if you want to do time to event prediction\nconfig.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",  # Custom name in prompt\n}\n\n# Load your patient data\n# Assuming your data is in df_events, df_constant, df_constant_description\ndm = DataManager(config=config)\ndm.load_indication_data(\n    df_events=df_events,\n    df_constant=df_constant,\n    df_constant_description=df_constant_description\n)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\ndm.infer_var_types()\n\n# Set up data splitters for different task types\n# Event prediction tasks (e.g., survival, progression)\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\n\n# Forecasting tasks (e.g., biomarker prediction)\ndata_splitter_forecasting = DataSplitterForecasting(\n    data_manager=dm,\n    config=config,\n)\n\n# Combined interface for both task types\ndata_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n\n# Set up the text converter\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config,\n    dm=dm,\n    variable_stats=data_splitter_forecasting.variable_stats,\n)\n\n# Get data for a specific patient\npatient_data = dm.get_patient_data(\"patient_id_0\")  # Set your patient id\n\n# Generate splits with targets\nforecasting_splits, events_splits, reference_dates = \\\n    data_splitter.get_splits_from_patient_with_target(patient_data)\n\n# Convert to training format\nsplit_idx = 0  # Use first split\ntraining_data = converter.forward_conversion(\n    forecasting_splits=forecasting_splits[split_idx],\n    event_splits=events_splits[split_idx],\n    override_mode_to_select_forecasting=\"both\",\n)\n\n# training_data now contains (Input, Target) pairs ready for LLM fine-tuning\nprint(\"Input prompt:\", training_data[\"input\"][:500], \"...\")\nprint(\"Target:\", training_data[\"target\"])\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Tutorials: Detailed notebooks with step-by-step explanations</li> <li>Dataset Format: Understand the expected data structure</li> <li>Framework Overview: Learn about TwinWeaver's architecture</li> <li>API Reference: Full API documentation</li> </ul>"},{"location":"tutorials/","title":"Tutorials &amp; Examples","text":"<p>The <code>examples/</code> directory provides comprehensive tutorials to help you get up and running with TwinWeaver.</p>"},{"location":"tutorials/#core-tutorials","title":"\ud83d\udd30 Core Tutorials","text":"<p>These notebooks cover the primary workflows for most users:</p>"},{"location":"tutorials/#0-raw-data-preprocessing","title":"0. Raw Data Preprocessing","text":"<p><code>examples/data_preprocessing/raw_data_preprocessing.ipynb</code></p> <p>Start here if you have raw clinical data. This tutorial demonstrates how to transform raw clinical data (e.g., EHR exports, clinical trial databases) into the standardized TwinWeaver format.</p> <p>What you'll learn:</p> <ul> <li>Creating the three required TwinWeaver dataframes (<code>df_events</code>, <code>df_constant</code>, <code>df_constant_description</code>)</li> <li>Best practices for deciding what goes into events vs. constants</li> <li>Handling time-to-event outcomes like death and progression</li> <li>Using preprocessing helpers for data aggregation and column classification</li> <li>Validating your data format before training</li> </ul>"},{"location":"tutorials/#1-data-preparation-for-training","title":"1. Data Preparation for Training","text":"<p><code>examples/01_data_preparation_for_training.ipynb</code></p> <p>Demonstrates how to convert raw patient data (events, constants, genetics) into the instruction-tuning text format used by TwinWeaver. This is the core step for preparing data for fine-tuning.</p> <p>What you'll learn:</p> <ul> <li>Loading patient data into the DataManager</li> <li>Configuring variables and data splits</li> <li>Converting data to text format for LLM training</li> </ul>"},{"location":"tutorials/#2-inference-prompt-preparation","title":"2. Inference Prompt Preparation","text":"<p><code>examples/02_inference_prompt_preparation.ipynb</code></p> <p>Shows how to run inference using the TwinWeaver framework, including setting up the data manager and generating prompts.</p> <p>What you'll learn:</p> <ul> <li>Setting up prompts for inference</li> <li>Generating predictions with trained models</li> <li>Processing model outputs</li> </ul>"},{"location":"tutorials/#3-end-to-end-llm-fine-tuning","title":"3. End-to-End LLM Fine-Tuning","text":"<p><code>examples/03_end_to_end_llm_finetuning.ipynb</code></p> <p>A complete guide covering the entire pipeline from data ingestion to LLM fine-tuning.</p> <p>Installation Note</p> <p>Please install the packages required via the exact following line: <pre><code>pip install twinweaver[fine-tuning-example]\n</code></pre> The torch CUDA version might need to be adapted to your system.</p>"},{"location":"tutorials/#advanced-usage","title":"\ud83d\ude80 Advanced Usage","text":"<p>For users needing custom behavior or specific integrations:</p>"},{"location":"tutorials/#pretraining-data-conversion","title":"Pretraining Data Conversion","text":"<p><code>examples/advanced/pretraining/prepare_pretraining_data.py</code></p> <p>A script illustrating how to convert data for the pretraining phase, using template-based generation. Useful if you want to pretrain on your own large-scale unlabeled clinical data.</p>"},{"location":"tutorials/#end-to-end-llm-training-with-pretraining-data","title":"End-to-End LLM Training with Pretraining Data","text":"<p><code>examples/advanced/pretraining/end_to_end_llm_training_with_pretrain.ipynb</code></p> <p>A complete notebook demonstrating how to train LLMs on full patient histories without a specific task. This approach can be used to develop models that generate synthetic patients or embeddings.</p> <p>Installation Note</p> <p>Please install the packages required via: <pre><code>pip install twinweaver[fine-tuning-example]\n</code></pre> Requires a GPU with at least 30GB of memory.</p>"},{"location":"tutorials/#custom-splitting","title":"Custom Splitting","text":"<ul> <li>Inference: <code>examples/advanced/custom_splitting/inference_individual_splitters.py</code> \u2014 Example script for inference using individual splitters.</li> <li>Training: <code>examples/advanced/custom_splitting/training_individual_splitters.ipynb</code> \u2014 Notebook demonstrating training data generation with individual splitters.</li> <li>Custom Split Events: <code>examples/advanced/custom_splitting/training_custom_split_events.ipynb</code> \u2014 Notebook showing how to customize split events and forecast different event categories (e.g., using genetic events as split points and forecasting vitals).</li> </ul>"},{"location":"tutorials/#custom-text-generation","title":"Custom Text Generation","text":"<p><code>examples/advanced/custom_output/customizing_text_generation.ipynb</code></p> <p>A comprehensive tutorial on customizing every textual component of the instruction generation pipeline. TwinWeaver provides extensive configuration options to tailor generated text prompts to your specific use case.</p> <p>What you'll learn:</p> <ul> <li>Customizing preamble and introduction text</li> <li>Modifying demographics section formatting</li> <li>Changing event day and time interval descriptions</li> <li>Switching time units between days and weeks</li> <li>Customizing genetic data tags and placeholder text</li> <li>Modifying forecasting, time-to-event, and QA task prompts</li> <li>Configuring multi-task instruction formatting</li> <li>Fine-grained control over specific event categories with overrides</li> </ul>"},{"location":"tutorials/#integrations","title":"\ud83d\udd17 Integrations","text":""},{"location":"tutorials/#meds-data-import","title":"MEDS Data Import","text":"<p><code>examples/integrations/meds_data_import.ipynb</code></p> <p>A tutorial on importing data in the Medical Event Data Standard (MEDS) format and converting it into TwinWeaver's internal format. Includes a synthetic data example.</p>"},{"location":"examples/","title":"TwinWeaver Examples","text":"<p>This directory contains examples demonstrating how to use TwinWeaver for various tasks including data preparation, inference, and fine-tuning.</p>"},{"location":"examples/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>data_preprocessing/raw_data_preprocessing.ipynb: Start here if you have raw clinical data. Shows how to transform raw EHR exports into the three TwinWeaver dataframes (<code>df_events</code>, <code>df_constant</code>, <code>df_constant_description</code>), including handling death events and other time-to-event outcomes.</li> </ul>"},{"location":"examples/#basic-examples","title":"Basic Examples","text":"<ul> <li>01_data_preparation_for_training.ipynb: A basic example showing how to convert data for a single patient using the instruction setup with a custom dataset.</li> <li>02_inference_prompt_preparation.ipynb: Demonstrates how to run inference using TwinWeaver.</li> <li>03_end_to_end_llm_finetuning.ipynb: A comprehensive end-to-end guide on fine-tuning an LLM for medical forecasting. It covers data processing, QLoRA fine-tuning, and inference.</li> </ul>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":"<p>Located in the <code>advanced/</code> directory, these examples cover more specific use cases.</p>"},{"location":"examples/#custom-splitting-advancedcustom_splitting","title":"Custom Splitting (<code>advanced/custom_splitting/</code>)","text":"<ul> <li>training_individual_splitters.ipynb: Demonstrates data preparation using individual data splitters for more granular control.</li> <li>inference_individual_splitters.py: A Python script showing how to run inference using the individual splitter setup.</li> </ul>"},{"location":"examples/#pretraining-advancedpretraining","title":"Pretraining (<code>advanced/pretraining/</code>)","text":"<ul> <li>prepare_pretraining_data.py: A script to prepare data for the pretraining phase.</li> <li>end_to_end_llm_training_with_pretrain.ipynb: An end-to-end example for training LLMs on full patient histories without a specific task, useful for developing models that can generate synthetic patients or embeddings.</li> </ul>"},{"location":"examples/#integrations","title":"Integrations","text":"<p>Located in the <code>integrations/</code> directory.</p> <ul> <li>meds_data_import.ipynb: Shows how to import and work with data in the MEDS format.</li> </ul>"},{"location":"examples/#data","title":"Data","text":"<ul> <li><code>example_data/</code>: Contains the generator script and sample CSV files (events, constants, etc.) used by the examples.</li> </ul>"},{"location":"examples/01_data_preparation_for_training/","title":"Example for converting patient data to instruction data","text":"<p>This notebook demonstrates the core workflow for converting raw clinical data into Instruction Tuning examples for the TwinWeaver model.</p> <p>We will walk through the process of:</p> <ol> <li>Loading Data: Importing raw tabular data (longitudinal events and static demographics).</li> <li>Configuration: Setting up the pipeline to match your data schema.</li> <li>Splitting: Generating \"Splits\" (input/output samples) from a patient's timeline.</li> <li>Conversion: Transforming these splits into text-based Instruction (Input) and Answer (Target) pairs suitable for fine-tuning an LLM.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n    DataSplitterForecasting,\n    DataSplitterEvents,\n    ConverterInstruction,\n    DataSplitter,\n)\n</pre> import pandas as pd  from twinweaver import (     DataManager,     Config,     DataSplitterForecasting,     DataSplitterEvents,     ConverterInstruction,     DataSplitter, ) In\u00a0[\u00a0]: Copied! <pre># Load data - generated example data\ndf_events = pd.read_csv(\"./example_data/events.csv\")\ndf_constant = pd.read_csv(\"./example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"./example_data/constant_description.csv\")\n</pre> # Load data - generated example data df_events = pd.read_csv(\"./example_data/events.csv\") df_constant = pd.read_csv(\"./example_data/constant.csv\") df_constant_description = pd.read_csv(\"./example_data/constant_description.csv\") In\u00a0[\u00a0]: Copied! <pre>config = Config()\n\n# &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt;\n# 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot')\n# Has to be set for all instruction tasks\nconfig.split_event_category = \"lot\"\n\n# 2. List of event categories we want to forecast (e.g., forecasting 'lab' values)\n# Only needs to be set if you want to forecast variables\nconfig.event_category_forecast = [\"lab\"]\n\n# 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression')\n# Only needs to be set if you want to do time to event prediction\nconfig.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\"\n}\n</pre> config = Config()  # &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt; # 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot') # Has to be set for all instruction tasks config.split_event_category = \"lot\"  # 2. List of event categories we want to forecast (e.g., forecasting 'lab' values) # Only needs to be set if you want to forecast variables config.event_category_forecast = [\"lab\"]  # 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression') # Only needs to be set if you want to do time to event prediction config.data_splitter_events_variables_category_mapping = {     \"death\": \"death\",     \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\" } <p>You can also customize the static configuration (demographics). This allows you to control which constant variables are included in the generated instructions.</p> In\u00a0[\u00a0]: Copied! <pre># &lt;----------------------- OPTIONAL CONFIGURATION ----------------------&gt;\n# Define which static/demographic columns to include in the prompt\nconfig.constant_columns_to_use = [\n    \"birthyear\",\n    \"gender\",\n    \"histology\",\n    \"smoking_history\",\n]\n\n# Specify the column representing birth year/date for age calculation\nconfig.constant_birthdate_column = \"birthyear\"\n</pre> # &lt;----------------------- OPTIONAL CONFIGURATION ----------------------&gt; # Define which static/demographic columns to include in the prompt config.constant_columns_to_use = [     \"birthyear\",     \"gender\",     \"histology\",     \"smoking_history\", ]  # Specify the column representing birth year/date for age calculation config.constant_birthdate_column = \"birthyear\" <p>The <code>DataManager</code> then ingests the raw dataframes, handling preprocessing steps like date parsing, unique event mapping, and train/test splitting at the patient level.</p> In\u00a0[\u00a0]: Copied! <pre>dm = DataManager(config=config)\n# Load in the data\ndm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\n# Perform initial processing (e.g. date parsing)\ndm.process_indication_data()\n# Setup unique mapping of events, to understand which events correspond to which categories\ndm.setup_unique_mapping_of_events()\n# (Optional) assign each patient to train/validation/test splits\ndm.setup_dataset_splits()\n# (Optional - needed for forecasting) infer variable types\ndm.infer_var_types()\n</pre> dm = DataManager(config=config) # Load in the data dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description) # Perform initial processing (e.g. date parsing) dm.process_indication_data() # Setup unique mapping of events, to understand which events correspond to which categories dm.setup_unique_mapping_of_events() # (Optional) assign each patient to train/validation/test splits dm.setup_dataset_splits() # (Optional - needed for forecasting) infer variable types dm.infer_var_types() In\u00a0[\u00a0]: Copied! <pre># This data splitter handles event prediction tasks\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\n\n# This data splitter handles forecasting tasks\ndata_splitter_forecasting = DataSplitterForecasting(\n    data_manager=dm,\n    config=config,\n)\n# If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step\ndata_splitter_forecasting.setup_statistics()\n\n# We will also use the easier interface that combines both data splitters\ndata_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n\n# Set up the converter instruction\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config,\n    dm=dm,\n    variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks\n)\n</pre> # This data splitter handles event prediction tasks data_splitter_events = DataSplitterEvents(dm, config=config) data_splitter_events.setup_variables()  # This data splitter handles forecasting tasks data_splitter_forecasting = DataSplitterForecasting(     data_manager=dm,     config=config, ) # If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step data_splitter_forecasting.setup_statistics()  # We will also use the easier interface that combines both data splitters data_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)  # Set up the converter instruction converter = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config,     dm=dm,     variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks ) <p>From the data manager we can get the patient, for example this patientid.</p> In\u00a0[\u00a0]: Copied! <pre>patientid = dm.all_patientids[4]\npatientid\n</pre> patientid = dm.all_patientids[4] patientid <p>Let's checkout the data of the patient. <code>patient_data</code> is a dictionary containing the patient's data, with two keys:</p> <ul> <li><code>\"events\"</code>: A pandas DataFrame containing all time-series events (original events and molecular data combined and sorted by date).</li> <li><code>\"constant\"</code>: A pandas DataFrame containing the static (constant) data for the patient.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>patient_data = dm.get_patient_data(patientid)\npatient_data[\"events\"].head(20)\n</pre> patient_data = dm.get_patient_data(patientid) patient_data[\"events\"].head(20) In\u00a0[\u00a0]: Copied! <pre>patient_data[\"constant\"]\n</pre> patient_data[\"constant\"] In\u00a0[\u00a0]: Copied! <pre>forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(\n    patient_data,\n)\n</pre> forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(     patient_data, ) <p>Now for each split, we can generate these strings. We just pick the first one as an example.</p> In\u00a0[\u00a0]: Copied! <pre>split_idx = 0\np_converted = converter.forward_conversion(\n    forecasting_splits=forecasting_splits[split_idx],\n    event_splits=events_splits[split_idx],\n    override_mode_to_select_forecasting=\"both\",\n)\n</pre> split_idx = 0 p_converted = converter.forward_conversion(     forecasting_splits=forecasting_splits[split_idx],     event_splits=events_splits[split_idx],     override_mode_to_select_forecasting=\"both\", ) In\u00a0[\u00a0]: Copied! <pre>print(p_converted[\"instruction\"])\n</pre> print(p_converted[\"instruction\"]) In\u00a0[\u00a0]: Copied! <pre>print(p_converted[\"answer\"])\n</pre> print(p_converted[\"answer\"]) In\u00a0[\u00a0]: Copied! <pre>p_converted[\"answer\"]\n</pre> p_converted[\"answer\"] In\u00a0[\u00a0]: Copied! <pre>date = reference_dates[\"date\"][0]\nreturn_list = converter.reverse_conversion(p_converted[\"answer\"], dm, date)\nreturn_list[0][\"result\"]\n</pre> date = reference_dates[\"date\"][0] return_list = converter.reverse_conversion(p_converted[\"answer\"], dm, date) return_list[0][\"result\"]"},{"location":"examples/01_data_preparation_for_training/#example-for-converting-patient-data-to-instruction-data","title":"Example for converting patient data to instruction data\u00b6","text":""},{"location":"examples/01_data_preparation_for_training/#basic-setup","title":"Basic Setup\u00b6","text":""},{"location":"examples/01_data_preparation_for_training/#load-data","title":"Load Data\u00b6","text":"<p>We require three standardized dataframes to construct the patient digital twin:</p> <ul> <li>Events (<code>df_events</code>): The longitudinal history in 'long' format (one row per event). Required columns:<ul> <li><code>patientid</code>: Unique patient identifier.</li> <li><code>date</code>: Date of the event.</li> <li><code>event_category</code>: High-level grouping (e.g., 'lab', 'drug', 'condition', 'lot').</li> <li><code>event_name</code>: Specific variable name (e.g., 'Hemoglobin', 'Metformin').</li> <li><code>event_value</code>: The result/value (e.g., '12.5', 'Start').</li> <li><code>event_descriptive_name</code>: Natural language description used in the text prompt.</li> </ul> </li> <li>Constant (<code>df_constant</code>): Static patient information (one row per patient). Contains demographics like birth year, gender, and histology.</li> <li>Constant Description (<code>df_constant_description</code>): Metadata mapping constant columns to natural language descriptions. Columns: <code>variable</code>, <code>comment</code>.</li> </ul>"},{"location":"examples/01_data_preparation_for_training/#configuration-and-data-manager","title":"Configuration and Data Manager\u00b6","text":"<p>We initialize the <code>Config</code> object, which serves as the central control for settings such as column mapping, token limits, and prompt templates. While defaults are provided, you can override them to align with your specific dataset schema (e.g., selecting which demographic columns in <code>df_constant</code> to utilize).</p> <p>Key Configuration Parameters:</p> <ul> <li><code>config.split_event_category</code>: Determines the event category used to split patient histories into input and output segments. In this example, we split data around \"Line of Therapy\" (<code>lot</code>) start dates.</li> <li><code>config.event_category_forecast</code>: Identifies which event categories should be forecasted as time-series values. Here, we target <code>lab</code> values.</li> <li><code>config.data_splitter_events_variables_category_mapping</code>: Maps specific events to survival analysis or classification tasks. We configure the model to predict outcomes for <code>death</code> and <code>progression</code>.</li> </ul>"},{"location":"examples/01_data_preparation_for_training/#initialize-splitters-and-converter","title":"Initialize Splitters and Converter\u00b6","text":"<p>To generate diverse training examples, we use specialized Data Splitters:</p> <ul> <li><code>DataSplitterEvents</code>: Identifies time points for predicting discrete outcomes (e.g., progression, death).</li> <li><code>DataSplitterForecasting</code>: Identifies time points for forecasting continuous variables (e.g., future lab values).</li> </ul> <p>The <code>ConverterInstruction</code> is the core engine that transforms these data points into tokenized text. It respects a token budget (e.g., 8192 tokens) to ensure the generated prompts fit within the model's context window.</p>"},{"location":"examples/01_data_preparation_for_training/#examine-patient-data","title":"Examine patient data\u00b6","text":""},{"location":"examples/01_data_preparation_for_training/#convert-patient-data-to-string","title":"Convert patient data to string\u00b6","text":""},{"location":"examples/01_data_preparation_for_training/#generate-training-splits","title":"Generate Training Splits\u00b6","text":"<p>A single patient's timeline can yield multiple training examples. A Split represents a specific point in time (the \"split date\") where we divide the data:</p> <ul> <li>Input: History before the split date.</li> <li>Target: Future events or values after the split date.</li> </ul> <p>The <code>get_splits_from_patient_with_target</code> method samples valid split points (anchored to sampling random times around the lines of therapy) and determines appropriate targets (forecasting vs. event prediction). This allows the model to learn from various stages of a patient's journey.</p>"},{"location":"examples/01_data_preparation_for_training/#inspect-the-output","title":"Inspect the Output\u00b6","text":"<p>The <code>forward_conversion</code> method returns <code>p_converted</code>, a dictionary containing the final LLM training example:</p> <ul> <li><code>instruction</code>: The full text prompt. It includes the patient's history (demographics + events) followed by the specific task questions.</li> <li><code>answer</code>: The target completion string containing the correct answers for the tasks.</li> <li><code>meta</code>: Structured metadata used to generate the text, useful for debugging or evaluation.</li> </ul>"},{"location":"examples/01_data_preparation_for_training/#reverse-conversion-text-to-structured-data","title":"Reverse Conversion: Text to Structured Data\u00b6","text":"<p>Finally, we demonstrate the Reverse Conversion process. This is the inverse of the instruction generation step. It takes the text string (which would be generated by the model during inference) and parses it back into structured Pandas DataFrames.</p> <p>This capability is crucial for:</p> <ul> <li>Evaluation: Comparing the model's text predictions against ground truth data programmatically.</li> <li>Integration: Converting the model's narrative outputs back into downstream clinical systems or dashboards.</li> </ul> <p>In this example, we take the <code>answer</code> string we just generated and confirm it can be reconstructed into a structured dataframe using the <code>reverse_conversion</code> method.</p>"},{"location":"examples/02_inference_prompt_preparation/","title":"Example of running inference","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom twinweaver import (\n    DataSplitterForecasting,\n    DataManager,\n    DataSplitterEvents,\n    ConverterInstruction,\n    Config,\n    DataSplitter,\n)\n</pre> import pandas as pd  from twinweaver import (     DataSplitterForecasting,     DataManager,     DataSplitterEvents,     ConverterInstruction,     Config,     DataSplitter, ) In\u00a0[\u00a0]: Copied! <pre>df_events = pd.read_csv(\"./example_data/events.csv\")\ndf_constant = pd.read_csv(\"./example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"./example_data/constant_description.csv\")\n</pre> df_events = pd.read_csv(\"./example_data/events.csv\") df_constant = pd.read_csv(\"./example_data/constant.csv\") df_constant_description = pd.read_csv(\"./example_data/constant_description.csv\") In\u00a0[\u00a0]: Copied! <pre>config = Config()  # Override values here to customize pipeline\n\n# &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt;\n# 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot')\n# Has to be set for all instruction tasks\nconfig.split_event_category = \"lot\"\n\n# 2. List of event categories we want to forecast (e.g., forecasting 'lab' values)\n# Only needs to be set if you want to forecast variables\nconfig.event_category_forecast = [\"lab\"]\n\n# 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression')\n# Only needs to be set if you want to do time to event prediction\nconfig.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\"\n}\n\n# Optional configs, here to correctly use the static information\nconfig.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]  # Manually set from constant\nconfig.constant_birthdate_column = \"birthyear\"\n</pre> config = Config()  # Override values here to customize pipeline  # &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt; # 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot') # Has to be set for all instruction tasks config.split_event_category = \"lot\"  # 2. List of event categories we want to forecast (e.g., forecasting 'lab' values) # Only needs to be set if you want to forecast variables config.event_category_forecast = [\"lab\"]  # 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression') # Only needs to be set if you want to do time to event prediction config.data_splitter_events_variables_category_mapping = {     \"death\": \"death\",     \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\" }  # Optional configs, here to correctly use the static information config.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]  # Manually set from constant config.constant_birthdate_column = \"birthyear\" In\u00a0[\u00a0]: Copied! <pre>dm = DataManager(config=config)\ndm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\ndm.infer_var_types()\n\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\n\ndata_splitter_forecasting = DataSplitterForecasting(\n    data_manager=dm,\n    config=config,\n)\n# In case you manually want to override the variables for forecasting selectiong, you can skip this next line.\ndata_splitter_forecasting.setup_statistics()\n\n# We will use the easier interface that combines both data splitters\ndata_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config,\n    dm=dm,\n    variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks\n)\n</pre> dm = DataManager(config=config) dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description) dm.process_indication_data() dm.setup_unique_mapping_of_events() dm.setup_dataset_splits() dm.infer_var_types()  data_splitter_events = DataSplitterEvents(dm, config=config) data_splitter_events.setup_variables()  data_splitter_forecasting = DataSplitterForecasting(     data_manager=dm,     config=config, ) # In case you manually want to override the variables for forecasting selectiong, you can skip this next line. data_splitter_forecasting.setup_statistics()  # We will use the easier interface that combines both data splitters data_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)  converter = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config,     dm=dm,     variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks ) In\u00a0[\u00a0]: Copied! <pre>patientid = dm.all_patientids[2]\npatientid\n</pre> patientid = dm.all_patientids[2] patientid In\u00a0[\u00a0]: Copied! <pre>patient_data = dm.get_patient_data(patientid)\npatient_data[\"events\"] = patient_data[\"events\"].sort_values(\"date\")\n\n# To simulate that we only have input, half the events\npatient_data[\"events\"] = patient_data[\"events\"].iloc[: int(len(patient_data[\"events\"]) / 2)]\n</pre> patient_data = dm.get_patient_data(patientid) patient_data[\"events\"] = patient_data[\"events\"].sort_values(\"date\")  # To simulate that we only have input, half the events patient_data[\"events\"] = patient_data[\"events\"].iloc[: int(len(patient_data[\"events\"]) / 2)] In\u00a0[\u00a0]: Copied! <pre>forecast_split, events_split = data_splitter.get_splits_from_patient_inference(\n    patient_data,\n    inference_type=\"both\",\n    forecasting_override_variables_to_predict=[\"hemoglobin_-_718-7\"],\n    events_override_category=\"death\",\n    events_override_observation_time_delta=pd.Timedelta(days=52 * 7),\n)\n</pre> forecast_split, events_split = data_splitter.get_splits_from_patient_inference(     patient_data,     inference_type=\"both\",     forecasting_override_variables_to_predict=[\"hemoglobin_-_718-7\"],     events_override_category=\"death\",     events_override_observation_time_delta=pd.Timedelta(days=52 * 7), ) In\u00a0[\u00a0]: Copied! <pre># We also need to setup when we want to forecast into the future for each variable, in weeks\n# By default the system tries to map the name below to event_name, and backup just uses the provided value\nforecasting_times_to_predict = {\n    \"hemoglobin_-_718-7\": [4, 8, 12],\n}\n</pre> # We also need to setup when we want to forecast into the future for each variable, in weeks # By default the system tries to map the name below to event_name, and backup just uses the provided value forecasting_times_to_predict = {     \"hemoglobin_-_718-7\": [4, 8, 12], } In\u00a0[\u00a0]: Copied! <pre># Convert to instruction\nconverted = converter.forward_conversion_inference(\n    forecasting_split=forecast_split,\n    forecasting_future_weeks_per_variable=forecasting_times_to_predict,\n    event_split=events_split,\n    custom_tasks=None,\n)\n</pre> # Convert to instruction converted = converter.forward_conversion_inference(     forecasting_split=forecast_split,     forecasting_future_weeks_per_variable=forecasting_times_to_predict,     event_split=events_split,     custom_tasks=None, ) In\u00a0[\u00a0]: Copied! <pre>print(converted[\"instruction\"])\n</pre> print(converted[\"instruction\"])"},{"location":"examples/02_inference_prompt_preparation/#example-of-running-inference","title":"Example of running inference\u00b6","text":""},{"location":"examples/02_inference_prompt_preparation/#setup-basic-data-loading","title":"Setup basic data loading\u00b6","text":""},{"location":"examples/02_inference_prompt_preparation/#example-patient-data","title":"Example patient data\u00b6","text":""},{"location":"examples/03_end_to_end_llm_finetuning/","title":"End to end instruction example using LLMs with fine-tuning","text":"In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoTokenizer\nimport pandas as pd\nimport gc\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    pipeline,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer, SFTConfig\n\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n    DataSplitterForecasting,\n    DataSplitterEvents,\n    ConverterInstruction,\n    DataSplitter,\n)\n</pre> from transformers import AutoTokenizer import pandas as pd import gc import torch from datasets import Dataset from transformers import (     AutoModelForCausalLM,     BitsAndBytesConfig,     pipeline, ) from peft import LoraConfig, PeftModel from trl import SFTTrainer, SFTConfig   from twinweaver import (     DataManager,     Config,     DataSplitterForecasting,     DataSplitterEvents,     ConverterInstruction,     DataSplitter, ) In\u00a0[\u00a0]: Copied! <pre># Some key settings\nBASE_MODEL = \"microsoft/Phi-4-mini-instruct\"  # NOTE: we haven't tested the performance of this model beyond examples\nMAX_CONTEXT_LENGTH = 8192  # Set this based on memory and model capabilities\n</pre> # Some key settings BASE_MODEL = \"microsoft/Phi-4-mini-instruct\"  # NOTE: we haven't tested the performance of this model beyond examples MAX_CONTEXT_LENGTH = 8192  # Set this based on memory and model capabilities <p>First, we need to set up the configuration. This includes specifying where we want to split, which variables we want to predict and which constant variables to use.</p> In\u00a0[\u00a0]: Copied! <pre># Load data\ndf_events = pd.read_csv(\"./example_data/events.csv\")\ndf_constant = pd.read_csv(\"./example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"./example_data/constant_description.csv\")\n</pre> # Load data df_events = pd.read_csv(\"./example_data/events.csv\") df_constant = pd.read_csv(\"./example_data/constant.csv\") df_constant_description = pd.read_csv(\"./example_data/constant_description.csv\") In\u00a0[\u00a0]: Copied! <pre># Manually set up which constant columns we want to use\nconfig = Config()  # Override values here to customize pipeline\n\n# &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt;\n# 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot')\n# Has to be set for all instruction tasks\nconfig.split_event_category = \"lot\"\n\n# 2. List of event categories we want to forecast (e.g., forecasting 'lab' values)\n# Only needs to be set if you want to forecast variables\nconfig.event_category_forecast = [\"lab\"]\n\n# 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression')\n# Only needs to be set if you want to do time to event prediction\nconfig.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\"\n}\n\n# Optional configs, here to correctly use the static information\nconfig.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]\nconfig.constant_birthdate_column = \"birthyear\"\n</pre> # Manually set up which constant columns we want to use config = Config()  # Override values here to customize pipeline  # &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt; # 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot') # Has to be set for all instruction tasks config.split_event_category = \"lot\"  # 2. List of event categories we want to forecast (e.g., forecasting 'lab' values) # Only needs to be set if you want to forecast variables config.event_category_forecast = [\"lab\"]  # 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression') # Only needs to be set if you want to do time to event prediction config.data_splitter_events_variables_category_mapping = {     \"death\": \"death\",     \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\" }  # Optional configs, here to correctly use the static information config.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"] config.constant_birthdate_column = \"birthyear\" <p>Here we initialize the <code>DataManager</code> to handle data loading and processing.</p> <p>We also set up the Data Splitters:</p> <ul> <li><code>DataSplitterEvents</code>: Handles splitting of event data (diagnoses, treatments).</li> <li><code>DataSplitterForecasting</code>: Handles splitting of time-series data (lab values) and statistics generation.</li> </ul> <p>Finally, <code>ConverterInstruction</code> is initialized. This component is responsible for translating the structured patient data splits into the textual instruction format (Prompt + Completion) that the LLM understands.</p> In\u00a0[\u00a0]: Copied! <pre>dm = DataManager(config=config)\ndm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\ndm.infer_var_types()\n\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\n\ndata_splitter_forecasting = DataSplitterForecasting(\n    data_manager=dm,\n    config=config,\n)\n\n# If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step\ndata_splitter_forecasting.setup_statistics()\n\n# We will also use the easier interface that combines both data splitters\ndata_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=MAX_CONTEXT_LENGTH,\n    config=config,\n    dm=dm,\n    variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks\n)\n</pre> dm = DataManager(config=config) dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description) dm.process_indication_data() dm.setup_unique_mapping_of_events() dm.setup_dataset_splits() dm.infer_var_types()  data_splitter_events = DataSplitterEvents(dm, config=config) data_splitter_events.setup_variables()  data_splitter_forecasting = DataSplitterForecasting(     data_manager=dm,     config=config, )  # If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step data_splitter_forecasting.setup_statistics()  # We will also use the easier interface that combines both data splitters data_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)  converter = ConverterInstruction(     nr_tokens_budget_total=MAX_CONTEXT_LENGTH,     config=config,     dm=dm,     variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks ) In\u00a0[\u00a0]: Copied! <pre># Get all training + validation patientids\ntraining_patientids = dm.get_all_patientids_in_split(config.train_split_name)\nvalidation_patientids = dm.get_all_patientids_in_split(config.validation_split_name)\n</pre> # Get all training + validation patientids training_patientids = dm.get_all_patientids_in_split(config.train_split_name) validation_patientids = dm.get_all_patientids_in_split(config.validation_split_name) <p>The <code>generate_transformers_df</code> function iterates through each patient and generates input/output pairs. For each patient, it may generate multiple \"splits\" (different reference dates in their history). Each split is converted into a text prompt (history) and a text completion (future events/values). The result is a DataFrame with \"prompt\" and \"completion\" columns.</p> In\u00a0[\u00a0]: Copied! <pre>def generate_transformers_df(patientids_list):\n    df = []\n\n    for patientid in patientids_list:\n        patient_data = dm.get_patient_data(patientid)\n\n        forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(\n            patient_data,\n            forecasting_filter_outliers=False,\n        )\n\n        for split_idx in range(len(forecasting_splits)):\n            p_converted = converter.forward_conversion(\n                forecasting_splits=forecasting_splits[split_idx],\n                event_splits=events_splits[split_idx],\n                override_mode_to_select_forecasting=\"both\",\n            )\n            new_data = {\n                \"prompt\": p_converted[\"instruction\"],\n                \"completion\": p_converted[\"answer\"],\n                \"patientid\": f\"{patientid}_split{split_idx}\",  # Just for ease of finding later\n            }\n            df.append(new_data)\n\n    df = pd.DataFrame(df)\n    return df\n</pre> def generate_transformers_df(patientids_list):     df = []      for patientid in patientids_list:         patient_data = dm.get_patient_data(patientid)          forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(             patient_data,             forecasting_filter_outliers=False,         )          for split_idx in range(len(forecasting_splits)):             p_converted = converter.forward_conversion(                 forecasting_splits=forecasting_splits[split_idx],                 event_splits=events_splits[split_idx],                 override_mode_to_select_forecasting=\"both\",             )             new_data = {                 \"prompt\": p_converted[\"instruction\"],                 \"completion\": p_converted[\"answer\"],                 \"patientid\": f\"{patientid}_split{split_idx}\",  # Just for ease of finding later             }             df.append(new_data)      df = pd.DataFrame(df)     return df In\u00a0[\u00a0]: Copied! <pre># Generate training and validation dfs\ndf_train = generate_transformers_df(training_patientids)\ndf_validation = generate_transformers_df(validation_patientids)\n</pre> # Generate training and validation dfs df_train = generate_transformers_df(training_patientids) df_validation = generate_transformers_df(validation_patientids) In\u00a0[\u00a0]: Copied! <pre>df_train\n</pre> df_train <p>We start by setting up the tokenizer. We set the padding token to be the same as the EOS (End of Sequence) token, which is a common practice for causal language models.</p> In\u00a0[\u00a0]: Copied! <pre># Setup tokenizer and datasets\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\n# Set padding token to eos_token\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ntrain_dataset = Dataset.from_pandas(df_train)\nvalidation_dataset = Dataset.from_pandas(df_validation)\n</pre> # Setup tokenizer and datasets tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)  # Set padding token to eos_token tokenizer.pad_token = tokenizer.eos_token tokenizer.pad_token_id = tokenizer.eos_token_id  train_dataset = Dataset.from_pandas(df_train) validation_dataset = Dataset.from_pandas(df_validation) <p>Instruction-tuned models expect data in a specific conversational format (e.g., User: ... Assistant: ...). We use <code>format_chat_template</code> to structure our raw prompt/completion strings into this list-of-messages format using the <code>user</code> and <code>assistant</code> roles.</p> In\u00a0[\u00a0]: Copied! <pre># Format data for chat template\ndef format_chat_template(example):\n    \"\"\"Convert prompt/completion pairs to proper prompt/completion format\"\"\"\n    return {\n        \"prompt\": [{\"role\": \"user\", \"content\": example[\"prompt\"]}],\n        \"completion\": [{\"role\": \"assistant\", \"content\": example[\"completion\"]}],\n    }\n\n\n# Apply formatting to datasets\ntrain_dataset = train_dataset.map(format_chat_template)\nvalidation_dataset = validation_dataset.map(format_chat_template)\n</pre> # Format data for chat template def format_chat_template(example):     \"\"\"Convert prompt/completion pairs to proper prompt/completion format\"\"\"     return {         \"prompt\": [{\"role\": \"user\", \"content\": example[\"prompt\"]}],         \"completion\": [{\"role\": \"assistant\", \"content\": example[\"completion\"]}],     }   # Apply formatting to datasets train_dataset = train_dataset.map(format_chat_template) validation_dataset = validation_dataset.map(format_chat_template) <p>We configure 4-bit quantization using <code>BitsAndBytesConfig</code> (QLoRA). This significantly lowers memory usage, allowing us to fine-tune the model on consumer GPUs.</p> In\u00a0[\u00a0]: Copied! <pre># Define Quantization Config (4-bit loading)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,  # This should be set based on your GPU capabilities\n    bnb_4bit_use_double_quant=True,\n)\n</pre> # Define Quantization Config (4-bit loading) bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.bfloat16,  # This should be set based on your GPU capabilities     bnb_4bit_use_double_quant=True, ) <p>Here we set up Low-Rank Adaptation (LoRA) configuration. <code>LoraConfig</code> defines the adapter parameters (rank <code>r</code>, <code>alpha</code>). we target linear layers (<code>q_proj</code>, <code>k_proj</code> etc.) which generally yields better results than just attending to query/value projections.</p> In\u00a0[\u00a0]: Copied! <pre>peft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=8,  # Rank (higher = more parameters to train)\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    # Target all linear layers for best performance (specific to Llama architecture)\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n)\n</pre> peft_config = LoraConfig(     lora_alpha=16,     lora_dropout=0.1,     r=8,  # Rank (higher = more parameters to train)     bias=\"none\",     task_type=\"CAUSAL_LM\",     # Target all linear layers for best performance (specific to Llama architecture)     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], ) <p>We define the training arguments in <code>SFTConfig</code>. Notice the higher learning rate (<code>1e-4</code>) compared to typical full fine-tuning in the GDT paper. We also set <code>bf16=True</code> for newer GPUs (Ampere+) to improve training stability.</p> In\u00a0[\u00a0]: Copied! <pre>training_arguments = SFTConfig(\n    output_dir=\"./results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=10,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    per_device_eval_batch_size=1,\n    learning_rate=1e-4,  # LR is higher for PEFT, see TwinWeaver paper for full fine-tuning details\n    fp16=False,  # Use fp16 for T4/V100, bf16 for Ampere and later (A100/3090/4090)\n    bf16=True,\n    max_grad_norm=1.0,\n    warmup_ratio=0.1,\n    group_by_length=True,\n    save_total_limit=1,\n    lr_scheduler_type=\"cosine\",\n    max_length=MAX_CONTEXT_LENGTH,\n    packing=False,  # Disable packing for instruction tuning\n    completion_only_loss=True,  # Only compute loss on assistant responses\n)\n</pre> training_arguments = SFTConfig(     output_dir=\"./results\",     num_train_epochs=2,     per_device_train_batch_size=1,     gradient_accumulation_steps=1,     optim=\"paged_adamw_32bit\",     save_steps=10,     logging_steps=10,     eval_strategy=\"steps\",     eval_steps=10,     per_device_eval_batch_size=1,     learning_rate=1e-4,  # LR is higher for PEFT, see TwinWeaver paper for full fine-tuning details     fp16=False,  # Use fp16 for T4/V100, bf16 for Ampere and later (A100/3090/4090)     bf16=True,     max_grad_norm=1.0,     warmup_ratio=0.1,     group_by_length=True,     save_total_limit=1,     lr_scheduler_type=\"cosine\",     max_length=MAX_CONTEXT_LENGTH,     packing=False,  # Disable packing for instruction tuning     completion_only_loss=True,  # Only compute loss on assistant responses ) In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=False,\n)\n\n# Disable cache for training (required for gradient checkpointing)\nmodel.config.use_cache = False\n</pre> model = AutoModelForCausalLM.from_pretrained(     BASE_MODEL,     quantization_config=bnb_config,     device_map=\"auto\",     trust_remote_code=False, )  # Disable cache for training (required for gradient checkpointing) model.config.use_cache = False In\u00a0[\u00a0]: Copied! <pre>trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n    args=training_arguments,\n    eval_dataset=validation_dataset,\n    peft_config=peft_config,\n)\n</pre> trainer = SFTTrainer(     model=model,     train_dataset=train_dataset,     processing_class=tokenizer,     args=training_arguments,     eval_dataset=validation_dataset,     peft_config=peft_config, ) In\u00a0[\u00a0]: Copied! <pre># Start training - takes around 5 mins, depending on hardware\ntrainer.train()\n</pre> # Start training - takes around 5 mins, depending on hardware trainer.train() In\u00a0[\u00a0]: Copied! <pre># Save the fine-tuned adapter\nadapter_path = \"./results/final_adapter\"\ntrainer.save_model(adapter_path)\nprint(f\"Adapter saved to {adapter_path}\")\n\ndel trainer\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\n</pre> # Save the fine-tuned adapter adapter_path = \"./results/final_adapter\" trainer.save_model(adapter_path) print(f\"Adapter saved to {adapter_path}\")  del trainer del model gc.collect() torch.cuda.empty_cache() In\u00a0[\u00a0]: Copied! <pre># Get the first test set patient\ntest_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0]\npatient_data = dm.get_patient_data(test_patientid)\n\n# Lets simulate forecasts for after the first line of therapy\ndf_constant_patient = patient_data[\"constant\"].copy()\ndf_events_patient = patient_data[\"events\"].copy()\ndate_of_first_lot = df_events_patient.loc[\n    df_events_patient[\"event_category\"] == config.event_category_lot, \"date\"\n].min()\n\n# Only keep data until (and including) first line of therapy\ndf_events_patient = df_events_patient.loc[df_events_patient[\"date\"] &lt;= date_of_first_lot]\n</pre> # Get the first test set patient test_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0] patient_data = dm.get_patient_data(test_patientid)  # Lets simulate forecasts for after the first line of therapy df_constant_patient = patient_data[\"constant\"].copy() df_events_patient = patient_data[\"events\"].copy() date_of_first_lot = df_events_patient.loc[     df_events_patient[\"event_category\"] == config.event_category_lot, \"date\" ].min()  # Only keep data until (and including) first line of therapy df_events_patient = df_events_patient.loc[df_events_patient[\"date\"] &lt;= date_of_first_lot] In\u00a0[\u00a0]: Copied! <pre># Lets forecast hemoglobin at 4, 8, and 12 weeks\n# and death within 52 weeks\nforecasting_times_to_predict = {\n    \"hemoglobin_-_718-7\": [4, 8, 12],\n}\n\nforecast_split, events_split = data_splitter.get_splits_from_patient_inference(\n    patient_data,\n    inference_type=\"both\",\n    forecasting_override_variables_to_predict=[\"hemoglobin_-_718-7\"],\n    events_override_category=\"death\",\n    events_override_observation_time_delta=pd.Timedelta(days=52 * 7),\n)\n</pre> # Lets forecast hemoglobin at 4, 8, and 12 weeks # and death within 52 weeks forecasting_times_to_predict = {     \"hemoglobin_-_718-7\": [4, 8, 12], }  forecast_split, events_split = data_splitter.get_splits_from_patient_inference(     patient_data,     inference_type=\"both\",     forecasting_override_variables_to_predict=[\"hemoglobin_-_718-7\"],     events_override_category=\"death\",     events_override_observation_time_delta=pd.Timedelta(days=52 * 7), ) <p>We convert the patient data into an instruction prompt. Unlike training, <code>forward_conversion_inference</code> only generates the input prompt (without the target answer), as we want the LLM to generate the answer.</p> In\u00a0[\u00a0]: Copied! <pre># Convert to instruction\nconverted = converter.forward_conversion_inference(\n    forecasting_split=forecast_split,\n    forecasting_future_weeks_per_variable=forecasting_times_to_predict,\n    event_split=events_split,\n    custom_tasks=None,\n)\n</pre> # Convert to instruction converted = converter.forward_conversion_inference(     forecasting_split=forecast_split,     forecasting_future_weeks_per_variable=forecasting_times_to_predict,     event_split=events_split,     custom_tasks=None, ) <p>For inference, we load the base model again (clean slate) to avoid any state from training, and then attach the adapter we trained. <code>PeftModel</code> handles the integration of the LoRA weights with the base model.</p> <p>For inference, we load the base model again (clean slate) and then attach the adapter we trained. <code>PeftModel</code> handles the integration of the LoRA weights.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Load the Base Model again (clean instance)\nbase_model_inference = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,  # Reuse the 4-bit config\n    device_map=\"auto\",\n    trust_remote_code=False,\n)\n\n# 2. Load the Saved Adapter\n# This wraps the base model with the fine-tuned LoRA layers\ninference_model = PeftModel.from_pretrained(base_model_inference, adapter_path)\n\n# 3. Switch to evaluation mode\ninference_model.eval()\n</pre> # 1. Load the Base Model again (clean instance) base_model_inference = AutoModelForCausalLM.from_pretrained(     BASE_MODEL,     quantization_config=bnb_config,  # Reuse the 4-bit config     device_map=\"auto\",     trust_remote_code=False, )  # 2. Load the Saved Adapter # This wraps the base model with the fine-tuned LoRA layers inference_model = PeftModel.from_pretrained(base_model_inference, adapter_path)  # 3. Switch to evaluation mode inference_model.eval() In\u00a0[\u00a0]: Copied! <pre># Create text generation pipeline\n# Re-enable cache for inference\ninference_model.config.use_cache = True\ntext_gen_pipeline = pipeline(\"text-generation\", model=inference_model, tokenizer=tokenizer)\n</pre> # Create text generation pipeline # Re-enable cache for inference inference_model.config.use_cache = True text_gen_pipeline = pipeline(\"text-generation\", model=inference_model, tokenizer=tokenizer) In\u00a0[\u00a0]: Copied! <pre># Generate with LLM\ngenerated_answer = text_gen_pipeline(\n    [{\"role\": \"user\", \"content\": converted[\"instruction\"]}],\n    max_new_tokens=128,\n    return_full_text=False,\n    do_sample=True,  # Using nucleus sampling\n    temperature=0.7,\n    top_p=0.9,\n)[0][\"generated_text\"]\n</pre> # Generate with LLM generated_answer = text_gen_pipeline(     [{\"role\": \"user\", \"content\": converted[\"instruction\"]}],     max_new_tokens=128,     return_full_text=False,     do_sample=True,  # Using nucleus sampling     temperature=0.7,     top_p=0.9, )[0][\"generated_text\"] In\u00a0[\u00a0]: Copied! <pre># Show the generated answer\ngenerated_answer\n</pre> # Show the generated answer generated_answer <p>The raw text output from the model needs to be parsed back into structured data. <code>reverse_conversion</code> handles this, returning a list of dictionaries with the predicted results for each task.</p> In\u00a0[\u00a0]: Copied! <pre># Reverse convert\nreturn_list = converter.reverse_conversion(generated_answer, dm, date_of_first_lot)\n</pre> # Reverse convert return_list = converter.reverse_conversion(generated_answer, dm, date_of_first_lot) In\u00a0[\u00a0]: Copied! <pre># Task 1 reverse conversion\nreturn_list[0][\"result\"]\n</pre> # Task 1 reverse conversion return_list[0][\"result\"] In\u00a0[\u00a0]: Copied! <pre># Task 2 reverse conversion\nreturn_list[1][\"result\"]\n</pre> # Task 2 reverse conversion return_list[1][\"result\"]"},{"location":"examples/03_end_to_end_llm_finetuning/#end-to-end-instruction-example-using-llms-with-fine-tuning","title":"End to end instruction example using LLMs with fine-tuning\u00b6","text":"<p>This notebook provides a comprehensive, end-to-end demonstration of fine-tuning a Large Language Model (LLM) for medical forecasting tasks using the twinweaver library. The workflow begins by processing raw medical data (events, constants, and lab values) into structured instruction-tuning datasets using DataManager and ConverterInstruction, effectively translating patient histories into prompt-completion pairs. We then implement Parameter-Efficient Fine-Tuning (PEFT) using QLoRA (4-bit quantization) and the SFTTrainer to adapt a microsoft/Phi-4-mini-instruct model, optimizing it for clinical predictions while managing memory constraints. Finally, the example concludes with an inference pipeline that loads the trained adapter to predict future clinical outcomes\u2014such as hemoglobin levels and mortality risks\u2014and reverse-converts the LLM's text output back into structured data.</p> <p>Note: You need a GPU with at least 30GB of memory for this example to work. We also have not tested the performance of PEFT models - only as examples.</p> <p>Important: Please install first the fine-tuning packages with <code>pip install twinweaver[fine-tuning-example]</code>.</p>"},{"location":"examples/03_end_to_end_llm_finetuning/#generate-training-data","title":"Generate training data\u00b6","text":""},{"location":"examples/03_end_to_end_llm_finetuning/#fine-tune-llm","title":"Fine-tune LLM\u00b6","text":""},{"location":"examples/03_end_to_end_llm_finetuning/#inference-example","title":"Inference example\u00b6","text":"<p>Inference example for a test set patient, where we want to make predictions after the first line of therapy.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/","title":"Customizing Text Generation in TwinWeaver","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n    DataSplitterForecasting,\n    DataSplitterEvents,\n    ConverterInstruction,\n    DataSplitter,\n)\n</pre> import pandas as pd  from twinweaver import (     DataManager,     Config,     DataSplitterForecasting,     DataSplitterEvents,     ConverterInstruction,     DataSplitter, ) In\u00a0[\u00a0]: Copied! <pre># Load data - generated example data\ndf_events = pd.read_csv(\"../../example_data/events.csv\")\ndf_constant = pd.read_csv(\"../../example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\")\n</pre> # Load data - generated example data df_events = pd.read_csv(\"../../example_data/events.csv\") df_constant = pd.read_csv(\"../../example_data/constant.csv\") df_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\") In\u00a0[\u00a0]: Copied! <pre># Create default config\nconfig_default = Config()\n\n# Required settings for instruction mode\nconfig_default.split_event_category = \"lot\"\nconfig_default.event_category_forecast = [\"lab\"]\nconfig_default.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",\n}\nconfig_default.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]\nconfig_default.constant_birthdate_column = \"birthyear\"\n</pre> # Create default config config_default = Config()  # Required settings for instruction mode config_default.split_event_category = \"lot\" config_default.event_category_forecast = [\"lab\"] config_default.data_splitter_events_variables_category_mapping = {     \"death\": \"death\",     \"progression\": \"next progression\", } config_default.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"] config_default.constant_birthdate_column = \"birthyear\" In\u00a0[\u00a0]: Copied! <pre># Setup data manager with default config\ndm_default = DataManager(config=config_default)\ndm_default.load_indication_data(\n    df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description\n)\ndm_default.process_indication_data()\ndm_default.setup_unique_mapping_of_events()\ndm_default.setup_dataset_splits()\ndm_default.infer_var_types()\n</pre> # Setup data manager with default config dm_default = DataManager(config=config_default) dm_default.load_indication_data(     df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description ) dm_default.process_indication_data() dm_default.setup_unique_mapping_of_events() dm_default.setup_dataset_splits() dm_default.infer_var_types() In\u00a0[\u00a0]: Copied! <pre># Setup splitters and converter\ndata_splitter_events_default = DataSplitterEvents(dm_default, config=config_default)\ndata_splitter_events_default.setup_variables()\n\ndata_splitter_forecasting_default = DataSplitterForecasting(data_manager=dm_default, config=config_default)\ndata_splitter_forecasting_default.setup_statistics()\n\ndata_splitter_default = DataSplitter(data_splitter_events_default, data_splitter_forecasting_default)\n\nconverter_default = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config_default,\n    dm=dm_default,\n    variable_stats=data_splitter_forecasting_default.variable_stats,\n)\n</pre> # Setup splitters and converter data_splitter_events_default = DataSplitterEvents(dm_default, config=config_default) data_splitter_events_default.setup_variables()  data_splitter_forecasting_default = DataSplitterForecasting(data_manager=dm_default, config=config_default) data_splitter_forecasting_default.setup_statistics()  data_splitter_default = DataSplitter(data_splitter_events_default, data_splitter_forecasting_default)  converter_default = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config_default,     dm=dm_default,     variable_stats=data_splitter_forecasting_default.variable_stats, ) In\u00a0[\u00a0]: Copied! <pre># Generate example with default text\npatientid = dm_default.all_patientids[4]\npatient_data_default = dm_default.get_patient_data(patientid)\n\nforecasting_splits, events_splits, reference_dates = data_splitter_default.get_splits_from_patient_with_target(\n    patient_data_default,\n)\n\np_converted_default = converter_default.forward_conversion(\n    forecasting_splits=forecasting_splits[0],\n    event_splits=events_splits[0],\n    override_mode_to_select_forecasting=\"both\",\n)\n\nprint(\"=\" * 80)\nprint(\"DEFAULT INSTRUCTION OUTPUT:\")\nprint(\"=\" * 80)\nprint(p_converted_default[\"instruction\"])\n</pre> # Generate example with default text patientid = dm_default.all_patientids[4] patient_data_default = dm_default.get_patient_data(patientid)  forecasting_splits, events_splits, reference_dates = data_splitter_default.get_splits_from_patient_with_target(     patient_data_default, )  p_converted_default = converter_default.forward_conversion(     forecasting_splits=forecasting_splits[0],     event_splits=events_splits[0],     override_mode_to_select_forecasting=\"both\", )  print(\"=\" * 80) print(\"DEFAULT INSTRUCTION OUTPUT:\") print(\"=\" * 80) print(p_converted_default[\"instruction\"]) In\u00a0[\u00a0]: Copied! <pre># Create custom config\nconfig_custom = Config()\n\n# Required settings\nconfig_custom.split_event_category = \"lot\"\nconfig_custom.event_category_forecast = [\"lab\"]\nconfig_custom.data_splitter_events_variables_category_mapping = {\n    \"death\": \"mortality\",  # Custom name for death event\n    \"progression\": \"disease progression\",  # Custom name for progression\n}\nconfig_custom.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]\nconfig_custom.constant_birthdate_column = \"birthyear\"\n\n# ============================================================================\n# CUSTOMIZING PREAMBLE TEXT\n# ============================================================================\n# This is the opening text that introduces the patient record\nconfig_custom.preamble_text = (\n    \"\ud83d\udccb ELECTRONIC HEALTH RECORD SUMMARY\\n\"\n    \"This document contains a chronological summary of a patient's medical journey. \"\n    \"The record begins with baseline demographics, followed by a timeline of clinical encounters. \"\n    \"Laboratory values use standardized LOINC coding.\"\n)\n</pre> # Create custom config config_custom = Config()  # Required settings config_custom.split_event_category = \"lot\" config_custom.event_category_forecast = [\"lab\"] config_custom.data_splitter_events_variables_category_mapping = {     \"death\": \"mortality\",  # Custom name for death event     \"progression\": \"disease progression\",  # Custom name for progression } config_custom.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"] config_custom.constant_birthdate_column = \"birthyear\"  # ============================================================================ # CUSTOMIZING PREAMBLE TEXT # ============================================================================ # This is the opening text that introduces the patient record config_custom.preamble_text = (     \"\ud83d\udccb ELECTRONIC HEALTH RECORD SUMMARY\\n\"     \"This document contains a chronological summary of a patient's medical journey. \"     \"The record begins with baseline demographics, followed by a timeline of clinical encounters. \"     \"Laboratory values use standardized LOINC coding.\" ) In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING DEMOGRAPHICS SECTION\n# ============================================================================\n# Text that introduces the demographics/constant data\nconfig_custom.constant_text = \"\\n\\n\ud83d\udc64 PATIENT DEMOGRAPHICS:\\n\"\n</pre> # ============================================================================ # CUSTOMIZING DEMOGRAPHICS SECTION # ============================================================================ # Text that introduces the demographics/constant data config_custom.constant_text = \"\\n\\n\ud83d\udc64 PATIENT DEMOGRAPHICS:\\n\" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING EVENT DAY TEXT\n# ============================================================================\n\n# Text for the first visit/encounter\nconfig_custom.first_day_text = \"\\n\ud83c\udfe5 INITIAL ENCOUNTER:\\nDuring the baseline visit, the following was documented:\\n\"\n\n# Preamble before each subsequent visit (appears before the time delta)\nconfig_custom.event_day_preamble = \"\\n\ud83d\udcc5 \"\n\n# Text describing time elapsed since previous visit\n# Note: {unit} placeholder is used by set_delta_time_unit(), or set directly\nconfig_custom.event_day_text = \" weeks after the previous encounter, a follow-up visit recorded:\\n\"\n\n# Text appended after listing events for a day\nconfig_custom.post_event_text = \".\\n\"\n</pre> # ============================================================================ # CUSTOMIZING EVENT DAY TEXT # ============================================================================  # Text for the first visit/encounter config_custom.first_day_text = \"\\n\ud83c\udfe5 INITIAL ENCOUNTER:\\nDuring the baseline visit, the following was documented:\\n\"  # Preamble before each subsequent visit (appears before the time delta) config_custom.event_day_preamble = \"\\n\ud83d\udcc5 \"  # Text describing time elapsed since previous visit # Note: {unit} placeholder is used by set_delta_time_unit(), or set directly config_custom.event_day_text = \" weeks after the previous encounter, a follow-up visit recorded:\\n\"  # Text appended after listing events for a day config_custom.post_event_text = \".\\n\" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING TIME UNITS\n# ============================================================================\n# Option 1: Use the helper method (updates all time-related prompts)\n# config_custom.set_delta_time_unit(\"days\", unit_sing=\"day\")\n\n# Option 2: Set directly (if you want different phrasing)\nconfig_custom.delta_time_unit = \"weeks\"\n\n# The time unit appears in several prompts - you can customize each:\nconfig_custom.forecasting_prompt_var_time = \" over the upcoming weeks \"\n</pre> # ============================================================================ # CUSTOMIZING TIME UNITS # ============================================================================ # Option 1: Use the helper method (updates all time-related prompts) # config_custom.set_delta_time_unit(\"days\", unit_sing=\"day\")  # Option 2: Set directly (if you want different phrasing) config_custom.delta_time_unit = \"weeks\"  # The time unit appears in several prompts - you can customize each: config_custom.forecasting_prompt_var_time = \" over the upcoming weeks \" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING GENETIC DATA FORMATTING\n# ============================================================================\n\n# Tags used to wrap genetic information in the text\nconfig_custom.genetic_tag_opening = \"[MOLECULAR: \"\nconfig_custom.genetic_tag_closing = \"]\"\n\n# Text shown when no genetic data is available\nconfig_custom.genetic_empty_text = \"\ud83e\uddec No molecular/genetic testing data available.\"\n\n# Value to skip when converting genetic events (often 'present' is implied)\nconfig_custom.genetic_skip_text_value = \"present\"\n</pre> # ============================================================================ # CUSTOMIZING GENETIC DATA FORMATTING # ============================================================================  # Tags used to wrap genetic information in the text config_custom.genetic_tag_opening = \"[MOLECULAR: \" config_custom.genetic_tag_closing = \"]\"  # Text shown when no genetic data is available config_custom.genetic_empty_text = \"\ud83e\uddec No molecular/genetic testing data available.\"  # Value to skip when converting genetic events (often 'present' is implied) config_custom.genetic_skip_text_value = \"present\" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING FORECASTING PROMPTS\n# ============================================================================\n\n# Main forecasting task prompt\nconfig_custom.forecasting_fval_prompt_start = (\n    \"\\n\ud83d\udd2e PREDICTION TASK - LABORATORY VALUES:\\n\"\n    \"Based on the patient history above, predict the expected values for the following \"\n    \"laboratory parameters at each future time point:\\n\"\n)\n\n# Summary section introducing last known values\nconfig_custom.forecasting_prompt_summarized_start = \"\\n\ud83d\udcca REFERENCE VALUES (most recent measurements):\\n\"\n\n# Text used when first day is overridden/truncated\nconfig_custom.forecasting_firstday_override = (\n    \"\\n\u26a0\ufe0f Note: Some early events may have been omitted due to context limits. Available history begins with:\\n\"\n)\n\n# Summary of last observed genetic events\nconfig_custom.forecasting_prompt_summarized_genetic = \"\\n\\n\ud83e\uddec MOLECULAR STATUS (last observed):\\n\"\n\n# Summary of most recent treatment line\nconfig_custom.forecasting_prompt_summarized_lot = \"\\n\ud83d\udc8a CURRENT TREATMENT REGIMEN:\\n\"\n</pre> # ============================================================================ # CUSTOMIZING FORECASTING PROMPTS # ============================================================================  # Main forecasting task prompt config_custom.forecasting_fval_prompt_start = (     \"\\n\ud83d\udd2e PREDICTION TASK - LABORATORY VALUES:\\n\"     \"Based on the patient history above, predict the expected values for the following \"     \"laboratory parameters at each future time point:\\n\" )  # Summary section introducing last known values config_custom.forecasting_prompt_summarized_start = \"\\n\ud83d\udcca REFERENCE VALUES (most recent measurements):\\n\"  # Text used when first day is overridden/truncated config_custom.forecasting_firstday_override = (     \"\\n\u26a0\ufe0f Note: Some early events may have been omitted due to context limits. Available history begins with:\\n\" )  # Summary of last observed genetic events config_custom.forecasting_prompt_summarized_genetic = \"\\n\\n\ud83e\uddec MOLECULAR STATUS (last observed):\\n\"  # Summary of most recent treatment line config_custom.forecasting_prompt_summarized_lot = \"\\n\ud83d\udc8a CURRENT TREATMENT REGIMEN:\\n\" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING TIME-TO-EVENT PROMPTS\n# ============================================================================\n\n# Start of TTE prediction prompt\nconfig_custom.forecasting_tte_prompt_start = (\n    \"\\n\u23f1\ufe0f OUTCOME PREDICTION TASK:\\nDetermine whether follow-up data was censored (incomplete) \"\n)\n\n# Middle section specifying time horizon\nconfig_custom.forecasting_tte_prompt_mid = \" weeks from the last documented visit, and whether the event occurred: \"\n\n# End section with output format instructions\nconfig_custom.forecasting_tte_prompt_end = (\n    \".\\n\ud83d\udcdd Format your response as: 'PREDICTION: [event_name] - Censored: [YES/NO], Occurred: [YES/NO]'\"\n)\n\n# Target/answer formatting\nconfig_custom.target_prompt_start = \"\\nPREDICTION: {event_name} - \"\nconfig_custom.target_prompt_censor_true = \"Censored: YES.\"\nconfig_custom.target_prompt_censor_false = \"Censored: NO, \"\nconfig_custom.target_prompt_before_occur = \"\"\nconfig_custom.target_prompt_occur = \"Occurred: YES.\"\nconfig_custom.target_prompt_not_occur = \"Occurred: NO.\"\n</pre> # ============================================================================ # CUSTOMIZING TIME-TO-EVENT PROMPTS # ============================================================================  # Start of TTE prediction prompt config_custom.forecasting_tte_prompt_start = (     \"\\n\u23f1\ufe0f OUTCOME PREDICTION TASK:\\nDetermine whether follow-up data was censored (incomplete) \" )  # Middle section specifying time horizon config_custom.forecasting_tte_prompt_mid = \" weeks from the last documented visit, and whether the event occurred: \"  # End section with output format instructions config_custom.forecasting_tte_prompt_end = (     \".\\n\ud83d\udcdd Format your response as: 'PREDICTION: [event_name] - Censored: [YES/NO], Occurred: [YES/NO]'\" )  # Target/answer formatting config_custom.target_prompt_start = \"\\nPREDICTION: {event_name} - \" config_custom.target_prompt_censor_true = \"Censored: YES.\" config_custom.target_prompt_censor_false = \"Censored: NO, \" config_custom.target_prompt_before_occur = \"\" config_custom.target_prompt_occur = \"Occurred: YES.\" config_custom.target_prompt_not_occur = \"Occurred: NO.\" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING QA/BINNING PROMPTS\n# ============================================================================\n\n# QA task prompt\nconfig_custom.qa_prompt_start = (\n    \"\\n\ud83d\udcca CLASSIFICATION TASK - VALUE RANGES:\\n\"\n    \"For each variable below, predict which range (bin) the future value will fall into \"\n    \"at each time point:\"\n)\n\n# Text introducing available bins\nconfig_custom.qa_bins_start = \"\\t\u27a1\ufe0f Available categories: \"\n</pre> # ============================================================================ # CUSTOMIZING QA/BINNING PROMPTS # ============================================================================  # QA task prompt config_custom.qa_prompt_start = (     \"\\n\ud83d\udcca CLASSIFICATION TASK - VALUE RANGES:\\n\"     \"For each variable below, predict which range (bin) the future value will fall into \"     \"at each time point:\" )  # Text introducing available bins config_custom.qa_bins_start = \"\\t\u27a1\ufe0f Available categories: \" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING MULTI-TASK PROMPTS\n# ============================================================================\n\n# Introduction to multi-task section\nconfig_custom.task_prompt_start = (\n    \"\\n\" + \"==================================\" + \"\\n\"\n    \"\ud83d\udccb MULTI-TASK INSTRUCTIONS\\n\"\n    \"Complete each task below. Label each response with the task number.\\n\"\n    \"==================================\" + \"\\n\\n\"\n)\n\n# Template for each task introduction\nconfig_custom.task_prompt_each_task = \"\ud83d\udccc TASK #{task_nr}: \"\n\n# End of task prompts section\nconfig_custom.task_prompt_end = \"\\n\" + \"-\" * 50 + \"\\n\"\n\n# Task type labels\nconfig_custom.task_prompt_forecasting = \"Value Forecasting\"\nconfig_custom.task_prompt_forecasting_qa = \"Value Range Classification\"\nconfig_custom.task_prompt_events = \"Outcome Prediction\"\nconfig_custom.task_prompt_custom = \"Custom Analysis\"\n\n# Target/answer formatting for multi-task\nconfig_custom.task_target_start = \"\\n\u2705 TASK #{task_nr} RESPONSE: \"\nconfig_custom.task_target_end = \"\\n\"\n</pre> # ============================================================================ # CUSTOMIZING MULTI-TASK PROMPTS # ============================================================================  # Introduction to multi-task section config_custom.task_prompt_start = (     \"\\n\" + \"==================================\" + \"\\n\"     \"\ud83d\udccb MULTI-TASK INSTRUCTIONS\\n\"     \"Complete each task below. Label each response with the task number.\\n\"     \"==================================\" + \"\\n\\n\" )  # Template for each task introduction config_custom.task_prompt_each_task = \"\ud83d\udccc TASK #{task_nr}: \"  # End of task prompts section config_custom.task_prompt_end = \"\\n\" + \"-\" * 50 + \"\\n\"  # Task type labels config_custom.task_prompt_forecasting = \"Value Forecasting\" config_custom.task_prompt_forecasting_qa = \"Value Range Classification\" config_custom.task_prompt_events = \"Outcome Prediction\" config_custom.task_prompt_custom = \"Custom Analysis\"  # Target/answer formatting for multi-task config_custom.task_target_start = \"\\n\u2705 TASK #{task_nr} RESPONSE: \" config_custom.task_target_end = \"\\n\" In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING EVENT CATEGORY PREAMBLES\n# ============================================================================\n# Override the introductory text for specific event categories\n# Structure: {event_category: preamble_string}\n\nconfig_custom.event_category_preamble_mapping_override = {\n    \"lab\": \"\ud83d\udd2c Laboratory Results: \",\n    \"drug\": \"\ud83d\udc8a Medications: \",\n    \"condition\": \"\ud83e\ude7a Diagnoses/Conditions: \",\n    \"lot\": \"\ud83d\udccb Treatment Line: \",\n    \"vitals\": \"\ud83d\udcc8 Vital Signs: \",\n}\n</pre> # ============================================================================ # CUSTOMIZING EVENT CATEGORY PREAMBLES # ============================================================================ # Override the introductory text for specific event categories # Structure: {event_category: preamble_string}  config_custom.event_category_preamble_mapping_override = {     \"lab\": \"\ud83d\udd2c Laboratory Results: \",     \"drug\": \"\ud83d\udc8a Medications: \",     \"condition\": \"\ud83e\ude7a Diagnoses/Conditions: \",     \"lot\": \"\ud83d\udccb Treatment Line: \",     \"vitals\": \"\ud83d\udcc8 Vital Signs: \", } In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# CUSTOMIZING SPECIFIC EVENT RENDERING\n# ============================================================================\n# Override how specific events are rendered in text\n# Structure: {event_category: {event_name: {\"full_replacement_string\": str, \"reverse_string_value\": str}}}\n\n# This allows complete control over how individual events appear in the generated text\nconfig_custom.event_category_and_name_replace_override = {\n    \"death\": {\n        \"death\": {\n            \"full_replacement_string\": \"\u26a0\ufe0f PATIENT DECEASED\",\n            \"reverse_string_value\": \"deceased\",\n        }\n    },\n    \"progression\": {\n        \"progression\": {\n            \"full_replacement_string\": \"\ud83d\udcc8 Disease progression documented\",\n            \"reverse_string_value\": \"progressed\",\n        }\n    },\n}\n</pre> # ============================================================================ # CUSTOMIZING SPECIFIC EVENT RENDERING # ============================================================================ # Override how specific events are rendered in text # Structure: {event_category: {event_name: {\"full_replacement_string\": str, \"reverse_string_value\": str}}}  # This allows complete control over how individual events appear in the generated text config_custom.event_category_and_name_replace_override = {     \"death\": {         \"death\": {             \"full_replacement_string\": \"\u26a0\ufe0f PATIENT DECEASED\",             \"reverse_string_value\": \"deceased\",         }     },     \"progression\": {         \"progression\": {             \"full_replacement_string\": \"\ud83d\udcc8 Disease progression documented\",             \"reverse_string_value\": \"progressed\",         }     }, } In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# ADDITIONAL FORMATTING OPTIONS\n# ============================================================================\n\n# Number of decimal places for numeric values\nconfig_custom.decimal_precision = 1\n\n# Whether to always include the first visit (even with token constraints)\nconfig_custom.always_keep_first_visit = True\n</pre> # ============================================================================ # ADDITIONAL FORMATTING OPTIONS # ============================================================================  # Number of decimal places for numeric values config_custom.decimal_precision = 1  # Whether to always include the first visit (even with token constraints) config_custom.always_keep_first_visit = True In\u00a0[\u00a0]: Copied! <pre># Setup data manager with custom config\ndm_custom = DataManager(config=config_custom)\ndm_custom.load_indication_data(\n    df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description\n)\ndm_custom.process_indication_data()\ndm_custom.setup_unique_mapping_of_events()\ndm_custom.setup_dataset_splits()\ndm_custom.infer_var_types()\n</pre> # Setup data manager with custom config dm_custom = DataManager(config=config_custom) dm_custom.load_indication_data(     df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description ) dm_custom.process_indication_data() dm_custom.setup_unique_mapping_of_events() dm_custom.setup_dataset_splits() dm_custom.infer_var_types() In\u00a0[\u00a0]: Copied! <pre># Setup splitters and converter with custom config\ndata_splitter_events_custom = DataSplitterEvents(dm_custom, config=config_custom)\ndata_splitter_events_custom.setup_variables()\n\ndata_splitter_forecasting_custom = DataSplitterForecasting(data_manager=dm_custom, config=config_custom)\ndata_splitter_forecasting_custom.setup_statistics()\n\ndata_splitter_custom = DataSplitter(data_splitter_events_custom, data_splitter_forecasting_custom)\n\nconverter_custom = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config_custom,\n    dm=dm_custom,\n    variable_stats=data_splitter_forecasting_custom.variable_stats,\n)\n</pre> # Setup splitters and converter with custom config data_splitter_events_custom = DataSplitterEvents(dm_custom, config=config_custom) data_splitter_events_custom.setup_variables()  data_splitter_forecasting_custom = DataSplitterForecasting(data_manager=dm_custom, config=config_custom) data_splitter_forecasting_custom.setup_statistics()  data_splitter_custom = DataSplitter(data_splitter_events_custom, data_splitter_forecasting_custom)  converter_custom = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config_custom,     dm=dm_custom,     variable_stats=data_splitter_forecasting_custom.variable_stats, ) In\u00a0[\u00a0]: Copied! <pre># Generate example with custom text\npatientid = dm_custom.all_patientids[4]\npatient_data_custom = dm_custom.get_patient_data(patientid)\n\nforecasting_splits_custom, events_splits_custom, reference_dates_custom = (\n    data_splitter_custom.get_splits_from_patient_with_target(patient_data_custom)\n)\n\np_converted_custom = converter_custom.forward_conversion(\n    forecasting_splits=forecasting_splits_custom[0],\n    event_splits=events_splits_custom[0],\n    override_mode_to_select_forecasting=\"both\",\n)\n\nprint(\"=\" * 80)\nprint(\"CUSTOMIZED INSTRUCTION OUTPUT:\")\nprint(\"=\" * 80)\nprint(p_converted_custom[\"instruction\"])\n</pre> # Generate example with custom text patientid = dm_custom.all_patientids[4] patient_data_custom = dm_custom.get_patient_data(patientid)  forecasting_splits_custom, events_splits_custom, reference_dates_custom = (     data_splitter_custom.get_splits_from_patient_with_target(patient_data_custom) )  p_converted_custom = converter_custom.forward_conversion(     forecasting_splits=forecasting_splits_custom[0],     event_splits=events_splits_custom[0],     override_mode_to_select_forecasting=\"both\", )  print(\"=\" * 80) print(\"CUSTOMIZED INSTRUCTION OUTPUT:\") print(\"=\" * 80) print(p_converted_custom[\"instruction\"]) In\u00a0[\u00a0]: Copied! <pre>print(\"=\" * 80)\nprint(\"CUSTOMIZED ANSWER OUTPUT:\")\nprint(\"=\" * 80)\nprint(p_converted_custom[\"answer\"])\n</pre> print(\"=\" * 80) print(\"CUSTOMIZED ANSWER OUTPUT:\") print(\"=\" * 80) print(p_converted_custom[\"answer\"])"},{"location":"examples/advanced/custom_output/customizing_text_generation/#customizing-text-generation-in-twinweaver","title":"Customizing Text Generation in TwinWeaver\u00b6","text":"<p>This tutorial demonstrates how to customize every textual component of the instruction generation pipeline. TwinWeaver provides extensive configuration options to tailor the generated text prompts to your specific use case, language preferences, or model requirements.</p> <p>We will cover:</p> <ol> <li>Preamble &amp; Introduction Text - Customizing the opening text of patient records</li> <li>Demographics Section - Modifying how constant/static data is introduced</li> <li>Event Day Formatting - Changing how visit days and time intervals are described</li> <li>Time Units - Switching between days and weeks</li> <li>Genetic Data Formatting - Customizing genetic event tags and text</li> <li>Forecasting Prompts - Modifying value prediction task descriptions</li> <li>Time-to-Event Prompts - Customizing survival/event prediction text</li> <li>QA/Binning Prompts - Changing quality assurance task descriptions</li> <li>Multi-Task Prompts - Customizing multi-task instruction formatting</li> <li>Event Category Overrides - Fine-grained control over specific event types</li> </ol>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#load-example-data","title":"Load Example Data\u00b6","text":"<p>First, let's load the example data to use throughout this tutorial.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#part-1-default-configuration","title":"Part 1: Default Configuration\u00b6","text":"<p>Let's first see the default text generation to understand what we're customizing. We'll set up a minimal config and generate an example.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#part-2-fully-customized-text-generation","title":"Part 2: Fully Customized Text Generation\u00b6","text":"<p>Now let's create a completely customized configuration, changing every textual element. This demonstrates all the available customization options.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#21-preamble-and-introduction-text","title":"2.1 Preamble and Introduction Text\u00b6","text":"<p>The <code>preamble_text</code> is the very first text that appears in the generated instruction, introducing the patient record format.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#22-demographics-section-text","title":"2.2 Demographics Section Text\u00b6","text":"<p>The <code>constant_text</code> introduces the static/demographic data section.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#23-event-day-formatting","title":"2.3 Event Day Formatting\u00b6","text":"<p>These settings control how clinical visits and time intervals are described.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#24-time-units","title":"2.4 Time Units\u00b6","text":"<p>You can switch between <code>days</code> and <code>weeks</code> for time intervals. Use <code>set_delta_time_unit()</code> to update all related prompts automatically.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#25-genetic-data-formatting","title":"2.5 Genetic Data Formatting\u00b6","text":"<p>Control how genetic/molecular data is tagged and displayed.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#26-forecasting-prompts-value-prediction","title":"2.6 Forecasting Prompts (Value Prediction)\u00b6","text":"<p>These settings control the task prompts for predicting future values.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#27-time-to-event-prompts-survival-analysis","title":"2.7 Time-to-Event Prompts (Survival Analysis)\u00b6","text":"<p>These settings control the task prompts for predicting whether events occur within a time horizon.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#28-qabinning-prompts","title":"2.8 QA/Binning Prompts\u00b6","text":"<p>These settings control the quality assurance task that predicts value bins.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#29-multi-task-prompts","title":"2.9 Multi-Task Prompts\u00b6","text":"<p>When multiple tasks are combined in one prompt, these settings control the formatting.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#210-event-category-overrides","title":"2.10 Event Category Overrides\u00b6","text":"<p>For fine-grained control, you can override how specific event categories or individual events are rendered.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#211-additional-text-formatting-options","title":"2.11 Additional Text Formatting Options\u00b6","text":""},{"location":"examples/advanced/custom_output/customizing_text_generation/#part-3-generate-output-with-custom-text","title":"Part 3: Generate Output with Custom Text\u00b6","text":"<p>Now let's set up the pipeline with our customized config and see the difference.</p>"},{"location":"examples/advanced/custom_output/customizing_text_generation/#part-4-quick-reference-all-text-configuration-options","title":"Part 4: Quick Reference - All Text Configuration Options\u00b6","text":"<p>Here's a comprehensive table of all text customization options available in <code>Config</code>:</p> Setting Description Default Patient Record Introduction <code>preamble_text</code> Opening text introducing the patient record \"The following is a patient...\" <code>constant_text</code> Text introducing demographics section \"\\n\\nStarting with demographic data:\\n\" Visit/Event Day Text <code>first_day_text</code> Text for the first visit \"\\nOn the first visit...\" <code>event_day_preamble</code> Preamble before subsequent visits \"\\n\" <code>event_day_text</code> Text for subsequent visits with time delta \" weeks later...\" <code>post_event_text</code> Text after listing day's events \".\\n\" Time Units <code>delta_time_unit</code> Time unit for intervals \"weeks\" <code>forecasting_prompt_var_time</code> Time description in forecasting \" the future weeks \" Genetic Data <code>genetic_tag_opening</code> Opening tag for genetic data \"\" <code>genetic_tag_closing</code> Closing tag for genetic data \"\" <code>genetic_empty_text</code> Text when no genetic data \"No genetic data available.\" <code>genetic_skip_text_value</code> Value to skip in genetic events \"present\" Forecasting Task Prompts <code>forecasting_fval_prompt_start</code> Main forecasting task introduction \"\\nYour task is to predict...\" <code>forecasting_prompt_summarized_start</code> Last values summary intro \"\\nThe last values...\" <code>forecasting_firstday_override</code> Text when first day truncated \"\\nThe following events...\" <code>forecasting_prompt_summarized_genetic</code> Genetic summary intro \"\\n\\n\\nHere we repeat...\" <code>forecasting_prompt_summarized_lot</code> Treatment line summary intro \"\\nThe most recent line...\" Time-to-Event Prompts <code>forecasting_tte_prompt_start</code> TTE task introduction \"\\nYour task is to predict...\" <code>forecasting_tte_prompt_mid</code> TTE time horizon text \" weeks from...\" <code>forecasting_tte_prompt_end</code> TTE format instructions \".\\nPlease provide...\" <code>target_prompt_start</code> TTE answer format start \"\\nHere is the prediction...\" <code>target_prompt_censor_true</code> Text for censored events \"censored.\" <code>target_prompt_censor_false</code> Text for non-censored events \"not censored \" <code>target_prompt_before_occur</code> Conjunction before occurrence \"and \" <code>target_prompt_occur</code> Text for occurred events \"occurred.\" <code>target_prompt_not_occur</code> Text for non-occurred events \"did not occur.\" QA/Binning Prompts <code>qa_prompt_start</code> QA task introduction \"\\nYour task is to predict...\" <code>qa_bins_start</code> Bins list introduction \"\\tThe possible bins are: \" Multi-Task Prompts <code>task_prompt_start</code> Multi-task section intro \"\\nYou will now have...\" <code>task_prompt_each_task</code> Template for each task \"Task {task_nr} is \" <code>task_prompt_end</code> End of task prompts \"\" <code>task_prompt_forecasting</code> Forecasting task label \"forecasting:\" <code>task_prompt_forecasting_qa</code> QA task label \"forecasting QA:\" <code>task_prompt_events</code> Events task label \"time to event prediction:\" <code>task_prompt_custom</code> Custom task label \" a custom task:\" <code>task_target_start</code> Multi-task answer format \"Task {task_nr} is \" <code>task_target_end</code> End of task answer \"\" Overrides <code>event_category_preamble_mapping_override</code> Custom preambles per category None <code>event_category_and_name_replace_override</code> Custom rendering per event None <code>decimal_precision</code> Decimal places for numbers 2"},{"location":"examples/advanced/custom_output/customizing_text_generation/#summary","title":"Summary\u00b6","text":"<p>This tutorial demonstrated how to customize every aspect of TwinWeaver's text generation through the <code>Config</code> class. Key takeaways:</p> <ol> <li>Preamble and introduction text sets the context for the patient record</li> <li>Event day formatting controls how clinical visits are described temporally</li> <li>Time units can be switched between days and weeks using <code>set_delta_time_unit()</code></li> <li>Genetic data formatting uses customizable tags and placeholder text</li> <li>Task prompts (forecasting, TTE, QA) can be fully rewritten for different LLM styles</li> <li>Multi-task formatting allows structured output for complex prediction tasks</li> <li>Category overrides provide fine-grained control over specific event types</li> </ol> <p>Use these customization options to:</p> <ul> <li>Adapt prompts for different language models</li> <li>Translate prompts to other languages</li> <li>Add visual formatting (emojis, separators) for clarity</li> <li>Match specific institutional or research requirements</li> </ul>"},{"location":"examples/advanced/custom_splitting/inference_individual_splitters/","title":"Inference individual splitters","text":"In\u00a0[\u00a0]: Copied! <pre>from twinweaver import (\n    DataSplitterForecasting,\n    DataManager,\n    DataSplitterEvents,\n    ConverterInstruction,\n    Config,\n)\nimport pandas as pd\n</pre> from twinweaver import (     DataSplitterForecasting,     DataManager,     DataSplitterEvents,     ConverterInstruction,     Config, ) import pandas as pd In\u00a0[\u00a0]: Copied! <pre>class ConvertToText:\n    def __init__(\n        self,\n    ):\n        # Set splitting and predictions\n        self.config = Config()\n        self.config.split_event_category = \"lot\"\n        self.config.event_category_forecast = [\"lab\"]\n        self.config.data_splitter_events_variables_category_mapping = {\n            \"death\": \"death\",\n            \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\"\n        }\n\n        # Set constant\n        self.config.constant_columns_to_use = [\n            \"birthyear\",\n            \"gender\",\n            \"histology\",\n            \"smoking_history\",\n        ]  # Manually set from constant\n        self.config.constant_birthdate_column = \"birthyear\"\n\n        # Load data\n        df_events = pd.read_csv(\"./examples/example_data/events.csv\")\n        df_constant = pd.read_csv(\"./examples/example_data/constant.csv\")\n        df_constant_description = pd.read_csv(\"./examples/example_data/constant_description.csv\")\n\n        # Init data managers\n        self.dm = DataManager(config=self.config)\n        self.dm.load_indication_data(\n            df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description\n        )\n        self.dm.process_indication_data()\n        self.dm.setup_unique_mapping_of_events()\n        self.dm.setup_dataset_splits()\n        self.dm.infer_var_types()\n\n        self.data_splitter_events = DataSplitterEvents(self.dm, config=self.config)\n        self.data_splitter_events.setup_variables()\n        self.data_splitter_forecasting = DataSplitterForecasting(data_manager=self.dm, config=self.config)\n        self.data_splitter_forecasting.setup_statistics()\n        self.converter = ConverterInstruction(\n            nr_tokens_budget_total=8192,\n            config=self.config,\n            dm=self.dm,\n        )\n\n    def convert_full_to_string_for_one_patient(self, patientid, override_events_or_forecasting=\"forecasting\"):\n        patient_data = self.dm.get_patient_data(patientid)\n        patient_data[\"events\"] = patient_data[\"events\"].sort_values(\"date\")\n\n        # To simulate that we only have input, half the events\n        patient_data[\"events\"] = patient_data[\"events\"].iloc[: int(len(patient_data[\"events\"]) / 2)]\n\n        # Here then split date\n        split_date = patient_data[\"events\"][\"date\"].iloc[-1]\n\n        #: generate event split - NOTE: this if statement is only to exemplify both cases!\n        if override_events_or_forecasting == \"events\":\n            ####### Example if we want to override for events\n\n            events_splits = self.data_splitter_events.get_splits_from_patient(\n                patient_data,\n                max_nr_samples=1,\n                override_split_dates=[split_date],\n                override_category=\"death\",\n                override_end_week_delta=52,\n            )\n            # We just pick the first one\n            events_split = events_splits[0][0]\n\n            #: no forecasting split\n            forecast_split = None\n            forecasting_times_to_predict = None\n        else:\n            ####### Example if we want to override for forecasting\n\n            #: generate forecasting split\n            forecast_splits = self.data_splitter_forecasting.get_splits_from_patient(\n                patient_data,\n                nr_samples_per_split=1,\n                filter_outliers=False,\n                override_split_dates=[split_date],\n                override_variables_to_predict=[\"Neutrophils\"],\n            )\n            # We just pick the first one\n            forecast_split = forecast_splits[0][0]\n\n            # We set which weeks to predict\n            forecasting_times_to_predict = {\n                \"Neutrophils\": [1, 2, 8, 11],\n            }\n\n            #: no events split\n            events_split = None\n\n        # Convert to text\n        converted = self.converter.forward_conversion_inference(\n            forecasting_split=forecast_split,\n            forecasting_future_weeks_per_variable=forecasting_times_to_predict,\n            event_split=events_split,\n            custom_tasks=None,\n        )\n        return converted\n</pre> class ConvertToText:     def __init__(         self,     ):         # Set splitting and predictions         self.config = Config()         self.config.split_event_category = \"lot\"         self.config.event_category_forecast = [\"lab\"]         self.config.data_splitter_events_variables_category_mapping = {             \"death\": \"death\",             \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\"         }          # Set constant         self.config.constant_columns_to_use = [             \"birthyear\",             \"gender\",             \"histology\",             \"smoking_history\",         ]  # Manually set from constant         self.config.constant_birthdate_column = \"birthyear\"          # Load data         df_events = pd.read_csv(\"./examples/example_data/events.csv\")         df_constant = pd.read_csv(\"./examples/example_data/constant.csv\")         df_constant_description = pd.read_csv(\"./examples/example_data/constant_description.csv\")          # Init data managers         self.dm = DataManager(config=self.config)         self.dm.load_indication_data(             df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description         )         self.dm.process_indication_data()         self.dm.setup_unique_mapping_of_events()         self.dm.setup_dataset_splits()         self.dm.infer_var_types()          self.data_splitter_events = DataSplitterEvents(self.dm, config=self.config)         self.data_splitter_events.setup_variables()         self.data_splitter_forecasting = DataSplitterForecasting(data_manager=self.dm, config=self.config)         self.data_splitter_forecasting.setup_statistics()         self.converter = ConverterInstruction(             nr_tokens_budget_total=8192,             config=self.config,             dm=self.dm,         )      def convert_full_to_string_for_one_patient(self, patientid, override_events_or_forecasting=\"forecasting\"):         patient_data = self.dm.get_patient_data(patientid)         patient_data[\"events\"] = patient_data[\"events\"].sort_values(\"date\")          # To simulate that we only have input, half the events         patient_data[\"events\"] = patient_data[\"events\"].iloc[: int(len(patient_data[\"events\"]) / 2)]          # Here then split date         split_date = patient_data[\"events\"][\"date\"].iloc[-1]          #: generate event split - NOTE: this if statement is only to exemplify both cases!         if override_events_or_forecasting == \"events\":             ####### Example if we want to override for events              events_splits = self.data_splitter_events.get_splits_from_patient(                 patient_data,                 max_nr_samples=1,                 override_split_dates=[split_date],                 override_category=\"death\",                 override_end_week_delta=52,             )             # We just pick the first one             events_split = events_splits[0][0]              #: no forecasting split             forecast_split = None             forecasting_times_to_predict = None         else:             ####### Example if we want to override for forecasting              #: generate forecasting split             forecast_splits = self.data_splitter_forecasting.get_splits_from_patient(                 patient_data,                 nr_samples_per_split=1,                 filter_outliers=False,                 override_split_dates=[split_date],                 override_variables_to_predict=[\"Neutrophils\"],             )             # We just pick the first one             forecast_split = forecast_splits[0][0]              # We set which weeks to predict             forecasting_times_to_predict = {                 \"Neutrophils\": [1, 2, 8, 11],             }              #: no events split             events_split = None          # Convert to text         converted = self.converter.forward_conversion_inference(             forecasting_split=forecast_split,             forecasting_future_weeks_per_variable=forecasting_times_to_predict,             event_split=events_split,             custom_tasks=None,         )         return converted In\u00a0[\u00a0]: Copied! <pre>################################### Running the example #######################################\nconverter = ConvertToText()\n</pre> ################################### Running the example ####################################### converter = ConvertToText() <p>Example on how to run conversion for inference (i.e. we do not have target) Here we predict 52 week survival (as an event), and no forecasting</p> <p>NOTE: run this from the root folder of twinweaver</p> In\u00a0[\u00a0]: Copied! <pre>all_patientids = converter.dm.all_patientids.copy()\nall_patientids = all_patientids[:10]\n</pre> all_patientids = converter.dm.all_patientids.copy() all_patientids = all_patientids[:10] In\u00a0[\u00a0]: Copied! <pre>for idx, patientid in enumerate(all_patientids):\n    print(idx)\n\n    #: go through all patients and convert them\n    patient_data = converter.convert_full_to_string_for_one_patient(\n        patientid, override_events_or_forecasting=\"forecasting\"\n    )\n    print(patient_data[\"instruction\"])\n</pre> for idx, patientid in enumerate(all_patientids):     print(idx)      #: go through all patients and convert them     patient_data = converter.convert_full_to_string_for_one_patient(         patientid, override_events_or_forecasting=\"forecasting\"     )     print(patient_data[\"instruction\"]) In\u00a0[\u00a0]: Copied! <pre>print(\"Finished\")\n</pre> print(\"Finished\")"},{"location":"examples/advanced/custom_splitting/training_custom_split_events/","title":"Example for custom split and forecasting events","text":"<p>This notebook demonstrates how to adjust the splitters to split at custom events, as well to forecast different categories (rather than the default labs).</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n    DataSplitterForecasting,\n    DataSplitterEvents,\n    ConverterInstruction,\n    DataSplitter,\n)\n</pre> import pandas as pd  from twinweaver import (     DataManager,     Config,     DataSplitterForecasting,     DataSplitterEvents,     ConverterInstruction,     DataSplitter, ) In\u00a0[\u00a0]: Copied! <pre># Load data - generated example data\ndf_events = pd.read_csv(\"../../example_data/events.csv\")\ndf_constant = pd.read_csv(\"../../example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\")\n</pre> # Load data - generated example data df_events = pd.read_csv(\"../../example_data/events.csv\") df_constant = pd.read_csv(\"../../example_data/constant.csv\") df_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\") In\u00a0[\u00a0]: Copied! <pre>config = Config()  # Override values here to customize pipeline\nconfig.constant_columns_to_use = [\n    \"birthyear\",\n    \"gender\",\n    \"histology\",\n    \"smoking_history\",\n]  # Manually set from constant DF\nconfig.constant_birthdate_column = \"birthyear\"\n\n# &lt;---------------------- IMPORTANT PARTS ----------------------------&gt;\n\n\n# To setup the different split events, we set this in the config\n# In this example, we use genetic events as custom split events\nconfig.split_event_category = \"basic_biomarker\"\n\n\n# And to forecast different categories, we set this in the config as well\n# In this example, lets say we want to forecast vitals (i.e. body weight in the example data)\nconfig.event_category_forecast = [\"vitals\"]\n\n# To predict different variables for the event categories, we set up a mapping here\nconfig.data_splitter_events_variables_category_mapping = {\n    \"lot\": \"time to next lot\",  # Custom name in prompt: \"time to next lot\" instead of \"lot\"\n}\n</pre> config = Config()  # Override values here to customize pipeline config.constant_columns_to_use = [     \"birthyear\",     \"gender\",     \"histology\",     \"smoking_history\", ]  # Manually set from constant DF config.constant_birthdate_column = \"birthyear\"  # &lt;---------------------- IMPORTANT PARTS ----------------------------&gt;   # To setup the different split events, we set this in the config # In this example, we use genetic events as custom split events config.split_event_category = \"basic_biomarker\"   # And to forecast different categories, we set this in the config as well # In this example, lets say we want to forecast vitals (i.e. body weight in the example data) config.event_category_forecast = [\"vitals\"]  # To predict different variables for the event categories, we set up a mapping here config.data_splitter_events_variables_category_mapping = {     \"lot\": \"time to next lot\",  # Custom name in prompt: \"time to next lot\" instead of \"lot\" } In\u00a0[\u00a0]: Copied! <pre># Setup the data manager\ndm = DataManager(config=config)\ndm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\ndm.infer_var_types()\n</pre> # Setup the data manager dm = DataManager(config=config) dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description) dm.process_indication_data() dm.setup_unique_mapping_of_events() dm.setup_dataset_splits() dm.infer_var_types() In\u00a0[\u00a0]: Copied! <pre># This data splitter handles event prediction tasks\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\n\n# This data splitter handles forecasting tasks\ndata_splitter_forecasting = DataSplitterForecasting(\n    data_manager=dm,\n    config=config,\n)\n# If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step\ndata_splitter_forecasting.setup_statistics()\n\n# We will also use the easier interface that combines both data splitters\ndata_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n\n# Set up the converter instruction\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config,\n    dm=dm,\n    variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks\n)\n</pre> # This data splitter handles event prediction tasks data_splitter_events = DataSplitterEvents(dm, config=config) data_splitter_events.setup_variables()  # This data splitter handles forecasting tasks data_splitter_forecasting = DataSplitterForecasting(     data_manager=dm,     config=config, ) # If you don't want to do forecasting QA, proportional sampling, or 3-sigma filtering, you can skip this step data_splitter_forecasting.setup_statistics()  # We will also use the easier interface that combines both data splitters data_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)  # Set up the converter instruction converter = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config,     dm=dm,     variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks ) <p>From the data manager we can get the patient, for example this patientid.</p> In\u00a0[\u00a0]: Copied! <pre>patientid = dm.all_patientids[4]\npatient_data = dm.get_patient_data(patientid)\n</pre> patientid = dm.all_patientids[4] patient_data = dm.get_patient_data(patientid) In\u00a0[\u00a0]: Copied! <pre>forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(\n    patient_data,\n)\n</pre> forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(     patient_data, ) <p>Now for each split, we can generate these strings. We just pick the first one as an example.</p> In\u00a0[\u00a0]: Copied! <pre>split_idx = 0\np_converted = converter.forward_conversion(\n    forecasting_splits=forecasting_splits[split_idx],\n    event_splits=events_splits[split_idx],\n    override_mode_to_select_forecasting=\"both\",\n)\n</pre> split_idx = 0 p_converted = converter.forward_conversion(     forecasting_splits=forecasting_splits[split_idx],     event_splits=events_splits[split_idx],     override_mode_to_select_forecasting=\"both\", ) In\u00a0[\u00a0]: Copied! <pre>forecasting_splits[0]\n</pre> forecasting_splits[0] In\u00a0[\u00a0]: Copied! <pre>print(p_converted[\"instruction\"])\n</pre> print(p_converted[\"instruction\"]) In\u00a0[\u00a0]: Copied! <pre>print(p_converted[\"answer\"])\n</pre> print(p_converted[\"answer\"]) In\u00a0[\u00a0]: Copied! <pre>date = reference_dates[\"date\"][0]\nreturn_list = converter.reverse_conversion(p_converted[\"answer\"], dm, date)\nreturn_list[0][\"result\"]\n</pre> date = reference_dates[\"date\"][0] return_list = converter.reverse_conversion(p_converted[\"answer\"], dm, date) return_list[0][\"result\"]"},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#example-for-custom-split-and-forecasting-events","title":"Example for custom split and forecasting events\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#basic-setup","title":"Basic Setup\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#load-data","title":"Load Data\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#configuration-and-data-manager","title":"Configuration and Data Manager\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#initialize-splitters-and-converter","title":"Initialize Splitters and Converter\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#examine-patient-data","title":"Examine patient data\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#convert-patient-data-to-string","title":"Convert patient data to string\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#generate-training-splits","title":"Generate Training Splits\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#inspect-the-output","title":"Inspect the Output\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_custom_split_events/#reverse-conversion-text-to-structured-data","title":"Reverse Conversion: Text to Structured Data\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_individual_splitters/","title":"Example for single patient to convert using the instruction setup with custom dataset","text":"<p>Start by loading in all libraries</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom twinweaver import (\n    DataSplitterForecasting,\n    DataManager,\n    DataSplitterEvents,\n    ConverterInstruction,\n    Config,\n)\n</pre> import pandas as pd  from twinweaver import (     DataSplitterForecasting,     DataManager,     DataSplitterEvents,     ConverterInstruction,     Config, ) <p>Set up the config - showing how to use custom dataset here from example data.</p> In\u00a0[\u00a0]: Copied! <pre>df_events = pd.read_csv(\"../../example_data/events.csv\")\ndf_constant = pd.read_csv(\"../../example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\")\n</pre> df_events = pd.read_csv(\"../../example_data/events.csv\") df_constant = pd.read_csv(\"../../example_data/constant.csv\") df_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\") <p>Set up the data managers which hold the patient data.</p> In\u00a0[\u00a0]: Copied! <pre>config = Config()  # Override values here to customize pipeline\n\n# &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt;\n# 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot')\n# Has to be set for all instruction tasks\nconfig.split_event_category = \"lot\"\n\n# 2. List of event categories we want to forecast (e.g., forecasting 'lab' values)\n# Only needs to be set if you want to forecast variables\nconfig.event_category_forecast = [\"lab\"]\n\n# 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression')\n# Only needs to be set if you want to do time to event prediction\nconfig.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\"\n}\n\n# Constant setup\nconfig.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]  # Manually set from constant\nconfig.constant_birthdate_column = \"birthyear\"\n</pre> config = Config()  # Override values here to customize pipeline  # &lt;---------------------- CRITICAL CONFIGURATION ----------------------&gt; # 1. Event category used for data splitting (e.g., split data around Lines of Therapy 'lot') # Has to be set for all instruction tasks config.split_event_category = \"lot\"  # 2. List of event categories we want to forecast (e.g., forecasting 'lab' values) # Only needs to be set if you want to forecast variables config.event_category_forecast = [\"lab\"]  # 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression') # Only needs to be set if you want to do time to event prediction config.data_splitter_events_variables_category_mapping = {     \"death\": \"death\",     \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\" }  # Constant setup config.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]  # Manually set from constant config.constant_birthdate_column = \"birthyear\" In\u00a0[\u00a0]: Copied! <pre>dm = DataManager(config=config)\ndm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\ndm.infer_var_types()\n\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\n\ndata_splitter_forecasting = DataSplitterForecasting(\n    data_manager=dm,\n    config=config,\n)\n# In case you manually want to override the variables for forecasting selectiong, you can skip this next line.\ndata_splitter_forecasting.setup_statistics()\n\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config,\n    dm=dm,\n    variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks\n)\n</pre> dm = DataManager(config=config) dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description) dm.process_indication_data() dm.setup_unique_mapping_of_events() dm.setup_dataset_splits() dm.infer_var_types()  data_splitter_events = DataSplitterEvents(dm, config=config) data_splitter_events.setup_variables()  data_splitter_forecasting = DataSplitterForecasting(     data_manager=dm,     config=config, ) # In case you manually want to override the variables for forecasting selectiong, you can skip this next line. data_splitter_forecasting.setup_statistics()  converter = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config,     dm=dm,     variable_stats=data_splitter_forecasting.variable_stats,  # Optional, needed for forecasting QA tasks ) <p>From the data manager we can get the patient, for example the first patientid.</p> In\u00a0[\u00a0]: Copied! <pre>patientid = dm.all_patientids[2]\npatientid\n</pre> patientid = dm.all_patientids[2] patientid <p>Let's checkout the data of the patient. <code>patient_data</code> is a dictionary containing the patient's data, with two keys:</p> <ul> <li>\"events\": A pandas DataFrame containing all time-series events (original events and molecular data combined and sorted by date).</li> <li>\"constant\": A pandas DataFrame containing the static (constant) data for the patient.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>patient_data = dm.get_patient_data(patientid)\npatient_data[\"events\"].head(20)\n</pre> patient_data = dm.get_patient_data(patientid) patient_data[\"events\"].head(20) In\u00a0[\u00a0]: Copied! <pre>patient_data[\"constant\"]\n</pre> patient_data[\"constant\"] <p>We start by generating random \"splits\" in the patient trajectory. We can make multiple relevant samples from each patient trajectory (e.g. depending on when the therapy started), and also to predict different variables (e.g. neutrophils/hemoglobin/... for forecasting, death/progression/metastases/next treatment for event).</p> <p>Here we generate these random splits. We can also manually override them (see other examples on inference).</p> In\u00a0[\u00a0]: Copied! <pre>processed_splits_fc, split_dates = data_splitter_forecasting.get_splits_from_patient(\n    patient_data,\n    nr_samples_per_split=4,\n    filter_outliers=False,\n    include_metadata=True,\n    max_num_splits_per_split_event=2,\n)\n\nprocessed_splits_ev = data_splitter_events.get_splits_from_patient(\n    patient_data,\n    reference_split_dates=split_dates,\n    max_nr_samples_per_split=3,\n)\n</pre> processed_splits_fc, split_dates = data_splitter_forecasting.get_splits_from_patient(     patient_data,     nr_samples_per_split=4,     filter_outliers=False,     include_metadata=True,     max_num_splits_per_split_event=2, )  processed_splits_ev = data_splitter_events.get_splits_from_patient(     patient_data,     reference_split_dates=split_dates,     max_nr_samples_per_split=3, ) <p>Now for each split, we can generate these strings.</p> In\u00a0[\u00a0]: Copied! <pre>split_idx = 0\np_converted = converter.forward_conversion(\n    forecasting_splits=processed_splits_fc[split_idx],\n    event_splits=processed_splits_ev[split_idx],\n    override_mode_to_select_forecasting=\"forecasting_qa\",\n)\n</pre> split_idx = 0 p_converted = converter.forward_conversion(     forecasting_splits=processed_splits_fc[split_idx],     event_splits=processed_splits_ev[split_idx],     override_mode_to_select_forecasting=\"forecasting_qa\", ) <p><code>p_converted</code> is a dictionary containing the final formatted data:</p> <ul> <li>'instruction': The complete input string for the model (context + multi-task prompt).</li> <li>'answer': The complete target string for the model (multi-task answer).</li> <li>'meta': A dictionary holding metadata including patient ID, structured constant and history data used, split date, combined metadata from sub-converters, and a list of detailed metadata for each individual task generated ('target_meta_detailed').</li> </ul> In\u00a0[\u00a0]: Copied! <pre>print(p_converted[\"instruction\"])\n</pre> print(p_converted[\"instruction\"]) In\u00a0[\u00a0]: Copied! <pre>print(p_converted[\"answer\"])\n</pre> print(p_converted[\"answer\"]) In\u00a0[\u00a0]: Copied! <pre>date = split_dates[\"date\"][0]\nreturn_list = converter.reverse_conversion(p_converted[\"answer\"], dm, date)\nreturn_list[2][\"result\"]\n</pre> date = split_dates[\"date\"][0] return_list = converter.reverse_conversion(p_converted[\"answer\"], dm, date) return_list[2][\"result\"]"},{"location":"examples/advanced/custom_splitting/training_individual_splitters/#example-for-single-patient-to-convert-using-the-instruction-setup-with-custom-dataset","title":"Example for single patient to convert using the instruction setup with custom dataset\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_individual_splitters/#basic-setup","title":"Basic Setup\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_individual_splitters/#examine-patient-data","title":"Examine patient data\u00b6","text":""},{"location":"examples/advanced/custom_splitting/training_individual_splitters/#convert-patient-data-to-string","title":"Convert patient data to string\u00b6","text":""},{"location":"examples/advanced/pretraining/end_to_end_llm_training_with_pretrain/","title":"End to end instruction example using LLMs with fine-tuning","text":"In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoTokenizer\nimport pandas as pd\nimport gc\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    pipeline,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer, SFTConfig\n\n\nfrom twinweaver import DataManager, Config, ConverterPretrain\n</pre> from transformers import AutoTokenizer import pandas as pd import gc import torch from datasets import Dataset from transformers import (     AutoModelForCausalLM,     BitsAndBytesConfig,     pipeline, ) from peft import LoraConfig, PeftModel from trl import SFTTrainer, SFTConfig   from twinweaver import DataManager, Config, ConverterPretrain In\u00a0[\u00a0]: Copied! <pre># Some key settings\nBASE_MODEL = \"microsoft/Phi-4-mini-instruct\"  # NOTE: we haven't tested the performance of this model beyond examples\n</pre> # Some key settings BASE_MODEL = \"microsoft/Phi-4-mini-instruct\"  # NOTE: we haven't tested the performance of this model beyond examples In\u00a0[\u00a0]: Copied! <pre># Load data\ndf_events = pd.read_csv(\"../../example_data/events.csv\")\ndf_constant = pd.read_csv(\"../../example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\")\n\n# Manually set up which constant columns we want to use\nconfig = Config()  # Override values here to customize pipeline\nconfig.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"]\nconfig.constant_birthdate_column = \"birthyear\"\n</pre> # Load data df_events = pd.read_csv(\"../../example_data/events.csv\") df_constant = pd.read_csv(\"../../example_data/constant.csv\") df_constant_description = pd.read_csv(\"../../example_data/constant_description.csv\")  # Manually set up which constant columns we want to use config = Config()  # Override values here to customize pipeline config.constant_columns_to_use = [\"birthyear\", \"gender\", \"histology\", \"smoking_history\"] config.constant_birthdate_column = \"birthyear\" In\u00a0[\u00a0]: Copied! <pre>dm = DataManager(config=config)\ndm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\ndm.infer_var_types()\n\nconverter = ConverterPretrain(config=config, dm=dm)\n</pre> dm = DataManager(config=config) dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description) dm.process_indication_data() dm.setup_unique_mapping_of_events() dm.setup_dataset_splits() dm.infer_var_types()  converter = ConverterPretrain(config=config, dm=dm) In\u00a0[\u00a0]: Copied! <pre># Get all training + validation patientids\ntraining_patientids = dm.get_all_patientids_in_split(config.train_split_name)\nvalidation_patientids = dm.get_all_patientids_in_split(config.validation_split_name)\n</pre> # Get all training + validation patientids training_patientids = dm.get_all_patientids_in_split(config.train_split_name) validation_patientids = dm.get_all_patientids_in_split(config.validation_split_name) <p>The <code>generate_transformers_df</code> function iterates through each patient and generates the text data.</p> In\u00a0[\u00a0]: Copied! <pre>def generate_transformers_df(patientids_list):\n    df = []\n\n    for patientid in patientids_list:\n        patient_data = dm.get_patient_data(patientid)\n\n        p_converted = converter.forward_conversion(events=patient_data[\"events\"], constant=patient_data[\"constant\"])\n        new_data = {\n            \"text\": p_converted[\"text\"],\n            \"patientid\": f\"{patientid}\",  # Just for ease of finding later\n        }\n        df.append(new_data)\n\n    df = pd.DataFrame(df)\n    return df\n</pre> def generate_transformers_df(patientids_list):     df = []      for patientid in patientids_list:         patient_data = dm.get_patient_data(patientid)          p_converted = converter.forward_conversion(events=patient_data[\"events\"], constant=patient_data[\"constant\"])         new_data = {             \"text\": p_converted[\"text\"],             \"patientid\": f\"{patientid}\",  # Just for ease of finding later         }         df.append(new_data)      df = pd.DataFrame(df)     return df In\u00a0[\u00a0]: Copied! <pre># Generate training and validation dfs\ndf_train = generate_transformers_df(training_patientids)\ndf_validation = generate_transformers_df(validation_patientids)\n</pre> # Generate training and validation dfs df_train = generate_transformers_df(training_patientids) df_validation = generate_transformers_df(validation_patientids) In\u00a0[\u00a0]: Copied! <pre>df_train.head()\n</pre> df_train.head() <p>We start by setting up the tokenizer. We set the padding token to be the same as the EOS (End of Sequence) token, which is a common practice for causal language models.</p> In\u00a0[\u00a0]: Copied! <pre># Setup tokenizer and datasets\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\n# Set padding token to eos_token\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ntrain_dataset = Dataset.from_pandas(df_train)\nvalidation_dataset = Dataset.from_pandas(df_validation)\n</pre> # Setup tokenizer and datasets tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)  # Set padding token to eos_token tokenizer.pad_token = tokenizer.eos_token tokenizer.pad_token_id = tokenizer.eos_token_id  train_dataset = Dataset.from_pandas(df_train) validation_dataset = Dataset.from_pandas(df_validation) <p>Instruction-tuned models expect data in a specific conversational format (e.g., User: ... Assistant: ...). We use <code>format_chat_template</code> to structure our raw prompt/completion strings into this list-of-messages format using the <code>user</code> and <code>assistant</code> roles.</p> In\u00a0[\u00a0]: Copied! <pre># Format data for chat template\ndef format_chat_template(example):\n    \"\"\"Convert prompt/completion pairs to proper prompt/completion format\"\"\"\n    return {\n        \"text\": example[\"text\"],\n    }\n\n\n# Apply formatting to datasets\ntrain_dataset = train_dataset.map(format_chat_template)\nvalidation_dataset = validation_dataset.map(format_chat_template)\n</pre> # Format data for chat template def format_chat_template(example):     \"\"\"Convert prompt/completion pairs to proper prompt/completion format\"\"\"     return {         \"text\": example[\"text\"],     }   # Apply formatting to datasets train_dataset = train_dataset.map(format_chat_template) validation_dataset = validation_dataset.map(format_chat_template) <p>We configure 4-bit quantization using <code>BitsAndBytesConfig</code> (QLoRA). This significantly lowers memory usage, allowing us to fine-tune the model on consumer GPUs.</p> In\u00a0[\u00a0]: Copied! <pre># Define Quantization Config (4-bit loading)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,  # This should be set based on your GPU capabilities\n    bnb_4bit_use_double_quant=True,\n)\n</pre> # Define Quantization Config (4-bit loading) bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.bfloat16,  # This should be set based on your GPU capabilities     bnb_4bit_use_double_quant=True, ) <p>Here we set up Low-Rank Adaptation (LoRA) configuration. <code>LoraConfig</code> defines the adapter parameters (rank <code>r</code>, <code>alpha</code>). we target linear layers (<code>q_proj</code>, <code>k_proj</code> etc.) which generally yields better results than just attending to query/value projections.</p> In\u00a0[\u00a0]: Copied! <pre>peft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=8,  # Rank (higher = more parameters to train)\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    # Target all linear layers for best performance (specific to Llama architecture)\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n)\n</pre> peft_config = LoraConfig(     lora_alpha=16,     lora_dropout=0.1,     r=8,  # Rank (higher = more parameters to train)     bias=\"none\",     task_type=\"CAUSAL_LM\",     # Target all linear layers for best performance (specific to Llama architecture)     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], ) <p>We define the training arguments in <code>SFTConfig</code>. Notice the higher learning rate (<code>1e-4</code>) compared to typical full fine-tuning in the GDT paper. We also set <code>bf16=True</code> for newer GPUs (Ampere+) to improve training stability.</p> In\u00a0[\u00a0]: Copied! <pre>training_arguments = SFTConfig(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=10,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    per_device_eval_batch_size=1,\n    learning_rate=1e-4,  # LR is higher for PEFT, see TwinWeaver paper for full fine-tuning details\n    fp16=False,  # Use fp16 for older GPUs T4/V100, bf16 for Ampere and later (A100/3090/4090)\n    bf16=True,\n    max_grad_norm=1.0,\n    warmup_ratio=0.1,\n    group_by_length=True,\n    save_total_limit=1,\n    lr_scheduler_type=\"cosine\",\n    max_length=8192,\n    packing=False,  # Disable packing for more exact training, though can be activated\n    completion_only_loss=False,  # Compute loss on entire text\n)\n</pre> training_arguments = SFTConfig(     output_dir=\"./results\",     num_train_epochs=5,     per_device_train_batch_size=1,     gradient_accumulation_steps=1,     optim=\"paged_adamw_32bit\",     save_steps=10,     logging_steps=10,     eval_strategy=\"steps\",     eval_steps=10,     per_device_eval_batch_size=1,     learning_rate=1e-4,  # LR is higher for PEFT, see TwinWeaver paper for full fine-tuning details     fp16=False,  # Use fp16 for older GPUs T4/V100, bf16 for Ampere and later (A100/3090/4090)     bf16=True,     max_grad_norm=1.0,     warmup_ratio=0.1,     group_by_length=True,     save_total_limit=1,     lr_scheduler_type=\"cosine\",     max_length=8192,     packing=False,  # Disable packing for more exact training, though can be activated     completion_only_loss=False,  # Compute loss on entire text ) In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=False,\n)\n\n# Disable cache for training (required for gradient checkpointing)\nmodel.config.use_cache = False\n</pre> model = AutoModelForCausalLM.from_pretrained(     BASE_MODEL,     quantization_config=bnb_config,     device_map=\"auto\",     trust_remote_code=False, )  # Disable cache for training (required for gradient checkpointing) model.config.use_cache = False In\u00a0[\u00a0]: Copied! <pre>trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n    args=training_arguments,\n    eval_dataset=validation_dataset,\n    peft_config=peft_config,\n)\n</pre> trainer = SFTTrainer(     model=model,     train_dataset=train_dataset,     processing_class=tokenizer,     args=training_arguments,     eval_dataset=validation_dataset,     peft_config=peft_config, ) In\u00a0[\u00a0]: Copied! <pre># Start training - takes around 5 mins, depending on hardware\ntrainer.train()\n</pre> # Start training - takes around 5 mins, depending on hardware trainer.train() In\u00a0[\u00a0]: Copied! <pre># Save the fine-tuned adapter\nadapter_path = \"./results/final_adapter\"\ntrainer.save_model(adapter_path)\nprint(f\"Adapter saved to {adapter_path}\")\n\ndel trainer\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\n</pre> # Save the fine-tuned adapter adapter_path = \"./results/final_adapter\" trainer.save_model(adapter_path) print(f\"Adapter saved to {adapter_path}\")  del trainer del model gc.collect() torch.cuda.empty_cache() In\u00a0[\u00a0]: Copied! <pre># Get the first test set patient\ntest_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0]\npatient_data = dm.get_patient_data(test_patientid)\n\n# Lets simulate forecasts for after the first line of therapy\ndf_constant_patient = patient_data[\"constant\"].copy()\ndf_events_patient = patient_data[\"events\"].copy()\ndate_of_first_lot = df_events_patient.loc[\n    df_events_patient[\"event_category\"] == config.event_category_lot, \"date\"\n].min()\ndate_of_first_event = df_events_patient[\"date\"].min()\n\n# Only keep data until (and including) first line of therapy\ndf_events_patient = df_events_patient.loc[df_events_patient[\"date\"] &lt;= date_of_first_lot]\n</pre> # Get the first test set patient test_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0] patient_data = dm.get_patient_data(test_patientid)  # Lets simulate forecasts for after the first line of therapy df_constant_patient = patient_data[\"constant\"].copy() df_events_patient = patient_data[\"events\"].copy() date_of_first_lot = df_events_patient.loc[     df_events_patient[\"event_category\"] == config.event_category_lot, \"date\" ].min() date_of_first_event = df_events_patient[\"date\"].min()  # Only keep data until (and including) first line of therapy df_events_patient = df_events_patient.loc[df_events_patient[\"date\"] &lt;= date_of_first_lot] <p>We convert the patient data into the first part.</p> In\u00a0[\u00a0]: Copied! <pre># Convert to instruction\nconverted = converter.forward_conversion(events=df_events_patient, constant=df_constant_patient)\n</pre> # Convert to instruction converted = converter.forward_conversion(events=df_events_patient, constant=df_constant_patient) <p>For inference, we load the base model again (clean slate) to avoid any state from training, and then attach the adapter we trained. <code>PeftModel</code> handles the integration of the LoRA weights with the base model.</p> <p>For inference, we load the base model again (clean slate) and then attach the adapter we trained. <code>PeftModel</code> handles the integration of the LoRA weights.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Load the Base Model again (clean instance)\nbase_model_inference = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,  # Reuse the 4-bit config\n    device_map=\"auto\",\n    trust_remote_code=False,\n)\n\n# 2. Load the Saved Adapter\n# This wraps the base model with the fine-tuned LoRA layers\ninference_model = PeftModel.from_pretrained(base_model_inference, adapter_path)\n\n# 3. Switch to evaluation mode\ninference_model.eval()\n</pre> # 1. Load the Base Model again (clean instance) base_model_inference = AutoModelForCausalLM.from_pretrained(     BASE_MODEL,     quantization_config=bnb_config,  # Reuse the 4-bit config     device_map=\"auto\",     trust_remote_code=False, )  # 2. Load the Saved Adapter # This wraps the base model with the fine-tuned LoRA layers inference_model = PeftModel.from_pretrained(base_model_inference, adapter_path)  # 3. Switch to evaluation mode inference_model.eval() In\u00a0[\u00a0]: Copied! <pre># Create text generation pipeline\n# Re-enable cache for inference\ninference_model.config.use_cache = True\ntext_gen_pipeline = pipeline(\"text-generation\", model=inference_model, tokenizer=tokenizer)\n</pre> # Create text generation pipeline # Re-enable cache for inference inference_model.config.use_cache = True text_gen_pipeline = pipeline(\"text-generation\", model=inference_model, tokenizer=tokenizer) In\u00a0[\u00a0]: Copied! <pre># Generate with LLM, for a given time\ngenerated_answer = text_gen_pipeline(\n    converted[\"text\"],\n    max_new_tokens=128,  # &lt;------- Set this higher for longer answers, lower for shorter answers\n    return_full_text=False,\n    do_sample=True,  # Using nucleus sampling\n    temperature=0.7,\n    top_p=0.9,\n)[0][\"generated_text\"]\n</pre> # Generate with LLM, for a given time generated_answer = text_gen_pipeline(     converted[\"text\"],     max_new_tokens=128,  # &lt;------- Set this higher for longer answers, lower for shorter answers     return_full_text=False,     do_sample=True,  # Using nucleus sampling     temperature=0.7,     top_p=0.9, )[0][\"generated_text\"] In\u00a0[\u00a0]: Copied! <pre># Show the generated answer\ngenerated_answer\n</pre> # Show the generated answer generated_answer <p>The raw text output from the model needs to be parsed back into structured data. <code>reverse_conversion</code> handles this, returning a dictionary with the data.</p> In\u00a0[\u00a0]: Copied! <pre># Reverse convert\nfull_trajectory = converted[\"text\"] + generated_answer\nret_dict = converter.reverse_conversion(full_trajectory, dm, date_of_first_event)\n</pre> # Reverse convert full_trajectory = converted[\"text\"] + generated_answer ret_dict = converter.reverse_conversion(full_trajectory, dm, date_of_first_event) In\u00a0[\u00a0]: Copied! <pre>ret_dict[\"events\"].head()\n</pre> ret_dict[\"events\"].head()"},{"location":"examples/advanced/pretraining/end_to_end_llm_training_with_pretrain/#end-to-end-instruction-example-using-llms-with-fine-tuning","title":"End to end instruction example using LLMs with fine-tuning\u00b6","text":"<p>This notebook provides an example for how to use the pretrain data. This means that the model is trained on full patient histories, without any specific task. This can be used to develop models that can generate synthetic patients or embeddings.</p> <p>Note: You need a GPU with at least 30GB of memory for this example to work. We also have not tested the performance of PEFT models - only as examples.</p> <p>Important: Please install first the fine-tuning packages with <code>pip install twinweaver[fine-tuning-example]</code>.</p>"},{"location":"examples/advanced/pretraining/end_to_end_llm_training_with_pretrain/#generate-training-data","title":"Generate training data\u00b6","text":""},{"location":"examples/advanced/pretraining/end_to_end_llm_training_with_pretrain/#fine-tune-llm","title":"Fine-tune LLM\u00b6","text":""},{"location":"examples/advanced/pretraining/end_to_end_llm_training_with_pretrain/#inference-example","title":"Inference example\u00b6","text":"<p>Inference example for a test set patient, where we want to generate the full patient trajectory after the first line of therapy.</p>"},{"location":"examples/advanced/pretraining/prepare_pretraining_data/","title":"Prepare pretraining data","text":"In\u00a0[\u00a0]: Copied! <pre>from twinweaver import DataManager, ConverterPretrain, Config\nimport pandas as pd\n</pre> from twinweaver import DataManager, ConverterPretrain, Config import pandas as pd In\u00a0[\u00a0]: Copied! <pre>class ConvertToText:\n    def __init__(self):\n        # Set basics\n        self.config = Config()  # Override values here to customize pipeline\n\n        # Manually set from constant\n        self.config.constant_columns_to_use = [\n            \"birthyear\",\n            \"gender\",\n            \"histology\",\n            \"smoking_history\",\n        ]\n        self.config.constant_birthdate_column = \"birthyear\"\n\n        # Load data\n        df_events = pd.read_csv(\"./examples/example_data/events.csv\")\n        df_constant = pd.read_csv(\"./examples/example_data/constant.csv\")\n        df_constant_description = pd.read_csv(\"./examples/example_data/constant_description.csv\")\n\n        # Init data managers\n        self.dm = DataManager(config=self.config)\n        self.dm.load_indication_data(\n            df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description\n        )\n        self.dm.process_indication_data()\n        self.dm.setup_unique_mapping_of_events()\n        self.dm.setup_dataset_splits()\n\n        #: set up converter\n        self.converter = ConverterPretrain(config=self.config, dm=self.dm)\n\n        # Due to internal data issue, we skip this one when doing reverse checks\n        self.reverse_patient_skip_list = [\"electrophoresis m spike\"]\n\n    def convert_full_to_string_for_one_patient(self, patientid):\n        patient_data = self.dm.get_patient_data(patientid)\n\n        #: convert patient data using ConverterPretrain\n        p_converted = self.converter.forward_conversion(patient_data[\"events\"], patient_data[\"constant\"])\n\n        #: convert extras into JSON using df.to_json\n        internal_meta = p_converted[\"meta\"].copy()\n        internal_meta[\"split\"] = self.dm.get_patient_split(patientid=patientid)\n        p_converted[\"meta\"] = {\n            \"patientid\": patientid,\n            \"split\": self.dm.get_patient_split(patientid=patientid),\n            \"constant\": p_converted[\"meta\"][\"processed_constant\"].to_json(orient=\"split\"),\n        }\n\n        return [(p_converted, internal_meta)]\n\n    def assess_reverse_conversion(self, all_patient_data) -&gt; None:\n        \"\"\"\n        Assesses the reverse conversion for a single patient to ensure data integrity.\n\n        Parameters\n        ----------\n        all_patient_data : list of tuples\n            A list of tuples, each containing converted patient data and internal metadata.\n        \"\"\"\n\n        # Split up\n        converted_data, internal_meta = all_patient_data[0]\n        first_event_date = internal_meta[\"events\"][\"date\"].min()\n\n        # Log that testing patient\n        print(\"Assessing reverse conversion for patient\" + str(converted_data[\"meta\"][\"patientid\"]))\n\n        #: do reverse conversion using ConverterPretrain\n        p_reverse_converted = self.converter.reverse_conversion(\n            converted_data[\"text\"],\n            data_manager=self.dm,\n            init_date=first_event_date,\n        )\n\n        #: check differences appropriately\n        diff = self.converter.get_difference_in_event_dataframes(\n            internal_meta[\"events\"],\n            p_reverse_converted[\"events\"],\n            skip_genetic=True,\n            skip_vals_list=self.reverse_patient_skip_list,\n        )\n\n        #: assert that no differences are found, and print patientid if issues are found\n        assert diff.shape[0] == 0, f\"Patient {internal_meta['patientid']} has differences in reverse conversion: {diff}\"\n</pre> class ConvertToText:     def __init__(self):         # Set basics         self.config = Config()  # Override values here to customize pipeline          # Manually set from constant         self.config.constant_columns_to_use = [             \"birthyear\",             \"gender\",             \"histology\",             \"smoking_history\",         ]         self.config.constant_birthdate_column = \"birthyear\"          # Load data         df_events = pd.read_csv(\"./examples/example_data/events.csv\")         df_constant = pd.read_csv(\"./examples/example_data/constant.csv\")         df_constant_description = pd.read_csv(\"./examples/example_data/constant_description.csv\")          # Init data managers         self.dm = DataManager(config=self.config)         self.dm.load_indication_data(             df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description         )         self.dm.process_indication_data()         self.dm.setup_unique_mapping_of_events()         self.dm.setup_dataset_splits()          #: set up converter         self.converter = ConverterPretrain(config=self.config, dm=self.dm)          # Due to internal data issue, we skip this one when doing reverse checks         self.reverse_patient_skip_list = [\"electrophoresis m spike\"]      def convert_full_to_string_for_one_patient(self, patientid):         patient_data = self.dm.get_patient_data(patientid)          #: convert patient data using ConverterPretrain         p_converted = self.converter.forward_conversion(patient_data[\"events\"], patient_data[\"constant\"])          #: convert extras into JSON using df.to_json         internal_meta = p_converted[\"meta\"].copy()         internal_meta[\"split\"] = self.dm.get_patient_split(patientid=patientid)         p_converted[\"meta\"] = {             \"patientid\": patientid,             \"split\": self.dm.get_patient_split(patientid=patientid),             \"constant\": p_converted[\"meta\"][\"processed_constant\"].to_json(orient=\"split\"),         }          return [(p_converted, internal_meta)]      def assess_reverse_conversion(self, all_patient_data) -&gt; None:         \"\"\"         Assesses the reverse conversion for a single patient to ensure data integrity.          Parameters         ----------         all_patient_data : list of tuples             A list of tuples, each containing converted patient data and internal metadata.         \"\"\"          # Split up         converted_data, internal_meta = all_patient_data[0]         first_event_date = internal_meta[\"events\"][\"date\"].min()          # Log that testing patient         print(\"Assessing reverse conversion for patient\" + str(converted_data[\"meta\"][\"patientid\"]))          #: do reverse conversion using ConverterPretrain         p_reverse_converted = self.converter.reverse_conversion(             converted_data[\"text\"],             data_manager=self.dm,             init_date=first_event_date,         )          #: check differences appropriately         diff = self.converter.get_difference_in_event_dataframes(             internal_meta[\"events\"],             p_reverse_converted[\"events\"],             skip_genetic=True,             skip_vals_list=self.reverse_patient_skip_list,         )          #: assert that no differences are found, and print patientid if issues are found         assert diff.shape[0] == 0, f\"Patient {internal_meta['patientid']} has differences in reverse conversion: {diff}\" <p>################################## Actual running ####################################### NOTE: run this from the root folder of twinweaver</p> In\u00a0[\u00a0]: Copied! <pre>converter = ConvertToText()\n</pre> converter = ConvertToText() In\u00a0[\u00a0]: Copied! <pre>all_patientids = converter.dm.all_patientids.copy()\nall_patientids = all_patientids[:10]\n</pre> all_patientids = converter.dm.all_patientids.copy() all_patientids = all_patientids[:10] In\u00a0[\u00a0]: Copied! <pre>for idx, patientid in enumerate(all_patientids):\n    print(idx)\n\n    #: go through all patients and convert them\n    patient_data = converter.convert_full_to_string_for_one_patient(patientid)\n\n    # Add check whether it is correct\n    converter.assess_reverse_conversion(patient_data)\n</pre> for idx, patientid in enumerate(all_patientids):     print(idx)      #: go through all patients and convert them     patient_data = converter.convert_full_to_string_for_one_patient(patientid)      # Add check whether it is correct     converter.assess_reverse_conversion(patient_data) In\u00a0[\u00a0]: Copied! <pre>print(\"Finished\")\n</pre> print(\"Finished\")"},{"location":"examples/advanced/pretraining/results/","title":"Model Card for results","text":"<p>This model is a fine-tuned version of microsoft/Phi-4-mini-instruct. It has been trained using TRL.</p>","tags":["generated_from_trainer","trl","sft"]},{"location":"examples/advanced/pretraining/results/#quick-start","title":"Quick start","text":"<pre><code>from transformers import pipeline\n\nquestion = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\ngenerator = pipeline(\"text-generation\", model=\"None\", device=\"cuda\")\noutput = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\nprint(output[\"generated_text\"])\n</code></pre>","tags":["generated_from_trainer","trl","sft"]},{"location":"examples/advanced/pretraining/results/#training-procedure","title":"Training procedure","text":"<p>This model was trained with SFT.</p>","tags":["generated_from_trainer","trl","sft"]},{"location":"examples/advanced/pretraining/results/#framework-versions","title":"Framework versions","text":"<ul> <li>TRL: 0.27.0</li> <li>Transformers: 4.57.6</li> <li>Pytorch: 2.9.1</li> <li>Datasets: 4.5.0</li> <li>Tokenizers: 0.22.2</li> </ul>","tags":["generated_from_trainer","trl","sft"]},{"location":"examples/advanced/pretraining/results/#citations","title":"Citations","text":"<p>Cite TRL as:</p> <pre><code>@misc{vonwerra2022trl,\n    title        = {{TRL: Transformer Reinforcement Learning}},\n    author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\\'e}dec},\n    year         = 2020,\n    journal      = {GitHub repository},\n    publisher    = {GitHub},\n    howpublished = {\\url{https://github.com/huggingface/trl}}\n}\n</code></pre>","tags":["generated_from_trainer","trl","sft"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/","title":"Model Card for Model ID","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#model-details","title":"Model Details","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#model-description","title":"Model Description","text":"<ul> <li>Developed by: [More Information Needed]</li> <li>Funded by [optional]: [More Information Needed]</li> <li>Shared by [optional]: [More Information Needed]</li> <li>Model type: [More Information Needed]</li> <li>Language(s) (NLP): [More Information Needed]</li> <li>License: [More Information Needed]</li> <li>Finetuned from model [optional]: [More Information Needed]</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#model-sources-optional","title":"Model Sources [optional]","text":"<ul> <li>Repository: [More Information Needed]</li> <li>Paper [optional]: [More Information Needed]</li> <li>Demo [optional]: [More Information Needed]</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#uses","title":"Uses","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#direct-use","title":"Direct Use","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#downstream-use-optional","title":"Downstream Use [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#out-of-scope-use","title":"Out-of-Scope Use","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#bias-risks-and-limitations","title":"Bias, Risks, and Limitations","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#recommendations","title":"Recommendations","text":"<p>Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#how-to-get-started-with-the-model","title":"How to Get Started with the Model","text":"<p>Use the code below to get started with the model.</p> <p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#training-details","title":"Training Details","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#training-data","title":"Training Data","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#training-procedure","title":"Training Procedure","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#preprocessing-optional","title":"Preprocessing [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#training-hyperparameters","title":"Training Hyperparameters","text":"<ul> <li>Training regime: [More Information Needed] </li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#speeds-sizes-times-optional","title":"Speeds, Sizes, Times [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#evaluation","title":"Evaluation","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#testing-data-factors-metrics","title":"Testing Data, Factors &amp; Metrics","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#testing-data","title":"Testing Data","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#factors","title":"Factors","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#metrics","title":"Metrics","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#results","title":"Results","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#summary","title":"Summary","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#model-examination-optional","title":"Model Examination [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#environmental-impact","title":"Environmental Impact","text":"<p>Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).</p> <ul> <li>Hardware Type: [More Information Needed]</li> <li>Hours used: [More Information Needed]</li> <li>Cloud Provider: [More Information Needed]</li> <li>Compute Region: [More Information Needed]</li> <li>Carbon Emitted: [More Information Needed]</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#technical-specifications-optional","title":"Technical Specifications [optional]","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#model-architecture-and-objective","title":"Model Architecture and Objective","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#compute-infrastructure","title":"Compute Infrastructure","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#hardware","title":"Hardware","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#software","title":"Software","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#citation-optional","title":"Citation [optional]","text":"<p>BibTeX:</p> <p>[More Information Needed]</p> <p>APA:</p> <p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#glossary-optional","title":"Glossary [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#more-information-optional","title":"More Information [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#model-card-authors-optional","title":"Model Card Authors [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#model-card-contact","title":"Model Card Contact","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/checkpoint-200/#framework-versions","title":"Framework versions","text":"<ul> <li>PEFT 0.18.1</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/","title":"Model Card for Model ID","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#model-details","title":"Model Details","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#model-description","title":"Model Description","text":"<ul> <li>Developed by: [More Information Needed]</li> <li>Funded by [optional]: [More Information Needed]</li> <li>Shared by [optional]: [More Information Needed]</li> <li>Model type: [More Information Needed]</li> <li>Language(s) (NLP): [More Information Needed]</li> <li>License: [More Information Needed]</li> <li>Finetuned from model [optional]: [More Information Needed]</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#model-sources-optional","title":"Model Sources [optional]","text":"<ul> <li>Repository: [More Information Needed]</li> <li>Paper [optional]: [More Information Needed]</li> <li>Demo [optional]: [More Information Needed]</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#uses","title":"Uses","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#direct-use","title":"Direct Use","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#downstream-use-optional","title":"Downstream Use [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#out-of-scope-use","title":"Out-of-Scope Use","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#bias-risks-and-limitations","title":"Bias, Risks, and Limitations","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#recommendations","title":"Recommendations","text":"<p>Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#how-to-get-started-with-the-model","title":"How to Get Started with the Model","text":"<p>Use the code below to get started with the model.</p> <p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#training-details","title":"Training Details","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#training-data","title":"Training Data","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#training-procedure","title":"Training Procedure","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#preprocessing-optional","title":"Preprocessing [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#training-hyperparameters","title":"Training Hyperparameters","text":"<ul> <li>Training regime: [More Information Needed] </li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#speeds-sizes-times-optional","title":"Speeds, Sizes, Times [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#evaluation","title":"Evaluation","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#testing-data-factors-metrics","title":"Testing Data, Factors &amp; Metrics","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#testing-data","title":"Testing Data","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#factors","title":"Factors","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#metrics","title":"Metrics","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#results","title":"Results","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#summary","title":"Summary","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#model-examination-optional","title":"Model Examination [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#environmental-impact","title":"Environmental Impact","text":"<p>Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).</p> <ul> <li>Hardware Type: [More Information Needed]</li> <li>Hours used: [More Information Needed]</li> <li>Cloud Provider: [More Information Needed]</li> <li>Compute Region: [More Information Needed]</li> <li>Carbon Emitted: [More Information Needed]</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#technical-specifications-optional","title":"Technical Specifications [optional]","text":"","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#model-architecture-and-objective","title":"Model Architecture and Objective","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#compute-infrastructure","title":"Compute Infrastructure","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#hardware","title":"Hardware","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#software","title":"Software","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#citation-optional","title":"Citation [optional]","text":"<p>BibTeX:</p> <p>[More Information Needed]</p> <p>APA:</p> <p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#glossary-optional","title":"Glossary [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#more-information-optional","title":"More Information [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#model-card-authors-optional","title":"Model Card Authors [optional]","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#model-card-contact","title":"Model Card Contact","text":"<p>[More Information Needed]</p>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/advanced/pretraining/results/final_adapter/#framework-versions","title":"Framework versions","text":"<ul> <li>PEFT 0.18.1</li> </ul>","tags":["base_model:adapter:microsoft/Phi-4-mini-instruct","lora","sft","transformers","trl"]},{"location":"examples/data_preprocessing/raw_data_preprocessing/","title":"Data Preprocessing: From Raw Clinical Data to TwinWeaver Format","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n    DataSplitterForecasting,\n    DataSplitterEvents,\n    ConverterInstruction,\n    DataSplitter,\n    identify_constant_and_changing_columns,\n    aggregate_events_to_weeks,\n)\n</pre> import pandas as pd  from twinweaver import (     DataManager,     Config,     DataSplitterForecasting,     DataSplitterEvents,     ConverterInstruction,     DataSplitter,     identify_constant_and_changing_columns,     aggregate_events_to_weeks, ) In\u00a0[\u00a0]: Copied! <pre># Raw Patient Demographics DataFrame\n# This simulates a typical patient registry export with static information\nraw_demographics = pd.DataFrame(\n    {\n        \"patient_id\": [\"PT001\", \"PT002\", \"PT003\", \"PT004\", \"PT005\"],\n        \"birth_year\": [1958, 1965, 1972, 1949, 1961],\n        \"sex\": [\"Male\", \"Female\", \"Male\", \"Female\", \"Male\"],\n        \"cancer_type\": [\"NSCLC\", \"NSCLC\", \"NSCLC\", \"NSCLC\", \"NSCLC\"],\n        \"histology\": [\n            \"Adenocarcinoma\",\n            \"Squamous Cell Carcinoma\",\n            \"Adenocarcinoma\",\n            \"Adenocarcinoma\",\n            \"Squamous Cell Carcinoma\",\n        ],\n        \"smoking_status\": [\"Former\", \"Never\", \"Current\", \"Former\", \"Current\"],\n        \"diagnosis_date\": [\"2020-03-15\", \"2020-06-22\", \"2021-01-10\", \"2019-11-05\", \"2020-09-18\"],\n        \"stage_at_diagnosis\": [\"IIIB\", \"IV\", \"IIIA\", \"IV\", \"IIIB\"],\n        \"egfr_status\": [\"Wild Type\", \"Wild Type\", \"L858R Mutation\", \"Wild Type\", \"Wild Type\"],\n        \"alk_status\": [\"Wild Type\", \"Wild Type\", \"Wild Type\", \"Rearrangement\", \"Wild Type\"],\n        \"pdl1_expression\": [\"50-100%\", \"1-49%\", \"&lt;1%\", \"1-49%\", \"50-100%\"],\n        # Death information: some patients died, others are censored (alive at last follow-up)\n        \"death_status\": [\"Deceased\", \"Alive\", \"Alive\", \"Deceased\", \"Alive\"],\n        \"death_date\": [\"2021-02-10\", None, None, \"2020-08-15\", None],  # None for alive patients\n    }\n)\n\nprint(\"Raw Demographics DataFrame:\")\nraw_demographics\n</pre> # Raw Patient Demographics DataFrame # This simulates a typical patient registry export with static information raw_demographics = pd.DataFrame(     {         \"patient_id\": [\"PT001\", \"PT002\", \"PT003\", \"PT004\", \"PT005\"],         \"birth_year\": [1958, 1965, 1972, 1949, 1961],         \"sex\": [\"Male\", \"Female\", \"Male\", \"Female\", \"Male\"],         \"cancer_type\": [\"NSCLC\", \"NSCLC\", \"NSCLC\", \"NSCLC\", \"NSCLC\"],         \"histology\": [             \"Adenocarcinoma\",             \"Squamous Cell Carcinoma\",             \"Adenocarcinoma\",             \"Adenocarcinoma\",             \"Squamous Cell Carcinoma\",         ],         \"smoking_status\": [\"Former\", \"Never\", \"Current\", \"Former\", \"Current\"],         \"diagnosis_date\": [\"2020-03-15\", \"2020-06-22\", \"2021-01-10\", \"2019-11-05\", \"2020-09-18\"],         \"stage_at_diagnosis\": [\"IIIB\", \"IV\", \"IIIA\", \"IV\", \"IIIB\"],         \"egfr_status\": [\"Wild Type\", \"Wild Type\", \"L858R Mutation\", \"Wild Type\", \"Wild Type\"],         \"alk_status\": [\"Wild Type\", \"Wild Type\", \"Wild Type\", \"Rearrangement\", \"Wild Type\"],         \"pdl1_expression\": [\"50-100%\", \"1-49%\", \"&lt;1%\", \"1-49%\", \"50-100%\"],         # Death information: some patients died, others are censored (alive at last follow-up)         \"death_status\": [\"Deceased\", \"Alive\", \"Alive\", \"Deceased\", \"Alive\"],         \"death_date\": [\"2021-02-10\", None, None, \"2020-08-15\", None],  # None for alive patients     } )  print(\"Raw Demographics DataFrame:\") raw_demographics In\u00a0[\u00a0]: Copied! <pre># Raw Clinical Observations DataFrame\n# This simulates longitudinal clinical data with labs, vitals, treatments, and outcomes\nraw_observations = pd.DataFrame(\n    {\n        \"patient_id\": [\n            # Patient PT001 - multiple visits\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            \"PT001\",\n            # Patient PT002 - multiple visits\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            \"PT002\",\n            # Patient PT003 - multiple visits\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            \"PT003\",\n            # Patient PT004 - multiple visits\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            \"PT004\",\n            # Patient PT005 - multiple visits\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n            \"PT005\",\n        ],\n        \"visit_date\": [\n            # PT001 visits\n            \"2020-03-20\",\n            \"2020-03-20\",\n            \"2020-03-20\",\n            \"2020-03-20\",  # Baseline\n            \"2020-04-17\",\n            \"2020-04-17\",\n            \"2020-04-17\",\n            \"2020-04-17\",  # Cycle 1\n            \"2020-05-15\",\n            \"2020-05-15\",\n            \"2020-05-15\",\n            \"2020-05-15\",  # Cycle 2\n            \"2020-06-12\",\n            \"2020-06-12\",\n            \"2020-06-12\",\n            \"2020-06-12\",  # Cycle 3\n            # PT002 visits\n            \"2020-06-25\",\n            \"2020-06-25\",\n            \"2020-06-25\",  # Baseline\n            \"2020-07-23\",\n            \"2020-07-23\",\n            \"2020-07-23\",\n            \"2020-07-23\",  # Cycle 1\n            \"2020-08-20\",\n            \"2020-08-20\",\n            \"2020-08-20\",  # Cycle 2\n            \"2020-09-17\",\n            \"2020-09-17\",\n            \"2020-09-17\",\n            \"2020-09-17\",  # Cycle 3\n            # PT003 visits\n            \"2021-01-15\",\n            \"2021-01-15\",\n            \"2021-01-15\",  # Baseline\n            \"2021-02-12\",\n            \"2021-02-12\",\n            \"2021-02-12\",  # Cycle 1\n            \"2021-03-12\",\n            \"2021-03-12\",\n            \"2021-03-12\",  # Cycle 2\n            \"2021-04-09\",\n            \"2021-04-09\",\n            \"2021-04-09\",  # Cycle 3\n            # PT004 visits\n            \"2019-11-10\",\n            \"2019-11-10\",\n            \"2019-11-10\",\n            \"2019-11-10\",  # Baseline\n            \"2019-12-08\",\n            \"2019-12-08\",\n            \"2019-12-08\",  # Cycle 1\n            \"2020-01-05\",\n            \"2020-01-05\",\n            \"2020-01-05\",  # Cycle 2\n            \"2020-02-02\",\n            \"2020-02-02\",\n            \"2020-02-02\",\n            \"2020-02-02\",  # Cycle 3\n            # PT005 visits\n            \"2020-09-22\",\n            \"2020-09-22\",\n            \"2020-09-22\",  # Baseline\n            \"2020-10-20\",\n            \"2020-10-20\",\n            \"2020-10-20\",  # Cycle 1\n            \"2020-11-17\",\n            \"2020-11-17\",\n            \"2020-11-17\",\n            \"2020-11-17\",  # Cycle 2\n        ],\n        \"observation_type\": [\n            # PT001\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"treatment_start\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"response_assessment\",\n            # PT002\n            \"hemoglobin\",\n            \"platelets\",\n            \"treatment_start\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"response_assessment\",\n            # PT003\n            \"hemoglobin\",\n            \"platelets\",\n            \"treatment_start\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"response_assessment\",\n            # PT004\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"treatment_start\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"response_assessment\",\n            # PT005\n            \"hemoglobin\",\n            \"platelets\",\n            \"treatment_start\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"drug_admin\",\n            \"hemoglobin\",\n            \"platelets\",\n            \"ecog\",\n            \"response_assessment\",\n        ],\n        \"observation_value\": [\n            # PT001 - stable patient\n            \"13.5\",\n            \"285\",\n            \"1\",\n            \"Carboplatin/Pemetrexed/Pembrolizumab\",\n            \"13.2\",\n            \"278\",\n            \"1\",\n            \"Carboplatin/Pemetrexed/Pembrolizumab\",\n            \"12.8\",\n            \"265\",\n            \"1\",\n            \"Carboplatin/Pemetrexed/Pembrolizumab\",\n            \"12.9\",\n            \"270\",\n            \"0\",\n            \"Partial Response\",\n            # PT002 - declining hemoglobin\n            \"14.1\",\n            \"310\",\n            \"Carboplatin/Paclitaxel/Pembrolizumab\",\n            \"13.5\",\n            \"295\",\n            \"1\",\n            \"Carboplatin/Paclitaxel/Pembrolizumab\",\n            \"12.8\",\n            \"280\",\n            \"Carboplatin/Paclitaxel/Pembrolizumab\",\n            \"12.2\",\n            \"268\",\n            \"1\",\n            \"Stable Disease\",\n            # PT003 - EGFR+ patient on targeted therapy\n            \"14.8\",\n            \"245\",\n            \"Osimertinib\",\n            \"14.5\",\n            \"250\",\n            \"Osimertinib\",\n            \"14.3\",\n            \"248\",\n            \"Osimertinib\",\n            \"14.6\",\n            \"252\",\n            \"Partial Response\",\n            # PT004 - ALK+ patient\n            \"11.8\",\n            \"198\",\n            \"2\",\n            \"Alectinib\",\n            \"12.1\",\n            \"210\",\n            \"Alectinib\",\n            \"12.5\",\n            \"225\",\n            \"Alectinib\",\n            \"12.8\",\n            \"235\",\n            \"1\",\n            \"Partial Response\",\n            # PT005 - IO monotherapy\n            \"15.2\",\n            \"320\",\n            \"Pembrolizumab\",\n            \"14.9\",\n            \"315\",\n            \"Pembrolizumab\",\n            \"14.6\",\n            \"308\",\n            \"0\",\n            \"Complete Response\",\n        ],\n        \"observation_unit\": [\n            # PT001\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            # PT002\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            # PT003\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            # PT004\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n            # PT005\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"g/dL\",\n            \"10^9/L\",\n            \"\",\n            \"\",\n        ],\n    }\n)\n\nprint(\"Raw Clinical Observations DataFrame:\")\nraw_observations.head(20)\n</pre> # Raw Clinical Observations DataFrame # This simulates longitudinal clinical data with labs, vitals, treatments, and outcomes raw_observations = pd.DataFrame(     {         \"patient_id\": [             # Patient PT001 - multiple visits             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             \"PT001\",             # Patient PT002 - multiple visits             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             \"PT002\",             # Patient PT003 - multiple visits             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             \"PT003\",             # Patient PT004 - multiple visits             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             \"PT004\",             # Patient PT005 - multiple visits             \"PT005\",             \"PT005\",             \"PT005\",             \"PT005\",             \"PT005\",             \"PT005\",             \"PT005\",             \"PT005\",             \"PT005\",             \"PT005\",         ],         \"visit_date\": [             # PT001 visits             \"2020-03-20\",             \"2020-03-20\",             \"2020-03-20\",             \"2020-03-20\",  # Baseline             \"2020-04-17\",             \"2020-04-17\",             \"2020-04-17\",             \"2020-04-17\",  # Cycle 1             \"2020-05-15\",             \"2020-05-15\",             \"2020-05-15\",             \"2020-05-15\",  # Cycle 2             \"2020-06-12\",             \"2020-06-12\",             \"2020-06-12\",             \"2020-06-12\",  # Cycle 3             # PT002 visits             \"2020-06-25\",             \"2020-06-25\",             \"2020-06-25\",  # Baseline             \"2020-07-23\",             \"2020-07-23\",             \"2020-07-23\",             \"2020-07-23\",  # Cycle 1             \"2020-08-20\",             \"2020-08-20\",             \"2020-08-20\",  # Cycle 2             \"2020-09-17\",             \"2020-09-17\",             \"2020-09-17\",             \"2020-09-17\",  # Cycle 3             # PT003 visits             \"2021-01-15\",             \"2021-01-15\",             \"2021-01-15\",  # Baseline             \"2021-02-12\",             \"2021-02-12\",             \"2021-02-12\",  # Cycle 1             \"2021-03-12\",             \"2021-03-12\",             \"2021-03-12\",  # Cycle 2             \"2021-04-09\",             \"2021-04-09\",             \"2021-04-09\",  # Cycle 3             # PT004 visits             \"2019-11-10\",             \"2019-11-10\",             \"2019-11-10\",             \"2019-11-10\",  # Baseline             \"2019-12-08\",             \"2019-12-08\",             \"2019-12-08\",  # Cycle 1             \"2020-01-05\",             \"2020-01-05\",             \"2020-01-05\",  # Cycle 2             \"2020-02-02\",             \"2020-02-02\",             \"2020-02-02\",             \"2020-02-02\",  # Cycle 3             # PT005 visits             \"2020-09-22\",             \"2020-09-22\",             \"2020-09-22\",  # Baseline             \"2020-10-20\",             \"2020-10-20\",             \"2020-10-20\",  # Cycle 1             \"2020-11-17\",             \"2020-11-17\",             \"2020-11-17\",             \"2020-11-17\",  # Cycle 2         ],         \"observation_type\": [             # PT001             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"treatment_start\",             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"response_assessment\",             # PT002             \"hemoglobin\",             \"platelets\",             \"treatment_start\",             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"response_assessment\",             # PT003             \"hemoglobin\",             \"platelets\",             \"treatment_start\",             \"hemoglobin\",             \"platelets\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"response_assessment\",             # PT004             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"treatment_start\",             \"hemoglobin\",             \"platelets\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"response_assessment\",             # PT005             \"hemoglobin\",             \"platelets\",             \"treatment_start\",             \"hemoglobin\",             \"platelets\",             \"drug_admin\",             \"hemoglobin\",             \"platelets\",             \"ecog\",             \"response_assessment\",         ],         \"observation_value\": [             # PT001 - stable patient             \"13.5\",             \"285\",             \"1\",             \"Carboplatin/Pemetrexed/Pembrolizumab\",             \"13.2\",             \"278\",             \"1\",             \"Carboplatin/Pemetrexed/Pembrolizumab\",             \"12.8\",             \"265\",             \"1\",             \"Carboplatin/Pemetrexed/Pembrolizumab\",             \"12.9\",             \"270\",             \"0\",             \"Partial Response\",             # PT002 - declining hemoglobin             \"14.1\",             \"310\",             \"Carboplatin/Paclitaxel/Pembrolizumab\",             \"13.5\",             \"295\",             \"1\",             \"Carboplatin/Paclitaxel/Pembrolizumab\",             \"12.8\",             \"280\",             \"Carboplatin/Paclitaxel/Pembrolizumab\",             \"12.2\",             \"268\",             \"1\",             \"Stable Disease\",             # PT003 - EGFR+ patient on targeted therapy             \"14.8\",             \"245\",             \"Osimertinib\",             \"14.5\",             \"250\",             \"Osimertinib\",             \"14.3\",             \"248\",             \"Osimertinib\",             \"14.6\",             \"252\",             \"Partial Response\",             # PT004 - ALK+ patient             \"11.8\",             \"198\",             \"2\",             \"Alectinib\",             \"12.1\",             \"210\",             \"Alectinib\",             \"12.5\",             \"225\",             \"Alectinib\",             \"12.8\",             \"235\",             \"1\",             \"Partial Response\",             # PT005 - IO monotherapy             \"15.2\",             \"320\",             \"Pembrolizumab\",             \"14.9\",             \"315\",             \"Pembrolizumab\",             \"14.6\",             \"308\",             \"0\",             \"Complete Response\",         ],         \"observation_unit\": [             # PT001             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             # PT002             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             # PT003             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             # PT004             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"\",             # PT005             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"g/dL\",             \"10^9/L\",             \"\",             \"\",         ],     } )  print(\"Raw Clinical Observations DataFrame:\") raw_observations.head(20) In\u00a0[\u00a0]: Copied! <pre># First, let's check which columns in our demographics data are truly constant\n# We'll merge demographics with a simplified observations view to check\n\n# Create a merged view for analysis\nmerged_for_analysis = raw_observations.merge(\n    raw_demographics[[\"patient_id\", \"birth_year\", \"sex\", \"histology\", \"smoking_status\"]], on=\"patient_id\", how=\"left\"\n)\n\n# Identify constant vs changing columns\nconstant_cols, changing_cols = identify_constant_and_changing_columns(\n    merged_for_analysis, date_column=\"visit_date\", patientid_column=\"patient_id\"\n)\n\nprint(\"Constant columns (same value across all visits for each patient):\")\nprint(constant_cols)\nprint(\"\\nChanging columns (values vary over time):\")\nprint(changing_cols)\n</pre> # First, let's check which columns in our demographics data are truly constant # We'll merge demographics with a simplified observations view to check  # Create a merged view for analysis merged_for_analysis = raw_observations.merge(     raw_demographics[[\"patient_id\", \"birth_year\", \"sex\", \"histology\", \"smoking_status\"]], on=\"patient_id\", how=\"left\" )  # Identify constant vs changing columns constant_cols, changing_cols = identify_constant_and_changing_columns(     merged_for_analysis, date_column=\"visit_date\", patientid_column=\"patient_id\" )  print(\"Constant columns (same value across all visits for each patient):\") print(constant_cols) print(\"\\nChanging columns (values vary over time):\") print(changing_cols) In\u00a0[\u00a0]: Copied! <pre>def transform_to_events(raw_obs: pd.DataFrame, raw_demo: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw clinical data into TwinWeaver events format.\n\n    The events dataframe has these required columns:\n    - patientid: Unique patient identifier\n    - date: Date of the event\n    - event_category: High-level grouping (e.g., 'lab', 'drug', 'lot', 'death')\n    - event_name: Specific variable name\n    - event_value: The result/value\n    - event_descriptive_name: Natural language description for prompts\n    - meta_data: Additional metadata (optional)\n    - source: Data source identifier (optional)\n    \"\"\"\n    events_list = []\n\n    # --- Process clinical observations ---\n    for _, row in raw_obs.iterrows():\n        patient_id = row[\"patient_id\"]\n        visit_date = row[\"visit_date\"]\n        obs_type = row[\"observation_type\"]\n        obs_value = row[\"observation_value\"]\n        obs_unit = row[\"observation_unit\"]\n\n        # Map observation types to TwinWeaver categories\n        if obs_type == \"hemoglobin\":\n            events_list.append(\n                {\n                    \"patientid\": patient_id,\n                    \"date\": visit_date,\n                    \"event_category\": \"lab\",\n                    \"event_name\": \"hemoglobin_-_718-7\",\n                    \"event_value\": obs_value,\n                    \"event_descriptive_name\": \"hemoglobin - 718-7\",\n                    \"meta_data\": f\"Test: hemoglobin, Cleaned lab units: {obs_unit}\",\n                    \"source\": \"clinical_observations\",\n                }\n            )\n        elif obs_type == \"platelets\":\n            events_list.append(\n                {\n                    \"patientid\": patient_id,\n                    \"date\": visit_date,\n                    \"event_category\": \"lab\",\n                    \"event_name\": \"platelets_-_26515-7\",\n                    \"event_value\": obs_value,\n                    \"event_descriptive_name\": \"platelets - 26515-7\",\n                    \"meta_data\": f\"Test: platelets, Cleaned lab units: {obs_unit}\",\n                    \"source\": \"clinical_observations\",\n                }\n            )\n        elif obs_type == \"ecog\":\n            events_list.append(\n                {\n                    \"patientid\": patient_id,\n                    \"date\": visit_date,\n                    \"event_category\": \"ecog\",\n                    \"event_name\": \"ecog\",\n                    \"event_value\": obs_value,\n                    \"event_descriptive_name\": \"ECOG Performance Status\",\n                    \"meta_data\": None,\n                    \"source\": \"clinical_observations\",\n                }\n            )\n        elif obs_type == \"treatment_start\":\n            # Treatment start creates a Line of Therapy (LoT) event\n            events_list.append(\n                {\n                    \"patientid\": patient_id,\n                    \"date\": visit_date,\n                    \"event_category\": \"lot\",\n                    \"event_name\": \"line_number\",\n                    \"event_value\": \"1\",\n                    \"event_descriptive_name\": \"line number\",\n                    \"meta_data\": None,\n                    \"source\": \"clinical_observations\",\n                }\n            )\n            events_list.append(\n                {\n                    \"patientid\": patient_id,\n                    \"date\": visit_date,\n                    \"event_category\": \"lot\",\n                    \"event_name\": \"line_name\",\n                    \"event_value\": obs_value,\n                    \"event_descriptive_name\": \"line of therapy\",\n                    \"meta_data\": None,\n                    \"source\": \"clinical_observations\",\n                }\n            )\n            # Also add individual drug LoT start events\n            for drug in obs_value.split(\"/\"):\n                events_list.append(\n                    {\n                        \"patientid\": patient_id,\n                        \"date\": visit_date,\n                        \"event_category\": \"lot\",\n                        \"event_name\": drug.lower(),\n                        \"event_value\": \"LoT Start\",\n                        \"event_descriptive_name\": \"LoT\",\n                        \"meta_data\": None,\n                        \"source\": \"clinical_observations\",\n                    }\n                )\n        elif obs_type == \"drug_admin\":\n            # Drug administration events\n            for drug in obs_value.split(\"/\"):\n                events_list.append(\n                    {\n                        \"patientid\": patient_id,\n                        \"date\": visit_date,\n                        \"event_category\": \"drug\",\n                        \"event_name\": drug.lower(),\n                        \"event_value\": \"administered\",\n                        \"event_descriptive_name\": drug.lower(),\n                        \"meta_data\": obs_value,\n                        \"source\": \"clinical_observations\",\n                    }\n                )\n        elif obs_type == \"response_assessment\":\n            events_list.append(\n                {\n                    \"patientid\": patient_id,\n                    \"date\": visit_date,\n                    \"event_category\": \"response\",\n                    \"event_name\": \"recist_response\",\n                    \"event_value\": obs_value,\n                    \"event_descriptive_name\": \"RECIST Response\",\n                    \"meta_data\": None,\n                    \"source\": \"clinical_observations\",\n                }\n            )\n\n    # --- Process diagnosis and biomarker data from demographics ---\n    # These are events because they have a specific date and could change over time\n    for _, row in raw_demo.iterrows():\n        patient_id = row[\"patient_id\"]\n        diagnosis_date = row[\"diagnosis_date\"]\n\n        # Initial diagnosis event\n        events_list.append(\n            {\n                \"patientid\": patient_id,\n                \"date\": diagnosis_date,\n                \"event_category\": \"main_diagnosis\",\n                \"event_name\": \"initial_diagnosis\",\n                \"event_value\": row[\"cancer_type\"],\n                \"event_descriptive_name\": \"initial cancer diagnosis\",\n                \"meta_data\": row[\"cancer_type\"],\n                \"source\": \"demographics\",\n            }\n        )\n\n        # Stage at diagnosis\n        events_list.append(\n            {\n                \"patientid\": patient_id,\n                \"date\": diagnosis_date,\n                \"event_category\": \"staging\",\n                \"event_name\": \"stage\",\n                \"event_value\": row[\"stage_at_diagnosis\"],\n                \"event_descriptive_name\": \"Cancer Stage\",\n                \"meta_data\": None,\n                \"source\": \"demographics\",\n            }\n        )\n\n        # Biomarker results (these go into events, not constants!)\n        events_list.append(\n            {\n                \"patientid\": patient_id,\n                \"date\": diagnosis_date,\n                \"event_category\": \"basic_biomarker\",\n                \"event_name\": \"EGFR\",\n                \"event_value\": row[\"egfr_status\"],\n                \"event_descriptive_name\": \"EGFR\",\n                \"meta_data\": \"NGS\",\n                \"source\": \"demographics\",\n            }\n        )\n        events_list.append(\n            {\n                \"patientid\": patient_id,\n                \"date\": diagnosis_date,\n                \"event_category\": \"basic_biomarker\",\n                \"event_name\": \"ALK\",\n                \"event_value\": row[\"alk_status\"],\n                \"event_descriptive_name\": \"ALK\",\n                \"meta_data\": \"NGS\",\n                \"source\": \"demographics\",\n            }\n        )\n        events_list.append(\n            {\n                \"patientid\": patient_id,\n                \"date\": diagnosis_date,\n                \"event_category\": \"biomarker_ihc\",\n                \"event_name\": \"PD-L1\",\n                \"event_value\": row[\"pdl1_expression\"],\n                \"event_descriptive_name\": \"PD-L1 Expression (TPS)\",\n                \"meta_data\": \"IHC 22C3\",\n                \"source\": \"demographics\",\n            }\n        )\n\n        # --- Process death events ---\n        # Death is a time-to-event outcome that occurs at a specific date\n        if row[\"death_status\"] == \"Deceased\" and pd.notna(row[\"death_date\"]):\n            events_list.append(\n                {\n                    \"patientid\": patient_id,\n                    \"date\": row[\"death_date\"],\n                    \"event_category\": \"death\",\n                    \"event_name\": \"death\",\n                    \"event_value\": \"Yes\",\n                    \"event_descriptive_name\": \"Death\",\n                    \"meta_data\": None,\n                    \"source\": \"demographics\",\n                }\n            )\n\n    # Create DataFrame and sort by patient and date\n    df_events = pd.DataFrame(events_list)\n    df_events[\"date\"] = pd.to_datetime(df_events[\"date\"])\n    df_events = df_events.sort_values([\"patientid\", \"date\"]).reset_index(drop=True)\n\n    return df_events\n\n\n# Transform the data\ndf_events = transform_to_events(raw_observations, raw_demographics)\n\nprint(f\"Created events DataFrame with {len(df_events)} events\")\nprint(f\"Unique patients: {df_events['patientid'].nunique()}\")\nprint(f\"\\nEvent categories: {df_events['event_category'].unique().tolist()}\")\ndf_events.head(15)\n</pre> def transform_to_events(raw_obs: pd.DataFrame, raw_demo: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Transform raw clinical data into TwinWeaver events format.      The events dataframe has these required columns:     - patientid: Unique patient identifier     - date: Date of the event     - event_category: High-level grouping (e.g., 'lab', 'drug', 'lot', 'death')     - event_name: Specific variable name     - event_value: The result/value     - event_descriptive_name: Natural language description for prompts     - meta_data: Additional metadata (optional)     - source: Data source identifier (optional)     \"\"\"     events_list = []      # --- Process clinical observations ---     for _, row in raw_obs.iterrows():         patient_id = row[\"patient_id\"]         visit_date = row[\"visit_date\"]         obs_type = row[\"observation_type\"]         obs_value = row[\"observation_value\"]         obs_unit = row[\"observation_unit\"]          # Map observation types to TwinWeaver categories         if obs_type == \"hemoglobin\":             events_list.append(                 {                     \"patientid\": patient_id,                     \"date\": visit_date,                     \"event_category\": \"lab\",                     \"event_name\": \"hemoglobin_-_718-7\",                     \"event_value\": obs_value,                     \"event_descriptive_name\": \"hemoglobin - 718-7\",                     \"meta_data\": f\"Test: hemoglobin, Cleaned lab units: {obs_unit}\",                     \"source\": \"clinical_observations\",                 }             )         elif obs_type == \"platelets\":             events_list.append(                 {                     \"patientid\": patient_id,                     \"date\": visit_date,                     \"event_category\": \"lab\",                     \"event_name\": \"platelets_-_26515-7\",                     \"event_value\": obs_value,                     \"event_descriptive_name\": \"platelets - 26515-7\",                     \"meta_data\": f\"Test: platelets, Cleaned lab units: {obs_unit}\",                     \"source\": \"clinical_observations\",                 }             )         elif obs_type == \"ecog\":             events_list.append(                 {                     \"patientid\": patient_id,                     \"date\": visit_date,                     \"event_category\": \"ecog\",                     \"event_name\": \"ecog\",                     \"event_value\": obs_value,                     \"event_descriptive_name\": \"ECOG Performance Status\",                     \"meta_data\": None,                     \"source\": \"clinical_observations\",                 }             )         elif obs_type == \"treatment_start\":             # Treatment start creates a Line of Therapy (LoT) event             events_list.append(                 {                     \"patientid\": patient_id,                     \"date\": visit_date,                     \"event_category\": \"lot\",                     \"event_name\": \"line_number\",                     \"event_value\": \"1\",                     \"event_descriptive_name\": \"line number\",                     \"meta_data\": None,                     \"source\": \"clinical_observations\",                 }             )             events_list.append(                 {                     \"patientid\": patient_id,                     \"date\": visit_date,                     \"event_category\": \"lot\",                     \"event_name\": \"line_name\",                     \"event_value\": obs_value,                     \"event_descriptive_name\": \"line of therapy\",                     \"meta_data\": None,                     \"source\": \"clinical_observations\",                 }             )             # Also add individual drug LoT start events             for drug in obs_value.split(\"/\"):                 events_list.append(                     {                         \"patientid\": patient_id,                         \"date\": visit_date,                         \"event_category\": \"lot\",                         \"event_name\": drug.lower(),                         \"event_value\": \"LoT Start\",                         \"event_descriptive_name\": \"LoT\",                         \"meta_data\": None,                         \"source\": \"clinical_observations\",                     }                 )         elif obs_type == \"drug_admin\":             # Drug administration events             for drug in obs_value.split(\"/\"):                 events_list.append(                     {                         \"patientid\": patient_id,                         \"date\": visit_date,                         \"event_category\": \"drug\",                         \"event_name\": drug.lower(),                         \"event_value\": \"administered\",                         \"event_descriptive_name\": drug.lower(),                         \"meta_data\": obs_value,                         \"source\": \"clinical_observations\",                     }                 )         elif obs_type == \"response_assessment\":             events_list.append(                 {                     \"patientid\": patient_id,                     \"date\": visit_date,                     \"event_category\": \"response\",                     \"event_name\": \"recist_response\",                     \"event_value\": obs_value,                     \"event_descriptive_name\": \"RECIST Response\",                     \"meta_data\": None,                     \"source\": \"clinical_observations\",                 }             )      # --- Process diagnosis and biomarker data from demographics ---     # These are events because they have a specific date and could change over time     for _, row in raw_demo.iterrows():         patient_id = row[\"patient_id\"]         diagnosis_date = row[\"diagnosis_date\"]          # Initial diagnosis event         events_list.append(             {                 \"patientid\": patient_id,                 \"date\": diagnosis_date,                 \"event_category\": \"main_diagnosis\",                 \"event_name\": \"initial_diagnosis\",                 \"event_value\": row[\"cancer_type\"],                 \"event_descriptive_name\": \"initial cancer diagnosis\",                 \"meta_data\": row[\"cancer_type\"],                 \"source\": \"demographics\",             }         )          # Stage at diagnosis         events_list.append(             {                 \"patientid\": patient_id,                 \"date\": diagnosis_date,                 \"event_category\": \"staging\",                 \"event_name\": \"stage\",                 \"event_value\": row[\"stage_at_diagnosis\"],                 \"event_descriptive_name\": \"Cancer Stage\",                 \"meta_data\": None,                 \"source\": \"demographics\",             }         )          # Biomarker results (these go into events, not constants!)         events_list.append(             {                 \"patientid\": patient_id,                 \"date\": diagnosis_date,                 \"event_category\": \"basic_biomarker\",                 \"event_name\": \"EGFR\",                 \"event_value\": row[\"egfr_status\"],                 \"event_descriptive_name\": \"EGFR\",                 \"meta_data\": \"NGS\",                 \"source\": \"demographics\",             }         )         events_list.append(             {                 \"patientid\": patient_id,                 \"date\": diagnosis_date,                 \"event_category\": \"basic_biomarker\",                 \"event_name\": \"ALK\",                 \"event_value\": row[\"alk_status\"],                 \"event_descriptive_name\": \"ALK\",                 \"meta_data\": \"NGS\",                 \"source\": \"demographics\",             }         )         events_list.append(             {                 \"patientid\": patient_id,                 \"date\": diagnosis_date,                 \"event_category\": \"biomarker_ihc\",                 \"event_name\": \"PD-L1\",                 \"event_value\": row[\"pdl1_expression\"],                 \"event_descriptive_name\": \"PD-L1 Expression (TPS)\",                 \"meta_data\": \"IHC 22C3\",                 \"source\": \"demographics\",             }         )          # --- Process death events ---         # Death is a time-to-event outcome that occurs at a specific date         if row[\"death_status\"] == \"Deceased\" and pd.notna(row[\"death_date\"]):             events_list.append(                 {                     \"patientid\": patient_id,                     \"date\": row[\"death_date\"],                     \"event_category\": \"death\",                     \"event_name\": \"death\",                     \"event_value\": \"Yes\",                     \"event_descriptive_name\": \"Death\",                     \"meta_data\": None,                     \"source\": \"demographics\",                 }             )      # Create DataFrame and sort by patient and date     df_events = pd.DataFrame(events_list)     df_events[\"date\"] = pd.to_datetime(df_events[\"date\"])     df_events = df_events.sort_values([\"patientid\", \"date\"]).reset_index(drop=True)      return df_events   # Transform the data df_events = transform_to_events(raw_observations, raw_demographics)  print(f\"Created events DataFrame with {len(df_events)} events\") print(f\"Unique patients: {df_events['patientid'].nunique()}\") print(f\"\\nEvent categories: {df_events['event_category'].unique().tolist()}\") df_events.head(15) In\u00a0[\u00a0]: Copied! <pre>def transform_to_constant(raw_demo: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Extract truly constant patient information.\n\n    Only include immutable characteristics that:\n    1. Never change over time\n    2. Don't have a meaningful \"measurement date\"\n    \"\"\"\n    df_constant = raw_demo[[\"patient_id\", \"birth_year\", \"sex\", \"histology\", \"smoking_status\"]].copy()\n\n    # Rename columns to match TwinWeaver format\n    df_constant = df_constant.rename(\n        columns={\n            \"patient_id\": \"patientid\",\n            \"birth_year\": \"birthyear\",\n            \"sex\": \"gender\",\n        }\n    )\n\n    return df_constant\n\n\ndf_constant = transform_to_constant(raw_demographics)\n\nprint(\"Constant DataFrame (static patient information):\")\ndf_constant\n</pre> def transform_to_constant(raw_demo: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Extract truly constant patient information.      Only include immutable characteristics that:     1. Never change over time     2. Don't have a meaningful \"measurement date\"     \"\"\"     df_constant = raw_demo[[\"patient_id\", \"birth_year\", \"sex\", \"histology\", \"smoking_status\"]].copy()      # Rename columns to match TwinWeaver format     df_constant = df_constant.rename(         columns={             \"patient_id\": \"patientid\",             \"birth_year\": \"birthyear\",             \"sex\": \"gender\",         }     )      return df_constant   df_constant = transform_to_constant(raw_demographics)  print(\"Constant DataFrame (static patient information):\") df_constant In\u00a0[\u00a0]: Copied! <pre>def create_constant_description(df_constant: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Create descriptions for each constant column.\n    These descriptions are used in prompt generation.\n    \"\"\"\n    descriptions = {\n        \"patientid\": \"Unique patient identifier\",\n        \"birthyear\": \"Year of birth of the patient\",\n        \"gender\": \"Gender of the patient\",\n        \"histology\": \"Histological subtype of NSCLC\",\n        \"smoking_status\": \"Smoking status at diagnosis\",\n    }\n\n    # Create description for each column that exists\n    rows = []\n    for col in df_constant.columns:\n        rows.append({\"variable\": col, \"comment\": descriptions.get(col, f\"Description for {col}\")})\n\n    return pd.DataFrame(rows)\n\n\ndf_constant_description = create_constant_description(df_constant)\n\nprint(\"Constant Description DataFrame:\")\ndf_constant_description\n</pre> def create_constant_description(df_constant: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Create descriptions for each constant column.     These descriptions are used in prompt generation.     \"\"\"     descriptions = {         \"patientid\": \"Unique patient identifier\",         \"birthyear\": \"Year of birth of the patient\",         \"gender\": \"Gender of the patient\",         \"histology\": \"Histological subtype of NSCLC\",         \"smoking_status\": \"Smoking status at diagnosis\",     }      # Create description for each column that exists     rows = []     for col in df_constant.columns:         rows.append({\"variable\": col, \"comment\": descriptions.get(col, f\"Description for {col}\")})      return pd.DataFrame(rows)   df_constant_description = create_constant_description(df_constant)  print(\"Constant Description DataFrame:\") df_constant_description In\u00a0[\u00a0]: Copied! <pre># Demonstrate weekly aggregation on lab values\ndf_labs_only = df_events[df_events[\"event_category\"] == \"lab\"].copy()\n\nprint(f\"Before aggregation: {len(df_labs_only)} lab events\")\n\n# Aggregate to weekly values\ndf_labs_aggregated = aggregate_events_to_weeks(\n    df_labs_only,\n    patientid_column=\"patientid\",\n    date_column=\"date\",\n    event_name_column=\"event_name\",\n    event_value_column=\"event_value\",\n    random_state=42,  # For reproducibility\n)\n\nprint(f\"After aggregation: {len(df_labs_aggregated)} lab events\")\nprint(\"\\nAggregated lab events (first 10):\")\ndf_labs_aggregated.sort_values(by=[\"patientid\", \"date\"]).head(10)\n</pre> # Demonstrate weekly aggregation on lab values df_labs_only = df_events[df_events[\"event_category\"] == \"lab\"].copy()  print(f\"Before aggregation: {len(df_labs_only)} lab events\")  # Aggregate to weekly values df_labs_aggregated = aggregate_events_to_weeks(     df_labs_only,     patientid_column=\"patientid\",     date_column=\"date\",     event_name_column=\"event_name\",     event_value_column=\"event_value\",     random_state=42,  # For reproducibility )  print(f\"After aggregation: {len(df_labs_aggregated)} lab events\") print(\"\\nAggregated lab events (first 10):\") df_labs_aggregated.sort_values(by=[\"patientid\", \"date\"]).head(10) In\u00a0[\u00a0]: Copied! <pre>def validate_twinweaver_format(df_events, df_constant, df_constant_description):\n    \"\"\"Validate that dataframes conform to TwinWeaver requirements.\"\"\"\n    issues = []\n\n    # Check df_events required columns\n    events_required = [\"patientid\", \"date\", \"event_category\", \"event_name\", \"event_value\", \"event_descriptive_name\"]\n    for col in events_required:\n        if col not in df_events.columns:\n            issues.append(f\"df_events missing required column: {col}\")\n\n    # Check df_constant has patientid\n    if \"patientid\" not in df_constant.columns:\n        issues.append(\"df_constant missing required column: patientid\")\n\n    # Check df_constant_description structure\n    if \"variable\" not in df_constant_description.columns:\n        issues.append(\"df_constant_description missing required column: variable\")\n    if \"comment\" not in df_constant_description.columns:\n        issues.append(\"df_constant_description missing required column: comment\")\n\n    # Check patient ID consistency\n    events_patients = set(df_events[\"patientid\"].unique())\n    constant_patients = set(df_constant[\"patientid\"].unique())\n    if events_patients != constant_patients:\n        missing_in_events = constant_patients - events_patients\n        missing_in_constant = events_patients - constant_patients\n        if missing_in_events:\n            issues.append(f\"Patients in constant but not in events: {missing_in_events}\")\n        if missing_in_constant:\n            issues.append(f\"Patients in events but not in constant: {missing_in_constant}\")\n\n    if issues:\n        print(\"\u274c Validation issues found:\")\n        for issue in issues:\n            print(f\"  - {issue}\")\n    else:\n        print(\"\u2705 All dataframes are in valid TwinWeaver format!\")\n        print(f\"   - Events: {len(df_events)} rows, {df_events['patientid'].nunique()} patients\")\n        print(f\"   - Constants: {len(df_constant)} rows, {len(df_constant.columns)} columns\")\n        print(f\"   - Descriptions: {len(df_constant_description)} variable descriptions\")\n\n    return len(issues) == 0\n\n\nvalidate_twinweaver_format(df_events, df_constant, df_constant_description)\n</pre> def validate_twinweaver_format(df_events, df_constant, df_constant_description):     \"\"\"Validate that dataframes conform to TwinWeaver requirements.\"\"\"     issues = []      # Check df_events required columns     events_required = [\"patientid\", \"date\", \"event_category\", \"event_name\", \"event_value\", \"event_descriptive_name\"]     for col in events_required:         if col not in df_events.columns:             issues.append(f\"df_events missing required column: {col}\")      # Check df_constant has patientid     if \"patientid\" not in df_constant.columns:         issues.append(\"df_constant missing required column: patientid\")      # Check df_constant_description structure     if \"variable\" not in df_constant_description.columns:         issues.append(\"df_constant_description missing required column: variable\")     if \"comment\" not in df_constant_description.columns:         issues.append(\"df_constant_description missing required column: comment\")      # Check patient ID consistency     events_patients = set(df_events[\"patientid\"].unique())     constant_patients = set(df_constant[\"patientid\"].unique())     if events_patients != constant_patients:         missing_in_events = constant_patients - events_patients         missing_in_constant = events_patients - constant_patients         if missing_in_events:             issues.append(f\"Patients in constant but not in events: {missing_in_events}\")         if missing_in_constant:             issues.append(f\"Patients in events but not in constant: {missing_in_constant}\")      if issues:         print(\"\u274c Validation issues found:\")         for issue in issues:             print(f\"  - {issue}\")     else:         print(\"\u2705 All dataframes are in valid TwinWeaver format!\")         print(f\"   - Events: {len(df_events)} rows, {df_events['patientid'].nunique()} patients\")         print(f\"   - Constants: {len(df_constant)} rows, {len(df_constant.columns)} columns\")         print(f\"   - Descriptions: {len(df_constant_description)} variable descriptions\")      return len(issues) == 0   validate_twinweaver_format(df_events, df_constant, df_constant_description) In\u00a0[\u00a0]: Copied! <pre># Configure TwinWeaver\nconfig = Config()\n\n# Set the event category used for data splitting (split around Lines of Therapy)\nconfig.split_event_category = \"lot\"\n\n# Define which event categories to forecast\nconfig.event_category_forecast = [\"lab\"]\n\n# 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression')\n# Only needs to be set if you want to do time to event prediction\nconfig.data_splitter_events_variables_category_mapping = {\n    \"death\": \"death\",\n    \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\"\n}\n\n# Define which static columns to include in prompts\nconfig.constant_columns_to_use = [\n    \"birthyear\",\n    \"gender\",\n    \"histology\",\n    \"smoking_status\",\n]\n\n# Specify the birth year column for age calculation\nconfig.constant_birthdate_column = \"birthyear\"\n</pre> # Configure TwinWeaver config = Config()  # Set the event category used for data splitting (split around Lines of Therapy) config.split_event_category = \"lot\"  # Define which event categories to forecast config.event_category_forecast = [\"lab\"]  # 3. Mapping of specific time to events to predict (e.g., we want to predict 'death' and 'progression') # Only needs to be set if you want to do time to event prediction config.data_splitter_events_variables_category_mapping = {     \"death\": \"death\",     \"progression\": \"next progression\",  # Custom name in prompt: \"next progression\" instead of \"progression\" }  # Define which static columns to include in prompts config.constant_columns_to_use = [     \"birthyear\",     \"gender\",     \"histology\",     \"smoking_status\", ]  # Specify the birth year column for age calculation config.constant_birthdate_column = \"birthyear\" In\u00a0[\u00a0]: Copied! <pre># Initialize the DataManager and load our processed data\ndm = DataManager(config=config)\ndm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\ndm.infer_var_types()\n\nprint(f\"Loaded {len(dm.all_patientids)} patients into DataManager\")\n</pre> # Initialize the DataManager and load our processed data dm = DataManager(config=config) dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description) dm.process_indication_data() dm.setup_unique_mapping_of_events() dm.setup_dataset_splits() dm.infer_var_types()  print(f\"Loaded {len(dm.all_patientids)} patients into DataManager\") In\u00a0[\u00a0]: Copied! <pre># Initialize data splitters and converter\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\n\ndata_splitter_forecasting = DataSplitterForecasting(\n    data_manager=dm,\n    config=config,\n)\ndata_splitter_forecasting.setup_statistics()\n\ndata_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)\n\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config,\n    dm=dm,\n    variable_stats=data_splitter_forecasting.variable_stats,\n)\n</pre> # Initialize data splitters and converter data_splitter_events = DataSplitterEvents(dm, config=config) data_splitter_events.setup_variables()  data_splitter_forecasting = DataSplitterForecasting(     data_manager=dm,     config=config, ) data_splitter_forecasting.setup_statistics()  data_splitter = DataSplitter(data_splitter_events, data_splitter_forecasting)  converter = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config,     dm=dm,     variable_stats=data_splitter_forecasting.variable_stats, ) In\u00a0[\u00a0]: Copied! <pre># Generate instruction-tuning examples for a patient\npatientid = dm.all_patientids[0]\npatient_data = dm.get_patient_data(patientid)\n\nprint(f\"Patient: {patientid}\")\nprint(f\"Number of events: {len(patient_data['events'])}\")\nprint(\"\\nPatient events:\")\npatient_data[\"events\"]\n</pre> # Generate instruction-tuning examples for a patient patientid = dm.all_patientids[0] patient_data = dm.get_patient_data(patientid)  print(f\"Patient: {patientid}\") print(f\"Number of events: {len(patient_data['events'])}\") print(\"\\nPatient events:\") patient_data[\"events\"] In\u00a0[\u00a0]: Copied! <pre># Generate training splits\nforecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(\n    patient_data,\n)\n\nprint(f\"Generated {len(forecasting_splits)} training splits for patient {patientid}\")\n</pre> # Generate training splits forecasting_splits, events_splits, reference_dates = data_splitter.get_splits_from_patient_with_target(     patient_data, )  print(f\"Generated {len(forecasting_splits)} training splits for patient {patientid}\") In\u00a0[\u00a0]: Copied! <pre># Convert first split to instruction format\nif len(forecasting_splits) &gt; 0:\n    split_idx = 0\n    p_converted = converter.forward_conversion(\n        forecasting_splits=forecasting_splits[split_idx],\n        event_splits=events_splits[split_idx],\n        override_mode_to_select_forecasting=\"both\",\n    )\n\n    print(\"=\" * 80)\n    print(\"INSTRUCTION (Model Input):\")\n    print(\"=\" * 80)\n    print(p_converted[\"instruction\"])\nelse:\n    print(\"No training splits generated for this patient.\")\n</pre> # Convert first split to instruction format if len(forecasting_splits) &gt; 0:     split_idx = 0     p_converted = converter.forward_conversion(         forecasting_splits=forecasting_splits[split_idx],         event_splits=events_splits[split_idx],         override_mode_to_select_forecasting=\"both\",     )      print(\"=\" * 80)     print(\"INSTRUCTION (Model Input):\")     print(\"=\" * 80)     print(p_converted[\"instruction\"]) else:     print(\"No training splits generated for this patient.\") In\u00a0[\u00a0]: Copied! <pre>if len(forecasting_splits) &gt; 0:\n    print(\"=\" * 80)\n    print(\"ANSWER (Target Output):\")\n    print(\"=\" * 80)\n    print(p_converted[\"answer\"])\n</pre> if len(forecasting_splits) &gt; 0:     print(\"=\" * 80)     print(\"ANSWER (Target Output):\")     print(\"=\" * 80)     print(p_converted[\"answer\"])"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#data-preprocessing-from-raw-clinical-data-to-twinweaver-format","title":"Data Preprocessing: From Raw Clinical Data to TwinWeaver Format\u00b6","text":"<p>This tutorial demonstrates how to transform raw clinical data into the standardized TwinWeaver format required for training digital twin models.</p> <p>Key Principles:</p> <ol> <li>Include as much data as possible - We aim to capture all available clinical information first, then trim down if needed during data generation.</li> <li>Prefer events over constants - Longitudinal data (events) provides richer temporal context than static data (constants). Put as much as possible into the events dataframe.</li> </ol> <p>We will cover:</p> <ol> <li>Creating synthetic raw clinical data (simulating real-world EHR exports)</li> <li>Transforming raw data into the three required TwinWeaver dataframes:<ul> <li><code>df_events</code>: Longitudinal patient events in long format</li> <li><code>df_constant</code>: Static patient demographics</li> <li><code>df_constant_description</code>: Metadata describing constant columns</li> </ul> </li> <li>Using preprocessing helper functions for data aggregation and column classification</li> <li>Converting the processed data into instruction-tuning format</li> </ol>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#1-create-synthetic-raw-clinical-data","title":"1. Create Synthetic Raw Clinical Data\u00b6","text":"<p>In real-world scenarios, you would receive data exports from electronic health records (EHR), clinical trial databases, or other clinical data sources. These typically come as wide-format tables with mixed static and longitudinal information.</p> <p>We'll create two raw dataframes simulating a typical oncology dataset:</p> <ul> <li>Raw Patient Demographics: Contains static information like birth year, gender, and diagnosis details</li> <li>Raw Clinical Observations: Contains longitudinal data like lab results, treatments, and clinical assessments</li> </ul>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#2-use-preprocessing-helpers-to-understand-your-data","title":"2. Use Preprocessing Helpers to Understand Your Data\u00b6","text":"<p>Before transforming the data, let's use the preprocessing helper functions to:</p> <ol> <li>Identify constant vs. changing columns - This helps decide what goes into <code>df_constant</code> vs <code>df_events</code></li> <li>Aggregate events to weeks - This reduces noise from multiple observations on nearby days</li> </ol>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#why-put-most-data-into-events","title":"Why Put Most Data into Events?\u00b6","text":"<p>Key Insight: Even data that appears \"constant\" (like biomarker status) is often better represented as events because:</p> <ol> <li>It has a specific date when it was measured</li> <li>It could potentially change over time (e.g., acquired resistance mutations)</li> <li>The temporal context of when information was known is clinically relevant</li> </ol> <p>Rule of thumb: Only truly immutable patient characteristics (birth year, biological sex) should go in <code>df_constant</code>. Everything else should be an event!</p>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#3-transform-raw-data-into-twinweaver-format","title":"3. Transform Raw Data into TwinWeaver Format\u00b6","text":"<p>Now we'll convert our raw data into the three required TwinWeaver dataframes.</p>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#31-create-df_events-longitudinal-events","title":"3.1 Create df_events (Longitudinal Events)\u00b6","text":""},{"location":"examples/data_preprocessing/raw_data_preprocessing/#32-create-df_constant-static-patient-information","title":"3.2 Create df_constant (Static Patient Information)\u00b6","text":"<p>Only truly immutable characteristics should go here. We keep this minimal!</p>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#33-create-df_constant_description-metadata-for-constants","title":"3.3 Create df_constant_description (Metadata for Constants)\u00b6","text":"<p>This provides human-readable descriptions for each column in <code>df_constant</code>.</p>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#4-apply-weekly-aggregation-optional-preprocessing","title":"4. Apply Weekly Aggregation (Optional Preprocessing)\u00b6","text":"<p>If your data has multiple observations on nearby days (e.g., labs taken daily), you may want to aggregate them to reduce noise. The <code>aggregate_events_to_weeks</code> function handles this automatically.</p>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#5-validate-the-twinweaver-format","title":"5. Validate the TwinWeaver Format\u00b6","text":"<p>Let's verify our data is in the correct format before proceeding.</p>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#6-convert-to-instruction-tuning-format","title":"6. Convert to Instruction-Tuning Format\u00b6","text":"<p>Now we can use our processed data with the TwinWeaver pipeline to generate instruction-tuning examples, just like in the <code>01_data_preparation_for_training</code> tutorial.</p>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#summary-key-takeaways","title":"Summary: Key Takeaways\u00b6","text":""},{"location":"examples/data_preprocessing/raw_data_preprocessing/#data-format-requirements","title":"Data Format Requirements\u00b6","text":"<p>TwinWeaver requires three dataframes:</p> <ol> <li><p><code>df_events</code> (Longitudinal data in long format)</p> <ul> <li>Required columns: <code>patientid</code>, <code>date</code>, <code>event_category</code>, <code>event_name</code>, <code>event_value</code>, <code>event_descriptive_name</code></li> <li>Optional columns: <code>meta_data</code>, <code>source</code></li> </ul> </li> <li><p><code>df_constant</code> (Static patient information)</p> <ul> <li>Required column: <code>patientid</code></li> <li>Additional columns for immutable characteristics (birthyear, gender, etc.)</li> </ul> </li> <li><p><code>df_constant_description</code> (Metadata for constants)</p> <ul> <li>Required columns: <code>variable</code>, <code>comment</code></li> </ul> </li> </ol>"},{"location":"examples/data_preprocessing/raw_data_preprocessing/#best-practices","title":"Best Practices\u00b6","text":"<ol> <li><p>Put as much as possible into events - Even data that seems \"constant\" often has temporal context:</p> <ul> <li>Biomarker results \u2192 events (they have a test date)</li> <li>Staging information \u2192 events (stage at diagnosis date)</li> <li>Demographics like birth year, biological sex \u2192 constants (truly immutable)</li> </ul> </li> <li><p>Include all available data first - Start with everything, then trim during data generation if needed:</p> <ul> <li>Use the token budget in <code>ConverterInstruction</code> to control output length</li> <li>The framework automatically prioritizes recent and relevant events</li> </ul> </li> <li><p>Use preprocessing helpers wisely:</p> <ul> <li><code>identify_constant_and_changing_columns()</code> - Helps decide what goes where</li> <li><code>aggregate_events_to_weeks()</code> - Reduces noise from frequent measurements</li> </ul> </li> <li><p>Validate your data before training to catch format issues early.</p> </li> </ol>"},{"location":"examples/example_data/generate_example_data/","title":"Generate example data","text":"In\u00a0[\u00a0]: Copied! <pre>import argparse\nimport pandas as pd\nimport random\nimport datetime\nimport numpy as np\nimport os\n</pre> import argparse import pandas as pd import random import datetime import numpy as np import os In\u00a0[\u00a0]: Copied! <pre>CONSTANT_FEATURES = {\n    \"birthyear\": (1945, 1975),\n    \"gender\": [\"Male\", \"Female\"],\n    \"histology\": [\"Adenocarcinoma\", \"Squamous Cell Carcinoma\"],\n    \"smoking_history\": [\"Never\", \"Former\", \"Current\"],\n}\n</pre> CONSTANT_FEATURES = {     \"birthyear\": (1945, 1975),     \"gender\": [\"Male\", \"Female\"],     \"histology\": [\"Adenocarcinoma\", \"Squamous Cell Carcinoma\"],     \"smoking_history\": [\"Never\", \"Former\", \"Current\"], } In\u00a0[\u00a0]: Copied! <pre>CONSTANT_FEATURES_DESCRIPTION = {\n    \"patientid\": \"Unique patient identifier\",\n    \"birthyear\": \"Age of patient at first event\",\n    \"gender\": \"Gender of the patient\",\n    \"histology\": \"Histological subtype of NSCLC\",\n    \"smoking_history\": \"Smoking status at diagnosis\",\n}\n</pre> CONSTANT_FEATURES_DESCRIPTION = {     \"patientid\": \"Unique patient identifier\",     \"birthyear\": \"Age of patient at first event\",     \"gender\": \"Gender of the patient\",     \"histology\": \"Histological subtype of NSCLC\",     \"smoking_history\": \"Smoking status at diagnosis\", } In\u00a0[\u00a0]: Copied! <pre># NSCLC Specific Lab Tests (loinc_code: (name, unit, mean, std_dev))\nLAB_TESTS = {\n    \"718-7\": (\"hemoglobin\", \"g/dL\", 13.5, 1.5),\n    \"26464-8\": (\"leukocytes\", \"10*9/L\", 7.0, 2.0),\n    \"26515-7\": (\"platelets\", \"10*9/L\", 250, 60),\n    \"2160-0\": (\"creatinine\", \"mg/dL\", 0.9, 0.3),\n    \"1742-6\": (\"alanine aminotransferase\", \"U/L\", 25, 10),\n    \"1751-7\": (\"albumin\", \"g/L\", 40, 5),\n    \"48642-3\": (\"eGFR\", \"ml/min/1.73m2\", 80, 20),\n    \"2951-2\": (\"sodium\", \"mmol/L\", 140, 3),\n    \"2823-3\": (\"potassium\", \"mmol/L\", 4.0, 0.4),\n}\n</pre> # NSCLC Specific Lab Tests (loinc_code: (name, unit, mean, std_dev)) LAB_TESTS = {     \"718-7\": (\"hemoglobin\", \"g/dL\", 13.5, 1.5),     \"26464-8\": (\"leukocytes\", \"10*9/L\", 7.0, 2.0),     \"26515-7\": (\"platelets\", \"10*9/L\", 250, 60),     \"2160-0\": (\"creatinine\", \"mg/dL\", 0.9, 0.3),     \"1742-6\": (\"alanine aminotransferase\", \"U/L\", 25, 10),     \"1751-7\": (\"albumin\", \"g/L\", 40, 5),     \"48642-3\": (\"eGFR\", \"ml/min/1.73m2\", 80, 20),     \"2951-2\": (\"sodium\", \"mmol/L\", 140, 3),     \"2823-3\": (\"potassium\", \"mmol/L\", 4.0, 0.4), } In\u00a0[\u00a0]: Copied! <pre># NSCLC Regimens\nREGIMENS = {\n    \"chemo_io\": [\"Carboplatin\", \"Pemetrexed\", \"Pembrolizumab\"],\n    \"targeted_egfr\": [\"Osimertinib\"],\n    \"targeted_alk\": [\"Alectinib\"],\n    \"second_line_chemo\": [\"Docetaxel\", \"Ramucirumab\"],\n    \"second_line_io\": [\"Nivolumab\"],\n}\n</pre> # NSCLC Regimens REGIMENS = {     \"chemo_io\": [\"Carboplatin\", \"Pemetrexed\", \"Pembrolizumab\"],     \"targeted_egfr\": [\"Osimertinib\"],     \"targeted_alk\": [\"Alectinib\"],     \"second_line_chemo\": [\"Docetaxel\", \"Ramucirumab\"],     \"second_line_io\": [\"Nivolumab\"], } In\u00a0[\u00a0]: Copied! <pre>DIAGNOSIS_CODES = [\n    (\"C34.1\", \"Malignant neoplasm of upper lobe, bronchus or lung\"),\n    (\"C34.3\", \"Malignant neoplasm of lower lobe, bronchus or lung\"),\n    (\"C34.90\", \"Malignant neoplasm of unspecified part of unspecified bronchus or lung\"),\n]\n</pre> DIAGNOSIS_CODES = [     (\"C34.1\", \"Malignant neoplasm of upper lobe, bronchus or lung\"),     (\"C34.3\", \"Malignant neoplasm of lower lobe, bronchus or lung\"),     (\"C34.90\", \"Malignant neoplasm of unspecified part of unspecified bronchus or lung\"), ] In\u00a0[\u00a0]: Copied! <pre># Standard NGS Panel for Lung Cancer\nGENE_PANEL = [\"EGFR\", \"ALK\", \"ROS1\", \"BRAF\", \"KRAS\", \"MET\", \"RET\", \"NTRK1\", \"NTRK2\", \"NTRK3\", \"HER2\"]\n# Common co-mutations to add \"noise\"\nCO_MUTATIONS = [\"TP53\", \"STK11\", \"KEAP1\"]\n</pre> # Standard NGS Panel for Lung Cancer GENE_PANEL = [\"EGFR\", \"ALK\", \"ROS1\", \"BRAF\", \"KRAS\", \"MET\", \"RET\", \"NTRK1\", \"NTRK2\", \"NTRK3\", \"HER2\"] # Common co-mutations to add \"noise\" CO_MUTATIONS = [\"TP53\", \"STK11\", \"KEAP1\"] In\u00a0[\u00a0]: Copied! <pre>def get_random_date(start_year, end_year):\n    start = datetime.date(start_year, 1, 1)\n    end = datetime.date(end_year, 12, 31)\n    return start + datetime.timedelta(days=random.randint(0, (end - start).days))\n</pre> def get_random_date(start_year, end_year):     start = datetime.date(start_year, 1, 1)     end = datetime.date(end_year, 12, 31)     return start + datetime.timedelta(days=random.randint(0, (end - start).days)) In\u00a0[\u00a0]: Copied! <pre>def add_days(date_obj, days):\n    return date_obj + datetime.timedelta(days=days)\n</pre> def add_days(date_obj, days):     return date_obj + datetime.timedelta(days=days) In\u00a0[\u00a0]: Copied! <pre>def generate_lab_event(patientid, current_date, start_date, lab_code, lab_trends):\n    \"\"\"\n    Generates a lab value based on a linear trend specific to the patient.\n    Formula: Value = Baseline + (Slope * Days_Elapsed) + Noise\n    \"\"\"\n    name, unit, mean, std = LAB_TESTS[lab_code]\n\n    # Retrieve patient-specific trend parameters\n    patient_params = lab_trends[lab_code]\n    baseline = patient_params[\"baseline\"]\n    slope = patient_params[\"slope\"]\n\n    # Calculate days elapsed since diagnosis (or start date)\n    days_elapsed = (current_date - start_date).days\n\n    # Calculate linear trend value\n    trend_value = baseline + (slope * days_elapsed)\n\n    # Add slight random noise (intra-patient variation is usually smaller than population std)\n    # We use std / 10 to ensure the trend remains visible over the noise.\n    noise = np.random.normal(0, std / 10.0)\n\n    val = trend_value + noise\n    val = max(0, round(val, 2))  # Ensure positive\n\n    clean_name = f\"{name} - {lab_code}\"\n    desc = f\"Test: {name}, Cleaned lab units: {unit}\"\n\n    return [patientid, current_date, \"lab\", clean_name.replace(\" \", \"_\"), str(val), clean_name, desc]\n</pre> def generate_lab_event(patientid, current_date, start_date, lab_code, lab_trends):     \"\"\"     Generates a lab value based on a linear trend specific to the patient.     Formula: Value = Baseline + (Slope * Days_Elapsed) + Noise     \"\"\"     name, unit, mean, std = LAB_TESTS[lab_code]      # Retrieve patient-specific trend parameters     patient_params = lab_trends[lab_code]     baseline = patient_params[\"baseline\"]     slope = patient_params[\"slope\"]      # Calculate days elapsed since diagnosis (or start date)     days_elapsed = (current_date - start_date).days      # Calculate linear trend value     trend_value = baseline + (slope * days_elapsed)      # Add slight random noise (intra-patient variation is usually smaller than population std)     # We use std / 10 to ensure the trend remains visible over the noise.     noise = np.random.normal(0, std / 10.0)      val = trend_value + noise     val = max(0, round(val, 2))  # Ensure positive      clean_name = f\"{name} - {lab_code}\"     desc = f\"Test: {name}, Cleaned lab units: {unit}\"      return [patientid, current_date, \"lab\", clean_name.replace(\" \", \"_\"), str(val), clean_name, desc] In\u00a0[\u00a0]: Copied! <pre>def generate_vital_event(patientid, date, vital_type):\n    if vital_type == \"body_weight\":\n        val = np.random.normal(75, 15)\n        return [patientid, date, \"vitals\", \"body_weight\", str(round(val, 2)), \"body weight\", \"NA\"]\n    elif vital_type == \"ecog\":\n        val = random.choice([\"0\", \"1\", \"2\"])\n        return [patientid, date, \"ecog\", \"ecog\", val, \"ECOG\", \"NA\"]\n    return None\n</pre> def generate_vital_event(patientid, date, vital_type):     if vital_type == \"body_weight\":         val = np.random.normal(75, 15)         return [patientid, date, \"vitals\", \"body_weight\", str(round(val, 2)), \"body weight\", \"NA\"]     elif vital_type == \"ecog\":         val = random.choice([\"0\", \"1\", \"2\"])         return [patientid, date, \"ecog\", \"ecog\", val, \"ECOG\", \"NA\"]     return None In\u00a0[\u00a0]: Copied! <pre>def generate_genomic_profile(patientid, test_date, mutation_status):\n    \"\"\"\n    Generates a list of genetic results based on the patient's hidden driver status.\n    Output aligns with columns:\n    [patientid, date, biomarker_category, biomarker_event, biomarker_descriptive_name, biomarker_value, meta_data]\n    \"\"\"\n    genetic_records = []\n\n    # 1. Generate Panel Results (Primary Driver + Negatives)\n    for gene in GENE_PANEL:\n        # Default to negative/wild type\n        is_positive = False\n        variant = \"Wild Type\"\n\n        # Check if this gene is the driver\n        if gene == mutation_status:\n            is_positive = True\n            if gene == \"EGFR\":\n                variant = random.choice([\"Exon 19 Deletion\", \"L858R\", \"Exon 20 Insertion\"])\n            elif gene == \"ALK\":\n                variant = \"EML4-ALK Fusion\"\n            elif gene == \"KRAS\":\n                variant = \"G12C\"\n            else:\n                variant = \"Gain of Function\"\n\n        # Check for sporadic KRAS in WT patients\n        elif gene == \"KRAS\" and mutation_status == \"WT\":\n            if random.random() &lt; 0.25:\n                is_positive = True\n                variant = random.choice([\"G12C\", \"G12D\", \"G12V\"])\n\n        # --- Construct Descriptive Name &amp; Value ---\n        if is_positive:\n            # Descriptive: \"EGFR L858R\", Value: \"Present\"\n            descriptive_name = f\"{gene} {variant}\"\n            biomarker_value = \"Present\"\n        else:\n            # Descriptive: \"EGFR\", Value: \"Wild Type\"\n            descriptive_name = gene\n            biomarker_value = \"Wild Type\"\n\n        genetic_records.append(\n            [\n                patientid,\n                test_date,\n                \"basic_biomarker\",  # biomarker_category\n                gene,  # biomarker_event\n                descriptive_name,  # biomarker_descriptive_name\n                biomarker_value,  # biomarker_value\n                \"NGS\",  # meta_data\n            ]\n        )\n\n    # 2. Generate Co-mutations (Noise)\n    for gene in CO_MUTATIONS:\n        # 40% chance of having a TP53 or STK11 mutation\n        if random.random() &lt; 0.4:\n            variant = \"Missense\"\n            descriptive_name = f\"{gene} {variant}\"\n\n            genetic_records.append(\n                [\n                    patientid,\n                    test_date,\n                    \"gene_sv\",  # biomarker_category\n                    gene,  # biomarker_event\n                    descriptive_name,  # biomarker_descriptive_name\n                    \"Present\",  # biomarker_value\n                    \"NGS\",  # meta_data\n                ]\n            )\n\n    # 3. Generate PD-L1 (IHC)\n    pdl1_score = random.choice([\"&lt; 1%\", \"1-49%\", \"&gt;= 50%\"])\n\n    # Descriptive: \"PD-L1 Expression\", Value: the actual score range\n    genetic_records.append(\n        [\n            patientid,\n            test_date,\n            \"biomarker_ihc\",  # biomarker_category\n            \"PD-L1\",  # biomarker_event\n            \"PD-L1 Expression (TPS)\",  # biomarker_descriptive_name\n            pdl1_score,  # biomarker_value\n            \"IHC 22C3\",  # meta_data\n        ]\n    )\n\n    # 4. Generate Signatures (TMB)\n    if mutation_status in [\"EGFR\", \"ALK\"]:\n        tmb_val = round(np.random.normal(5, 2), 2)\n    else:\n        tmb_val = round(np.random.normal(12, 5), 2)\n\n    genetic_records.append(\n        [\n            patientid,\n            test_date,\n            \"signature\",  # biomarker_category\n            \"TMB\",  # biomarker_event\n            \"Tumor Mutational Burden\",  # biomarker_descriptive_name\n            max(0, tmb_val),  # biomarker_value\n            \"NGS\",  # meta_data\n        ]\n    )\n\n    return genetic_records\n</pre> def generate_genomic_profile(patientid, test_date, mutation_status):     \"\"\"     Generates a list of genetic results based on the patient's hidden driver status.     Output aligns with columns:     [patientid, date, biomarker_category, biomarker_event, biomarker_descriptive_name, biomarker_value, meta_data]     \"\"\"     genetic_records = []      # 1. Generate Panel Results (Primary Driver + Negatives)     for gene in GENE_PANEL:         # Default to negative/wild type         is_positive = False         variant = \"Wild Type\"          # Check if this gene is the driver         if gene == mutation_status:             is_positive = True             if gene == \"EGFR\":                 variant = random.choice([\"Exon 19 Deletion\", \"L858R\", \"Exon 20 Insertion\"])             elif gene == \"ALK\":                 variant = \"EML4-ALK Fusion\"             elif gene == \"KRAS\":                 variant = \"G12C\"             else:                 variant = \"Gain of Function\"          # Check for sporadic KRAS in WT patients         elif gene == \"KRAS\" and mutation_status == \"WT\":             if random.random() &lt; 0.25:                 is_positive = True                 variant = random.choice([\"G12C\", \"G12D\", \"G12V\"])          # --- Construct Descriptive Name &amp; Value ---         if is_positive:             # Descriptive: \"EGFR L858R\", Value: \"Present\"             descriptive_name = f\"{gene} {variant}\"             biomarker_value = \"Present\"         else:             # Descriptive: \"EGFR\", Value: \"Wild Type\"             descriptive_name = gene             biomarker_value = \"Wild Type\"          genetic_records.append(             [                 patientid,                 test_date,                 \"basic_biomarker\",  # biomarker_category                 gene,  # biomarker_event                 descriptive_name,  # biomarker_descriptive_name                 biomarker_value,  # biomarker_value                 \"NGS\",  # meta_data             ]         )      # 2. Generate Co-mutations (Noise)     for gene in CO_MUTATIONS:         # 40% chance of having a TP53 or STK11 mutation         if random.random() &lt; 0.4:             variant = \"Missense\"             descriptive_name = f\"{gene} {variant}\"              genetic_records.append(                 [                     patientid,                     test_date,                     \"gene_sv\",  # biomarker_category                     gene,  # biomarker_event                     descriptive_name,  # biomarker_descriptive_name                     \"Present\",  # biomarker_value                     \"NGS\",  # meta_data                 ]             )      # 3. Generate PD-L1 (IHC)     pdl1_score = random.choice([\"&lt; 1%\", \"1-49%\", \"&gt;= 50%\"])      # Descriptive: \"PD-L1 Expression\", Value: the actual score range     genetic_records.append(         [             patientid,             test_date,             \"biomarker_ihc\",  # biomarker_category             \"PD-L1\",  # biomarker_event             \"PD-L1 Expression (TPS)\",  # biomarker_descriptive_name             pdl1_score,  # biomarker_value             \"IHC 22C3\",  # meta_data         ]     )      # 4. Generate Signatures (TMB)     if mutation_status in [\"EGFR\", \"ALK\"]:         tmb_val = round(np.random.normal(5, 2), 2)     else:         tmb_val = round(np.random.normal(12, 5), 2)      genetic_records.append(         [             patientid,             test_date,             \"signature\",  # biomarker_category             \"TMB\",  # biomarker_event             \"Tumor Mutational Burden\",  # biomarker_descriptive_name             max(0, tmb_val),  # biomarker_value             \"NGS\",  # meta_data         ]     )      return genetic_records In\u00a0[\u00a0]: Copied! <pre>def main(num_patients_to_generate, seed, save_folder):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    all_constant = []\n    all_events = []\n    all_genetic = []\n\n    event_columns = [\n        \"patientid\",\n        \"date\",\n        \"event_category\",\n        \"event_name\",\n        \"event_value\",\n        \"event_descriptive_name\",\n        \"meta_data\",\n    ]\n\n    genetic_columns = [\n        \"patientid\",\n        \"date\",\n        \"event_category\",  # basic_biomarker, gene_sv, signature\n        \"event_name\",  # EGFR, ALK, TMB\n        \"event_descriptive_name\",  # e.g. \"EGFR L858R\", \"PD-L1 Expression\"\n        \"event_value\",  # \"Present\", \"Wild Type\", \"&gt;= 50%\"\n        \"meta_data\",\n    ]\n\n    print(f\"Generating {num_patients_to_generate} patients with NSCLC trajectories...\")\n\n    #: iterate over number of patients\n    for patient_idx in range(num_patients_to_generate):\n        patientid = f\"p{patient_idx}\"\n\n        # 1. Generate Constant Data\n        constant_data = {\"patientid\": patientid}\n        for const_feat, const_val in CONSTANT_FEATURES.items():\n            if isinstance(const_val, list):\n                chosen_val = random.choice(const_val)\n                constant_data[const_feat] = chosen_val\n            else:\n                chosen_val = random.randint(const_val[0], const_val[1])\n                constant_data[const_feat] = chosen_val\n\n        all_constant.append(constant_data)\n\n        # -----------------------------------------------------------------\n        # PRE-CALCULATE LAB TRENDS FOR THIS PATIENT\n        # -----------------------------------------------------------------\n        patient_lab_trends = {}\n        for lab_code, (name, unit, mean, std) in LAB_TESTS.items():\n            # 1. Establish a baseline for this patient (offset from population mean)\n            baseline = np.random.normal(mean, std)\n\n            # 2. Determine a slope (change per day).\n            # We assume a max drift of roughly 1.5 standard deviations over a year (365 days)\n            # This ensures the trend is visible but not immediately catastrophic.\n            max_daily_drift = (std * 1.5) / 365.0\n            slope = random.uniform(-max_daily_drift, max_daily_drift)\n\n            patient_lab_trends[lab_code] = {\"baseline\": baseline, \"slope\": slope}\n        # -----------------------------------------------------------------\n\n        # 2. Determine Genetic Profile (Hidden driver of the simulation)\n        # 15% EGFR+, 5% ALK+, rest Wild Type\n        r = random.random()\n        if r &lt; 0.15:\n            mutation_status = \"EGFR\"\n        elif r &lt; 0.20:\n            mutation_status = \"ALK\"\n        else:\n            mutation_status = \"WT\"\n\n        # 3. Generate Events Data\n\n        # A. Diagnosis\n        diagnosis_date = get_random_date(2015, 2022)\n        dx_code, dx_desc = random.choice(DIAGNOSIS_CODES)\n\n        all_events.append(\n            [\n                patientid,\n                diagnosis_date,\n                \"main_diagnosis\",\n                \"initial_diagnosis\",\n                \"NSCLC\",\n                \"initial cancer diagnosis\",\n                \"Non-Small Cell Lung Cancer\",\n            ]\n        )\n        all_events.append(\n            [patientid, diagnosis_date, \"diagnosis\", dx_code, \"diagnosed\", f\"{dx_desc} ({dx_code})\", dx_desc]\n        )\n\n        # B. Biomarker Testing (Genetic Data in Event Stream)\n        test_date = add_days(diagnosis_date, random.randint(7, 21))\n\n        # Add a high level event indicating testing happened\n        all_events.append(\n            [patientid, test_date, \"lab\", \"biomarker_test\", \"performed\", \"Molecular Profiling\", \"NGS Panel\"]\n        )\n\n        # --- GENERATE GENETIC DATA HERE ---\n        patient_genetics = generate_genomic_profile(patientid, test_date, mutation_status)\n        all_genetic.extend(patient_genetics)\n        # ----------------------------------\n\n        # Add clinical summaries of genetics to the event stream for easy reading\n        if mutation_status == \"EGFR\":\n            all_events.append(\n                [\n                    patientid,\n                    test_date,\n                    \"biomarker\",\n                    \"EGFR\",\n                    \"Positive\",\n                    \"Epidermal Growth Factor Receptor\",\n                    \"Exon 19/21\",\n                ]\n            )\n        elif mutation_status == \"ALK\":\n            all_events.append(\n                [patientid, test_date, \"biomarker\", \"ALK\", \"Positive\", \"Anaplastic Lymphoma Kinase\", \"Translocation\"]\n            )\n        else:\n            all_events.append(\n                [patientid, test_date, \"biomarker\", \"EGFR\", \"Negative\", \"Epidermal Growth Factor Receptor\", \"WT\"]\n            )\n            all_events.append(\n                [patientid, test_date, \"biomarker\", \"ALK\", \"Negative\", \"Anaplastic Lymphoma Kinase\", \"WT\"]\n            )\n\n        # C. Treatment Trajectory\n        current_date = add_days(test_date, random.randint(7, 14))\n\n        # Determine number of lines (1 to 3) based on survival/progression logic\n        num_lines = random.choice([1, 1, 2, 2, 3])\n\n        for line_num in range(1, num_lines + 1):\n            # Select Regimen based on Line and Genetics\n            regimen_drugs = []\n            regimen_name = \"\"\n\n            if line_num == 1:\n                if mutation_status == \"EGFR\":\n                    regimen_drugs = REGIMENS[\"targeted_egfr\"]\n                    regimen_name = \"Osimertinib Monotherapy\"\n                elif mutation_status == \"ALK\":\n                    regimen_drugs = REGIMENS[\"targeted_alk\"]\n                    regimen_name = \"Alectinib Monotherapy\"\n                else:\n                    regimen_drugs = REGIMENS[\"chemo_io\"]\n                    regimen_name = \"Carboplatin/Pemetrexed/Pembrolizumab\"\n            elif line_num == 2:\n                if mutation_status in [\"EGFR\", \"ALK\"]:\n                    regimen_drugs = REGIMENS[\"chemo_io\"]  # Switch to chemo after TKI fails\n                    regimen_name = \"Platinum Doublet\"\n                else:\n                    regimen_drugs = REGIMENS[\"second_line_chemo\"]\n                    regimen_name = \"Docetaxel/Ramucirumab\"\n            else:\n                regimen_drugs = REGIMENS[\"second_line_io\"]  # Salvage\n                regimen_name = \"Nivolumab\"\n\n            line_start_date = current_date\n\n            # Define length of this line (e.g., 3 to 12 months)\n            line_duration_days = random.randint(90, 360)\n            line_end_date = add_days(line_start_date, line_duration_days)\n\n            # Record Line Start Metadata\n            regimen_str = \",\".join(regimen_drugs)\n            all_events.append([patientid, line_start_date, \"lot\", \"line_number\", str(line_num), \"line number\", \"NA\"])\n            all_events.append([patientid, line_start_date, \"lot\", \"line_name\", regimen_name, \"line of therapy\", \"NA\"])\n            all_events.append(\n                [patientid, line_start_date, \"lot\", \"is_maintenance_therapy\", \"FALSE\", \"is maintenance therapy\", \"NA\"]\n            )\n\n            for drug in regimen_drugs:\n                all_events.append([patientid, line_start_date, \"lot\", drug.lower(), \"LoT Start\", \"LoT\", \"NA\"])\n\n            # Simulate Cycles/Visits within this line\n            # Cycles are typically every 21 or 28 days, with some noise added\n            cycle_length = 21 + random.choice([0, 0, 0, 0, 0, 7, 7])  # Mostly 21 days\n            visit_date = line_start_date\n\n            while visit_date &lt; line_end_date:\n                # 1. Labs (Complete Blood Count &amp; Chemistry)\n                for lab_code in LAB_TESTS.keys():\n                    # UPDATED CALL: Pass diagnosis_date and patient_lab_trends\n                    all_events.append(\n                        generate_lab_event(patientid, visit_date, diagnosis_date, lab_code, patient_lab_trends)\n                    )\n\n                # 2. Vitals\n                all_events.append(generate_vital_event(patientid, visit_date, \"body_weight\"))\n                if random.random() &lt; 0.3:  # ECOG recorded less frequently\n                    all_events.append(generate_vital_event(patientid, visit_date, \"ecog\"))\n\n                # 3. Drug Administration\n                for drug in regimen_drugs:\n                    all_events.append(\n                        [patientid, visit_date, \"drug\", drug.lower(), \"administered\", drug.lower(), regimen_str]\n                    )\n\n                # Advance time\n                visit_date = add_days(visit_date, cycle_length)\n\n            # Gap before next line (progression wash-out)\n            current_date = add_days(line_end_date, random.randint(14, 45))\n\n            # Add a progression event, radomly since sometimes patients stop for toxicity\n            if random.random() &lt; 0.6:\n                all_events.append(\n                    [\n                        patientid,\n                        line_end_date,\n                        \"progression\",\n                        \"progression\",\n                        \"progression\",\n                        \"progression\",\n                        \"Radiological PD\",\n                    ]\n                )\n\n        # Add death event at end of trajectory\n        death_date = add_days(current_date, random.randint(30, 180))\n        all_events.append([patientid, death_date, \"death\", \"death\", \"death\", \"Death\", \"Cause: Cancer Progression\"])\n\n    # Generate constant_description dataframe\n    df_constant_description = pd.DataFrame(CONSTANT_FEATURES_DESCRIPTION.items(), columns=[\"variable\", \"comment\"])\n\n    # Convert all to dataframes\n    df_constant = pd.DataFrame(all_constant)\n    df_events = pd.DataFrame(all_events, columns=event_columns)\n    df_genetic = pd.DataFrame(all_genetic, columns=genetic_columns)\n\n    # Merge genetic data into events\n    df_events[\"source\"] = \"events\"\n    df_genetic[\"source\"] = \"genetic\"\n    df_events = pd.concat([df_events, df_genetic], ignore_index=True)\n\n    # Sort events by patient and date\n    df_events.sort_values(by=[\"patientid\", \"date\"], inplace=True)\n\n    # Create Output Directory\n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder)\n\n    # Save Data\n    df_constant.to_csv(os.path.join(save_folder, \"constant.csv\"), index=False)\n    df_events.to_csv(os.path.join(save_folder, \"events.csv\"), index=False)\n    df_constant_description.to_csv(os.path.join(save_folder, \"constant_description.csv\"), index=False)\n\n    print(f\"Successfully generated data for {num_patients_to_generate} patients in '{save_folder}'.\")\n</pre> def main(num_patients_to_generate, seed, save_folder):     random.seed(seed)     np.random.seed(seed)      all_constant = []     all_events = []     all_genetic = []      event_columns = [         \"patientid\",         \"date\",         \"event_category\",         \"event_name\",         \"event_value\",         \"event_descriptive_name\",         \"meta_data\",     ]      genetic_columns = [         \"patientid\",         \"date\",         \"event_category\",  # basic_biomarker, gene_sv, signature         \"event_name\",  # EGFR, ALK, TMB         \"event_descriptive_name\",  # e.g. \"EGFR L858R\", \"PD-L1 Expression\"         \"event_value\",  # \"Present\", \"Wild Type\", \"&gt;= 50%\"         \"meta_data\",     ]      print(f\"Generating {num_patients_to_generate} patients with NSCLC trajectories...\")      #: iterate over number of patients     for patient_idx in range(num_patients_to_generate):         patientid = f\"p{patient_idx}\"          # 1. Generate Constant Data         constant_data = {\"patientid\": patientid}         for const_feat, const_val in CONSTANT_FEATURES.items():             if isinstance(const_val, list):                 chosen_val = random.choice(const_val)                 constant_data[const_feat] = chosen_val             else:                 chosen_val = random.randint(const_val[0], const_val[1])                 constant_data[const_feat] = chosen_val          all_constant.append(constant_data)          # -----------------------------------------------------------------         # PRE-CALCULATE LAB TRENDS FOR THIS PATIENT         # -----------------------------------------------------------------         patient_lab_trends = {}         for lab_code, (name, unit, mean, std) in LAB_TESTS.items():             # 1. Establish a baseline for this patient (offset from population mean)             baseline = np.random.normal(mean, std)              # 2. Determine a slope (change per day).             # We assume a max drift of roughly 1.5 standard deviations over a year (365 days)             # This ensures the trend is visible but not immediately catastrophic.             max_daily_drift = (std * 1.5) / 365.0             slope = random.uniform(-max_daily_drift, max_daily_drift)              patient_lab_trends[lab_code] = {\"baseline\": baseline, \"slope\": slope}         # -----------------------------------------------------------------          # 2. Determine Genetic Profile (Hidden driver of the simulation)         # 15% EGFR+, 5% ALK+, rest Wild Type         r = random.random()         if r &lt; 0.15:             mutation_status = \"EGFR\"         elif r &lt; 0.20:             mutation_status = \"ALK\"         else:             mutation_status = \"WT\"          # 3. Generate Events Data          # A. Diagnosis         diagnosis_date = get_random_date(2015, 2022)         dx_code, dx_desc = random.choice(DIAGNOSIS_CODES)          all_events.append(             [                 patientid,                 diagnosis_date,                 \"main_diagnosis\",                 \"initial_diagnosis\",                 \"NSCLC\",                 \"initial cancer diagnosis\",                 \"Non-Small Cell Lung Cancer\",             ]         )         all_events.append(             [patientid, diagnosis_date, \"diagnosis\", dx_code, \"diagnosed\", f\"{dx_desc} ({dx_code})\", dx_desc]         )          # B. Biomarker Testing (Genetic Data in Event Stream)         test_date = add_days(diagnosis_date, random.randint(7, 21))          # Add a high level event indicating testing happened         all_events.append(             [patientid, test_date, \"lab\", \"biomarker_test\", \"performed\", \"Molecular Profiling\", \"NGS Panel\"]         )          # --- GENERATE GENETIC DATA HERE ---         patient_genetics = generate_genomic_profile(patientid, test_date, mutation_status)         all_genetic.extend(patient_genetics)         # ----------------------------------          # Add clinical summaries of genetics to the event stream for easy reading         if mutation_status == \"EGFR\":             all_events.append(                 [                     patientid,                     test_date,                     \"biomarker\",                     \"EGFR\",                     \"Positive\",                     \"Epidermal Growth Factor Receptor\",                     \"Exon 19/21\",                 ]             )         elif mutation_status == \"ALK\":             all_events.append(                 [patientid, test_date, \"biomarker\", \"ALK\", \"Positive\", \"Anaplastic Lymphoma Kinase\", \"Translocation\"]             )         else:             all_events.append(                 [patientid, test_date, \"biomarker\", \"EGFR\", \"Negative\", \"Epidermal Growth Factor Receptor\", \"WT\"]             )             all_events.append(                 [patientid, test_date, \"biomarker\", \"ALK\", \"Negative\", \"Anaplastic Lymphoma Kinase\", \"WT\"]             )          # C. Treatment Trajectory         current_date = add_days(test_date, random.randint(7, 14))          # Determine number of lines (1 to 3) based on survival/progression logic         num_lines = random.choice([1, 1, 2, 2, 3])          for line_num in range(1, num_lines + 1):             # Select Regimen based on Line and Genetics             regimen_drugs = []             regimen_name = \"\"              if line_num == 1:                 if mutation_status == \"EGFR\":                     regimen_drugs = REGIMENS[\"targeted_egfr\"]                     regimen_name = \"Osimertinib Monotherapy\"                 elif mutation_status == \"ALK\":                     regimen_drugs = REGIMENS[\"targeted_alk\"]                     regimen_name = \"Alectinib Monotherapy\"                 else:                     regimen_drugs = REGIMENS[\"chemo_io\"]                     regimen_name = \"Carboplatin/Pemetrexed/Pembrolizumab\"             elif line_num == 2:                 if mutation_status in [\"EGFR\", \"ALK\"]:                     regimen_drugs = REGIMENS[\"chemo_io\"]  # Switch to chemo after TKI fails                     regimen_name = \"Platinum Doublet\"                 else:                     regimen_drugs = REGIMENS[\"second_line_chemo\"]                     regimen_name = \"Docetaxel/Ramucirumab\"             else:                 regimen_drugs = REGIMENS[\"second_line_io\"]  # Salvage                 regimen_name = \"Nivolumab\"              line_start_date = current_date              # Define length of this line (e.g., 3 to 12 months)             line_duration_days = random.randint(90, 360)             line_end_date = add_days(line_start_date, line_duration_days)              # Record Line Start Metadata             regimen_str = \",\".join(regimen_drugs)             all_events.append([patientid, line_start_date, \"lot\", \"line_number\", str(line_num), \"line number\", \"NA\"])             all_events.append([patientid, line_start_date, \"lot\", \"line_name\", regimen_name, \"line of therapy\", \"NA\"])             all_events.append(                 [patientid, line_start_date, \"lot\", \"is_maintenance_therapy\", \"FALSE\", \"is maintenance therapy\", \"NA\"]             )              for drug in regimen_drugs:                 all_events.append([patientid, line_start_date, \"lot\", drug.lower(), \"LoT Start\", \"LoT\", \"NA\"])              # Simulate Cycles/Visits within this line             # Cycles are typically every 21 or 28 days, with some noise added             cycle_length = 21 + random.choice([0, 0, 0, 0, 0, 7, 7])  # Mostly 21 days             visit_date = line_start_date              while visit_date &lt; line_end_date:                 # 1. Labs (Complete Blood Count &amp; Chemistry)                 for lab_code in LAB_TESTS.keys():                     # UPDATED CALL: Pass diagnosis_date and patient_lab_trends                     all_events.append(                         generate_lab_event(patientid, visit_date, diagnosis_date, lab_code, patient_lab_trends)                     )                  # 2. Vitals                 all_events.append(generate_vital_event(patientid, visit_date, \"body_weight\"))                 if random.random() &lt; 0.3:  # ECOG recorded less frequently                     all_events.append(generate_vital_event(patientid, visit_date, \"ecog\"))                  # 3. Drug Administration                 for drug in regimen_drugs:                     all_events.append(                         [patientid, visit_date, \"drug\", drug.lower(), \"administered\", drug.lower(), regimen_str]                     )                  # Advance time                 visit_date = add_days(visit_date, cycle_length)              # Gap before next line (progression wash-out)             current_date = add_days(line_end_date, random.randint(14, 45))              # Add a progression event, radomly since sometimes patients stop for toxicity             if random.random() &lt; 0.6:                 all_events.append(                     [                         patientid,                         line_end_date,                         \"progression\",                         \"progression\",                         \"progression\",                         \"progression\",                         \"Radiological PD\",                     ]                 )          # Add death event at end of trajectory         death_date = add_days(current_date, random.randint(30, 180))         all_events.append([patientid, death_date, \"death\", \"death\", \"death\", \"Death\", \"Cause: Cancer Progression\"])      # Generate constant_description dataframe     df_constant_description = pd.DataFrame(CONSTANT_FEATURES_DESCRIPTION.items(), columns=[\"variable\", \"comment\"])      # Convert all to dataframes     df_constant = pd.DataFrame(all_constant)     df_events = pd.DataFrame(all_events, columns=event_columns)     df_genetic = pd.DataFrame(all_genetic, columns=genetic_columns)      # Merge genetic data into events     df_events[\"source\"] = \"events\"     df_genetic[\"source\"] = \"genetic\"     df_events = pd.concat([df_events, df_genetic], ignore_index=True)      # Sort events by patient and date     df_events.sort_values(by=[\"patientid\", \"date\"], inplace=True)      # Create Output Directory     if not os.path.exists(save_folder):         os.makedirs(save_folder)      # Save Data     df_constant.to_csv(os.path.join(save_folder, \"constant.csv\"), index=False)     df_events.to_csv(os.path.join(save_folder, \"events.csv\"), index=False)     df_constant_description.to_csv(os.path.join(save_folder, \"constant_description.csv\"), index=False)      print(f\"Successfully generated data for {num_patients_to_generate} patients in '{save_folder}'.\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Generate example patient data.\")\n    parser.add_argument(\n        \"--num_patients_to_generate\",\n        type=int,\n        default=50,\n        help=\"Number of patients to generate.\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=42,\n        help=\"Random seed for reproducibility.\",\n    )\n\n    parser.add_argument(\n        \"--save_folder\",\n        type=str,\n        default=os.path.dirname(os.path.abspath(__file__)),\n        help=\"Folder to save the generated example data.\",\n    )\n\n    args = parser.parse_args()\n\n    main(args.num_patients_to_generate, args.seed, args.save_folder)\n</pre> if __name__ == \"__main__\":     parser = argparse.ArgumentParser(description=\"Generate example patient data.\")     parser.add_argument(         \"--num_patients_to_generate\",         type=int,         default=50,         help=\"Number of patients to generate.\",     )     parser.add_argument(         \"--seed\",         type=int,         default=42,         help=\"Random seed for reproducibility.\",     )      parser.add_argument(         \"--save_folder\",         type=str,         default=os.path.dirname(os.path.abspath(__file__)),         help=\"Folder to save the generated example data.\",     )      args = parser.parse_args()      main(args.num_patients_to_generate, args.seed, args.save_folder)"},{"location":"examples/example_data/generate_example_data/#constants-dictionaries","title":"CONSTANTS &amp; DICTIONARIES\u00b6","text":""},{"location":"examples/example_data/generate_example_data/#helper-functions","title":"HELPER FUNCTIONS\u00b6","text":""},{"location":"examples/hackathon/","title":"\ud83c\udfc6 TwinWeaver Hackathon Challenges","text":"<p>Welcome to the TwinWeaver Hackathon! These interactive notebooks are designed to test your understanding of clinical data processing and LLM fine-tuning for medical forecasting.</p>"},{"location":"examples/hackathon/#challenge-format","title":"\ud83c\udfaf Challenge Format","text":"<p>Unlike the standard tutorials, these notebooks require active participation:</p> <ul> <li>\ud83d\udd34 TODO Sections: Code cells marked with <code># TODO:</code> require you to write code</li> <li>\u2753 Quiz Questions: Test your understanding before moving forward</li> <li>\ud83e\uddea Experiments: Design and run your own experiments</li> <li>\ud83c\udfc1 Checkpoints: Verify your solutions before proceeding</li> <li>\ud83c\udf1f Bonus Challenges: Optional advanced tasks for extra credit</li> </ul>"},{"location":"examples/hackathon/#available-challenges","title":"\ud83d\udcda Available Challenges","text":""},{"location":"examples/hackathon/#challenge-1-data-preparation-01_data_preparation_challengeipynb","title":"Challenge 1: Data Preparation (<code>01_data_preparation_challenge.ipynb</code>)","text":"<p>Difficulty: \u2b50\u2b50 (Intermediate) Time: 45-60 minutes</p> <p>Learn to transform raw clinical data into instruction-tuning format by: - Configuring the data pipeline from scratch - Understanding split strategies for patient timelines - Debugging common configuration errors - Experimenting with different forecasting targets</p>"},{"location":"examples/hackathon/#challenge-2-end-to-end-llm-fine-tuning-02_llm_finetuning_challengeipynb","title":"Challenge 2: End-to-End LLM Fine-tuning (<code>02_llm_finetuning_challenge.ipynb</code>)","text":"<p>Difficulty: \u2b50\u2b50\u2b50 (Advanced) Time: 90-120 minutes</p> <p>Master the complete LLM fine-tuning workflow by: - Setting up training data generation - Configuring LoRA hyperparameters - Analyzing training dynamics - Running inference experiments - Evaluating model predictions</p>"},{"location":"examples/hackathon/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Make sure you have TwinWeaver installed: <code>pip install twinweaver</code></li> <li>For Challenge 2, also install: <code>pip install twinweaver[fine-tuning-example]</code></li> <li>Open the challenge notebook and follow the instructions</li> <li>Don't peek at the solutions until you've tried!</li> </ol>"},{"location":"examples/hackathon/#tips-for-success","title":"\ud83d\udca1 Tips for Success","text":"<ul> <li>Read the context: Each section provides background information</li> <li>Check the hints: Stuck? Look for the \ud83d\udca1 hint markers</li> <li>Run checkpoints: Use the validation cells to verify your code</li> <li>Experiment freely: The bonus sections encourage exploration</li> </ul>"},{"location":"examples/hackathon/#scoring-optional","title":"\ud83d\udcca Scoring (Optional)","text":"<p>If running as a competition: - Each TODO section: 10 points - Each quiz question: 5 points - Bonus challenges: 15-25 points each - Clean, well-documented code: up to 10 bonus points</p> <p>Good luck! \ud83c\udf40</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/","title":"\ud83c\udfc6 Challenge 1: Data Preparation for Training","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n)\n</pre> import pandas as pd  from twinweaver import (     DataManager,     Config, ) In\u00a0[\u00a0]: Copied! <pre># Load the example data\ndf_events = pd.read_csv(\"../example_data/events.csv\")\ndf_constant = pd.read_csv(\"../example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"../example_data/constant_description.csv\")\n</pre> # Load the example data df_events = pd.read_csv(\"../example_data/events.csv\") df_constant = pd.read_csv(\"../example_data/constant.csv\") df_constant_description = pd.read_csv(\"../example_data/constant_description.csv\") In\u00a0[\u00a0]: Copied! <pre># TODO: Explore df_events - what columns does it have? What are the unique event categories?\n# Write your exploration code here\n</pre> # TODO: Explore df_events - what columns does it have? What are the unique event categories? # Write your exploration code here  In\u00a0[\u00a0]: Copied! <pre># TODO: Explore df_constant - what patient-level information is available?\n</pre> # TODO: Explore df_constant - what patient-level information is available?  In\u00a0[\u00a0]: Copied! <pre># TODO: Explore df_constant_description - how does this map to df_constant?\n</pre> # TODO: Explore df_constant_description - how does this map to df_constant?  <p>Your Answers:</p> <p>Q1.1:</p> <p>Q1.2:</p> <p>Q1.3:</p> <p>Q1.4:</p> In\u00a0[\u00a0]: Copied! <pre>config = Config()\n\n# TODO: Set the event category used for splitting patient timelines\n# HINT: Look at your answer to Q1.2 - which category represents treatment lines?\nconfig.split_event_category = None  # Replace None with the correct value\n\n# TODO: Set which event categories should be forecasted as time-series\n# HINT: We want to predict future lab values\nconfig.event_category_forecast = None  # Replace None with a list\n\n# TODO: Configure time-to-event prediction targets\n# HINT: This should be a dictionary mapping event names to display names\n# Example: {\"original_name\": \"display name in prompt\"}\nconfig.data_splitter_events_variables_category_mapping = None  # Replace with dict\n</pre> config = Config()  # TODO: Set the event category used for splitting patient timelines # HINT: Look at your answer to Q1.2 - which category represents treatment lines? config.split_event_category = None  # Replace None with the correct value  # TODO: Set which event categories should be forecasted as time-series # HINT: We want to predict future lab values config.event_category_forecast = None  # Replace None with a list  # TODO: Configure time-to-event prediction targets # HINT: This should be a dictionary mapping event names to display names # Example: {\"original_name\": \"display name in prompt\"} config.data_splitter_events_variables_category_mapping = None  # Replace with dict In\u00a0[\u00a0]: Copied! <pre># Run this cell to check your configuration\ndef validate_config_part1(config):\n    errors = []\n\n    if config.split_event_category is None:\n        errors.append(\"\u274c split_event_category is not set\")\n    elif config.split_event_category not in df_events[\"event_category\"].unique():\n        errors.append(f\"\u274c split_event_category '{config.split_event_category}' not found in data\")\n    else:\n        print(f\"\u2705 split_event_category: '{config.split_event_category}'\")\n\n    if config.event_category_forecast is None:\n        errors.append(\"\u274c event_category_forecast is not set\")\n    elif not isinstance(config.event_category_forecast, list):\n        errors.append(\"\u274c event_category_forecast should be a list\")\n    elif any([cat not in df_events[\"event_category\"].unique() for cat in config.event_category_forecast]):\n        errors.append(\"\u274c At least one of the event_category_forecast values not found in data\")\n    else:\n        print(f\"\u2705 event_category_forecast: {config.event_category_forecast}\")\n\n    if config.data_splitter_events_variables_category_mapping is None:\n        errors.append(\"\u274c data_splitter_events_variables_category_mapping is not set\")\n    elif not isinstance(config.data_splitter_events_variables_category_mapping, dict):\n        errors.append(\"\u274c data_splitter_events_variables_category_mapping should be a dict\")\n    elif any(\n        [\n            cat not in df_events[\"event_category\"].unique()\n            for cat in config.data_splitter_events_variables_category_mapping.keys()\n        ]\n    ):\n        errors.append(\"\u274c At least one key in data_splitter_events_variables_category_mapping not found in data\")\n    else:\n        print(f\"\u2705 Event mapping: {config.data_splitter_events_variables_category_mapping}\")\n\n    if errors:\n        print(\"\\n\" + \"\\n\".join(errors))\n        print(\"\\n\ud83d\udca1 Hint: Review Part 1 exploration to find the correct values\")\n    else:\n        print(\"\\n\ud83c\udf89 Part 2.1 Complete! Configuration looks good.\")\n\n    return len(errors) == 0\n\n\nvalidate_config_part1(config)\n</pre> # Run this cell to check your configuration def validate_config_part1(config):     errors = []      if config.split_event_category is None:         errors.append(\"\u274c split_event_category is not set\")     elif config.split_event_category not in df_events[\"event_category\"].unique():         errors.append(f\"\u274c split_event_category '{config.split_event_category}' not found in data\")     else:         print(f\"\u2705 split_event_category: '{config.split_event_category}'\")      if config.event_category_forecast is None:         errors.append(\"\u274c event_category_forecast is not set\")     elif not isinstance(config.event_category_forecast, list):         errors.append(\"\u274c event_category_forecast should be a list\")     elif any([cat not in df_events[\"event_category\"].unique() for cat in config.event_category_forecast]):         errors.append(\"\u274c At least one of the event_category_forecast values not found in data\")     else:         print(f\"\u2705 event_category_forecast: {config.event_category_forecast}\")      if config.data_splitter_events_variables_category_mapping is None:         errors.append(\"\u274c data_splitter_events_variables_category_mapping is not set\")     elif not isinstance(config.data_splitter_events_variables_category_mapping, dict):         errors.append(\"\u274c data_splitter_events_variables_category_mapping should be a dict\")     elif any(         [             cat not in df_events[\"event_category\"].unique()             for cat in config.data_splitter_events_variables_category_mapping.keys()         ]     ):         errors.append(\"\u274c At least one key in data_splitter_events_variables_category_mapping not found in data\")     else:         print(f\"\u2705 Event mapping: {config.data_splitter_events_variables_category_mapping}\")      if errors:         print(\"\\n\" + \"\\n\".join(errors))         print(\"\\n\ud83d\udca1 Hint: Review Part 1 exploration to find the correct values\")     else:         print(\"\\n\ud83c\udf89 Part 2.1 Complete! Configuration looks good.\")      return len(errors) == 0   validate_config_part1(config) In\u00a0[\u00a0]: Copied! <pre># TODO: Look at df_constant columns and decide which ones to include\n# Consider: Which variables are clinically relevant for predictions?\n\n# First, explore what's available\nprint(\"Available columns in df_constant:\")\nprint(df_constant.columns.tolist())\n</pre> # TODO: Look at df_constant columns and decide which ones to include # Consider: Which variables are clinically relevant for predictions?  # First, explore what's available print(\"Available columns in df_constant:\") print(df_constant.columns.tolist()) In\u00a0[\u00a0]: Copied! <pre># TODO: Select which constant columns to use (list of column names)\nconfig.constant_columns_to_use = []  # Fill in the list\n\n# TODO: Specify which column contains birth year/date for age calculation\nconfig.constant_birthdate_column = None  # Set the column name\n</pre> # TODO: Select which constant columns to use (list of column names) config.constant_columns_to_use = []  # Fill in the list  # TODO: Specify which column contains birth year/date for age calculation config.constant_birthdate_column = None  # Set the column name In\u00a0[\u00a0]: Copied! <pre># TODO: Initialize DataManager and load data\n# The DataManager needs to:\n# 1. Be created with your config\n# 2. Load the indication data (events, constant, constant_description)\n# 3. Process the indication data\n# 4. Setup unique mapping of events\n# 5. Setup dataset splits\n# 6. Infer variable types\n\ndm = DataManager(config=config)\n\n# TODO: Call the required methods in the correct order\n# dm.????\n# dm.????\n# dm.????\n# dm.????\n# dm.????\n</pre> # TODO: Initialize DataManager and load data # The DataManager needs to: # 1. Be created with your config # 2. Load the indication data (events, constant, constant_description) # 3. Process the indication data # 4. Setup unique mapping of events # 5. Setup dataset splits # 6. Infer variable types  dm = DataManager(config=config)  # TODO: Call the required methods in the correct order # dm.???? # dm.???? # dm.???? # dm.???? # dm.???? In\u00a0[\u00a0]: Copied! <pre># Run this to verify DataManager is set up correctly\ntry:\n    n_patients = len(dm.all_patientids)\n    print(f\"\u2705 DataManager initialized with {n_patients} patients\")\n\n    # Check if we can get patient data\n    test_patient = dm.all_patientids[0]\n    patient_data = dm.get_patient_data(test_patient)\n    print(f\"\u2705 Successfully retrieved data for patient {test_patient}\")\n    print(f\"   - Events: {len(patient_data['events'])} rows\")\n    print(f\"   - Constant: {len(patient_data['constant'])} rows\")\n    print(\"\\n\ud83c\udf89 Part 3 Complete!\")\nexcept Exception as e:\n    print(f\"\u274c Error: {e}\")\n    print(\"\\n\ud83d\udca1 Hint: Make sure you called all DataManager methods in the correct order\")\n</pre> # Run this to verify DataManager is set up correctly try:     n_patients = len(dm.all_patientids)     print(f\"\u2705 DataManager initialized with {n_patients} patients\")      # Check if we can get patient data     test_patient = dm.all_patientids[0]     patient_data = dm.get_patient_data(test_patient)     print(f\"\u2705 Successfully retrieved data for patient {test_patient}\")     print(f\"   - Events: {len(patient_data['events'])} rows\")     print(f\"   - Constant: {len(patient_data['constant'])} rows\")     print(\"\\n\ud83c\udf89 Part 3 Complete!\") except Exception as e:     print(f\"\u274c Error: {e}\")     print(\"\\n\ud83d\udca1 Hint: Make sure you called all DataManager methods in the correct order\") <p>Your Answers:</p> <p>Q2.1:</p> <p>Q2.2:</p> <p>Q2.3:</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Initialize DataSplitterEvents\n# This handles event prediction tasks (death, progression)\ndata_splitter_events = None  # Create the splitter\n\n# TODO: Don't forget to call setup_variables() on it!\n</pre> # TODO: Initialize DataSplitterEvents # This handles event prediction tasks (death, progression) data_splitter_events = None  # Create the splitter  # TODO: Don't forget to call setup_variables() on it! In\u00a0[\u00a0]: Copied! <pre># TODO: Initialize DataSplitterForecasting\n# This handles continuous variable forecasting (lab values)\ndata_splitter_forecasting = None  # Create the splitter\n\n# TODO: Call setup_statistics() for forecasting QA and filtering\n</pre> # TODO: Initialize DataSplitterForecasting # This handles continuous variable forecasting (lab values) data_splitter_forecasting = None  # Create the splitter  # TODO: Call setup_statistics() for forecasting QA and filtering In\u00a0[\u00a0]: Copied! <pre># TODO: Combine both splitters using DataSplitter wrapper\ndata_splitter = None  # Create the combined splitter\n</pre> # TODO: Combine both splitters using DataSplitter wrapper data_splitter = None  # Create the combined splitter In\u00a0[\u00a0]: Copied! <pre># TODO: Initialize ConverterInstruction\n# Parameters needed:\n# - nr_tokens_budget_total: How many tokens can the prompt be? (try 8192)\n# - config: Your configuration object\n# - dm: Your DataManager\n# - variable_stats: Statistics from forecasting splitter (optional but recommended)\n\nconverter = None  # Create the converter\n</pre> # TODO: Initialize ConverterInstruction # Parameters needed: # - nr_tokens_budget_total: How many tokens can the prompt be? (try 8192) # - config: Your configuration object # - dm: Your DataManager # - variable_stats: Statistics from forecasting splitter (optional but recommended)  converter = None  # Create the converter In\u00a0[\u00a0]: Copied! <pre># Validate all components are created\ncomponents_valid = True\n\nif data_splitter_events is None:\n    print(\"\u274c data_splitter_events is not initialized\")\n    components_valid = False\nelse:\n    print(\"\u2705 data_splitter_events initialized\")\n\nif data_splitter_forecasting is None:\n    print(\"\u274c data_splitter_forecasting is not initialized\")\n    components_valid = False\nelse:\n    print(\"\u2705 data_splitter_forecasting initialized\")\n\nif data_splitter is None:\n    print(\"\u274c data_splitter is not initialized\")\n    components_valid = False\nelse:\n    print(\"\u2705 data_splitter initialized\")\n\nif converter is None:\n    print(\"\u274c converter is not initialized\")\n    components_valid = False\nelse:\n    print(\"\u2705 converter initialized\")\n\nif components_valid:\n    print(\"\\n\ud83c\udf89 Part 4 Complete! All components ready.\")\n</pre> # Validate all components are created components_valid = True  if data_splitter_events is None:     print(\"\u274c data_splitter_events is not initialized\")     components_valid = False else:     print(\"\u2705 data_splitter_events initialized\")  if data_splitter_forecasting is None:     print(\"\u274c data_splitter_forecasting is not initialized\")     components_valid = False else:     print(\"\u2705 data_splitter_forecasting initialized\")  if data_splitter is None:     print(\"\u274c data_splitter is not initialized\")     components_valid = False else:     print(\"\u2705 data_splitter initialized\")  if converter is None:     print(\"\u274c converter is not initialized\")     components_valid = False else:     print(\"\u2705 converter initialized\")  if components_valid:     print(\"\\n\ud83c\udf89 Part 4 Complete! All components ready.\") In\u00a0[\u00a0]: Copied! <pre># Select a patient to work with\npatientid = dm.all_patientids[4]\nprint(f\"Working with patient: {patientid}\")\n\n# Get patient data\npatient_data = dm.get_patient_data(patientid)\n</pre> # Select a patient to work with patientid = dm.all_patientids[4] print(f\"Working with patient: {patientid}\")  # Get patient data patient_data = dm.get_patient_data(patientid) In\u00a0[\u00a0]: Copied! <pre># TODO: Generate splits from this patient's data\n# Use data_splitter.get_splits_from_patient_with_target()\n# This returns: forecasting_splits, events_splits, reference_dates\n\nforecasting_splits, events_splits, reference_dates = None, None, None  # Replace with actual call\n</pre> # TODO: Generate splits from this patient's data # Use data_splitter.get_splits_from_patient_with_target() # This returns: forecasting_splits, events_splits, reference_dates  forecasting_splits, events_splits, reference_dates = None, None, None  # Replace with actual call In\u00a0[\u00a0]: Copied! <pre># TODO: Answer these questions by exploring the splits:\n# 1. How many splits were generated for this patient?\n# 2. What dates are the reference points (split dates)?\n# 3. What does each split contain?\n\nprint(\"Number of splits: ???\")  # Fill in\nprint(\"Reference dates: ???\")  # Fill in\n</pre> # TODO: Answer these questions by exploring the splits: # 1. How many splits were generated for this patient? # 2. What dates are the reference points (split dates)? # 3. What does each split contain?  print(\"Number of splits: ???\")  # Fill in print(\"Reference dates: ???\")  # Fill in In\u00a0[\u00a0]: Copied! <pre># TODO: Convert the first split to instruction format\n# Use converter.forward_conversion()\n# Parameters:\n# - forecasting_splits: the forecasting split for one time point\n# - event_splits: the event split for one time point\n# - override_mode_to_select_forecasting: set to \"both\"\n\nsplit_idx = 0\np_converted = None  # Replace with actual conversion call\n</pre> # TODO: Convert the first split to instruction format # Use converter.forward_conversion() # Parameters: # - forecasting_splits: the forecasting split for one time point # - event_splits: the event split for one time point # - override_mode_to_select_forecasting: set to \"both\"  split_idx = 0 p_converted = None  # Replace with actual conversion call In\u00a0[\u00a0]: Copied! <pre># TODO: Print and examine the instruction (input prompt)\n# What information is included? What's the structure?\n</pre> # TODO: Print and examine the instruction (input prompt) # What information is included? What's the structure?  In\u00a0[\u00a0]: Copied! <pre># TODO: Print and examine the answer (target output)\n# What format is the answer in? What predictions are being made?\n</pre> # TODO: Print and examine the answer (target output) # What format is the answer in? What predictions are being made?  <p>Your Answers:</p> <p>Q3.1:</p> <p>Q3.2:</p> <p>Q3.3:</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Use reverse_conversion to parse the answer back to structured data\n# You'll need:\n# - The answer string from p_converted\n# - The data manager (dm)\n# - The reference date for this split\n\ndate = reference_dates[\"date\"][split_idx]\nreturn_list = None  # Call converter.reverse_conversion()\n</pre> # TODO: Use reverse_conversion to parse the answer back to structured data # You'll need: # - The answer string from p_converted # - The data manager (dm) # - The reference date for this split  date = reference_dates[\"date\"][split_idx] return_list = None  # Call converter.reverse_conversion() In\u00a0[\u00a0]: Copied! <pre># TODO: Examine what the reverse conversion produced\n# What structure does return_list have? What's in each element?\n</pre> # TODO: Examine what the reverse conversion produced # What structure does return_list have? What's in each element?  In\u00a0[\u00a0]: Copied! <pre># BONUS: Implement your custom configuration here\n</pre> # BONUS: Implement your custom configuration here  In\u00a0[\u00a0]: Copied! <pre># BONUS: Implement the multi-patient dataset generator\n\n\ndef generate_training_dataset(dm, data_splitter, converter):\n    \"\"\"\n    Generate training examples for all patients.\n\n    Returns:\n        pd.DataFrame with columns: patientid, split_idx, instruction, answer\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n\n# Test your function\n# df_training = generate_training_dataset(dm, data_splitter, converter)\n# print(f\"Generated {len(df_training)} training examples\")\n</pre> # BONUS: Implement the multi-patient dataset generator   def generate_training_dataset(dm, data_splitter, converter):     \"\"\"     Generate training examples for all patients.      Returns:         pd.DataFrame with columns: patientid, split_idx, instruction, answer     \"\"\"     # TODO: Implement this function     pass   # Test your function # df_training = generate_training_dataset(dm, data_splitter, converter) # print(f\"Generated {len(df_training)} training examples\")"},{"location":"examples/hackathon/01_data_preparation_challenge/#challenge-1-data-preparation-for-training","title":"\ud83c\udfc6 Challenge 1: Data Preparation for Training\u00b6","text":"<p>Difficulty: \u2b50\u2b50 (Intermediate) | Time: 45-60 minutes</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#learning-objectives","title":"\ud83c\udfaf Learning Objectives\u00b6","text":"<p>By completing this challenge, you will:</p> <ol> <li>Understand the data format required by TwinWeaver</li> <li>Configure the pipeline for different prediction tasks</li> <li>Generate training splits from patient timelines</li> <li>Convert structured data to instruction-tuning format</li> </ol>"},{"location":"examples/hackathon/01_data_preparation_challenge/#rules","title":"\ud83d\udccb Rules\u00b6","text":"<ul> <li>Complete all <code># TODO:</code> sections</li> <li>Answer quiz questions before proceeding</li> <li>Run checkpoint cells to validate your solutions</li> <li>No peeking at the original tutorial!</li> </ul>"},{"location":"examples/hackathon/01_data_preparation_challenge/#part-1-understanding-the-data","title":"Part 1: Understanding the Data\u00b6","text":"<p>Before we start coding, let's understand what data we're working with.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#exercise-11-explore-the-data","title":"\ud83d\udd0d Exercise 1.1: Explore the Data\u00b6","text":"<p>Before configuring the pipeline, you need to understand your data. Explore the three dataframes to answer the quiz questions below.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#quiz-1-data-understanding","title":"\u2753 Quiz 1: Data Understanding\u00b6","text":"<p>Answer these questions based on your exploration:</p> <p>Q1.1: What column in <code>df_events</code> contains the type of medical event (lab, drug, condition, etc.)?</p> <p>Q1.2: List all unique event categories in the dataset:</p> <p>Q1.3: How many unique patients are in the dataset?</p> <p>Q1.4: What column in <code>df_constant</code> could be used to calculate a patient's age?</p> <p>Write your answers in the cell below:</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#part-2-configuration-challenge","title":"Part 2: Configuration Challenge\u00b6","text":"<p>Now you need to configure the TwinWeaver pipeline. This is where understanding your data pays off!</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#your-task","title":"\ud83c\udfaf Your Task\u00b6","text":"<p>Configure the pipeline to:</p> <ol> <li>Split patient histories around Lines of Therapy (treatment changes)</li> <li>Forecast lab values into the future</li> <li>Predict time-to-event for death and progression</li> </ol>"},{"location":"examples/hackathon/01_data_preparation_challenge/#checkpoint-21-validate-configuration","title":"\ud83c\udfc1 Checkpoint 2.1: Validate Configuration\u00b6","text":""},{"location":"examples/hackathon/01_data_preparation_challenge/#exercise-22-configure-static-variables","title":"\ud83d\udd27 Exercise 2.2: Configure Static Variables\u00b6","text":"<p>Now configure which patient demographics to include in the prompts.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#part-3-initialize-the-pipeline","title":"Part 3: Initialize the Pipeline\u00b6","text":"<p>With configuration complete, let's initialize the data processing components.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#checkpoint-31-validate-datamanager","title":"\ud83c\udfc1 Checkpoint 3.1: Validate DataManager\u00b6","text":""},{"location":"examples/hackathon/01_data_preparation_challenge/#part-4-create-splitters-and-converter","title":"Part 4: Create Splitters and Converter\u00b6","text":""},{"location":"examples/hackathon/01_data_preparation_challenge/#quiz-2-understanding-splitters","title":"\u2753 Quiz 2: Understanding Splitters\u00b6","text":"<p>Before creating the splitters, answer these conceptual questions:</p> <p>Q2.1: What is the purpose of splitting a patient's timeline? Why not use the entire history?</p> <p>Q2.2: What's the difference between <code>DataSplitterEvents</code> and <code>DataSplitterForecasting</code>?</p> <p>Q2.3: Why do we need a token budget for the converter?</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#checkpoint-41-validate-pipeline-components","title":"\ud83c\udfc1 Checkpoint 4.1: Validate Pipeline Components\u00b6","text":""},{"location":"examples/hackathon/01_data_preparation_challenge/#part-5-generate-training-examples","title":"Part 5: Generate Training Examples\u00b6","text":"<p>Now let's generate actual training examples!</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#exercise-51-analyze-the-splits","title":"\ud83d\udd0d Exercise 5.1: Analyze the Splits\u00b6","text":"<p>Before converting, understand what the splitter produced.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#exercise-52-examine-the-output","title":"\ud83d\udd0d Exercise 5.2: Examine the Output\u00b6","text":""},{"location":"examples/hackathon/01_data_preparation_challenge/#quiz-3-output-analysis","title":"\u2753 Quiz 3: Output Analysis\u00b6","text":"<p>Q3.1: What sections can you identify in the instruction prompt?</p> <p>Q3.2: How are the forecasting predictions formatted in the answer?</p> <p>Q3.3: How are the time-to-event predictions formatted?</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#part-6-reverse-conversion","title":"Part 6: Reverse Conversion\u00b6","text":"<p>An important capability is converting model outputs back to structured data.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#bonus-challenge-1-custom-configuration","title":"\ud83c\udf1f Bonus Challenge 1: Custom Configuration\u00b6","text":"<p>+15 points</p> <p>Modify the configuration to predict only drug-related events instead of death and progression. Generate a new training example and compare the output.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#bonus-challenge-2-multi-patient-dataset","title":"\ud83c\udf1f Bonus Challenge 2: Multi-Patient Dataset\u00b6","text":"<p>+25 points</p> <p>Write a function that generates training examples for ALL patients in the dataset and returns a pandas DataFrame with columns: <code>patientid</code>, <code>split_idx</code>, <code>instruction</code>, <code>answer</code>.</p>"},{"location":"examples/hackathon/01_data_preparation_challenge/#challenge-complete","title":"\ud83c\udfc6 Challenge Complete!\u00b6","text":"<p>Congratulations on completing Challenge 1! You've learned how to:</p> <ul> <li>\u2705 Explore and understand clinical data formats</li> <li>\u2705 Configure the TwinWeaver pipeline for different tasks</li> <li>\u2705 Generate training splits from patient timelines</li> <li>\u2705 Convert data to instruction-tuning format</li> <li>\u2705 Reverse convert predictions back to structured data</li> </ul> <p>Ready for the next challenge? Move on to Challenge 2: End-to-End LLM Fine-tuning!</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/","title":"\ud83c\udfc6 Challenge 2: End-to-End LLM Fine-tuning","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport gc\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig\nfrom trl import SFTConfig\n\nfrom twinweaver import (\n    DataManager,\n    Config,\n)\n</pre> import pandas as pd import gc import torch from datasets import Dataset from transformers import (     BitsAndBytesConfig, ) from peft import LoraConfig from trl import SFTConfig  from twinweaver import (     DataManager,     Config, ) <p>Your Answers:</p> <p>Q1.1:</p> <p>Q1.2:</p> <p>Q1.3:</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Choose your base model\n# Options to consider:\n# - \"microsoft/Phi-4-mini-instruct\" (small, fast)\n# - \"meta-llama/Llama-3.2-3B-Instruct\" (medium)\n# - \"mistralai/Mistral-7B-Instruct-v0.3\" (larger)\n\nBASE_MODEL = None  # Choose your model\n\n# TODO: Choose your context length\n# Consider: Longer = more patient history, but more memory\n# Reasonable range: 2048 - 16384\n\nMAX_CONTEXT_LENGTH = None  # Choose your context length\n</pre> # TODO: Choose your base model # Options to consider: # - \"microsoft/Phi-4-mini-instruct\" (small, fast) # - \"meta-llama/Llama-3.2-3B-Instruct\" (medium) # - \"mistralai/Mistral-7B-Instruct-v0.3\" (larger)  BASE_MODEL = None  # Choose your model  # TODO: Choose your context length # Consider: Longer = more patient history, but more memory # Reasonable range: 2048 - 16384  MAX_CONTEXT_LENGTH = None  # Choose your context length In\u00a0[\u00a0]: Copied! <pre># Load data\ndf_events = pd.read_csv(\"../example_data/events.csv\")\ndf_constant = pd.read_csv(\"../example_data/constant.csv\")\ndf_constant_description = pd.read_csv(\"../example_data/constant_description.csv\")\n</pre> # Load data df_events = pd.read_csv(\"../example_data/events.csv\") df_constant = pd.read_csv(\"../example_data/constant.csv\") df_constant_description = pd.read_csv(\"../example_data/constant_description.csv\") In\u00a0[\u00a0]: Copied! <pre># TODO: Configure the pipeline (you did this in Challenge 1!)\n# Set up:\n# - split_event_category\n# - event_category_forecast\n# - data_splitter_events_variables_category_mapping\n# - constant_columns_to_use\n# - constant_birthdate_column\n\nconfig = Config()\n\n# Your configuration here...\n</pre> # TODO: Configure the pipeline (you did this in Challenge 1!) # Set up: # - split_event_category # - event_category_forecast # - data_splitter_events_variables_category_mapping # - constant_columns_to_use # - constant_birthdate_column  config = Config()  # Your configuration here... In\u00a0[\u00a0]: Copied! <pre># TODO: Initialize DataManager and all splitters\n# This should be familiar from Challenge 1\n\ndm = DataManager(config=config)\n# ... complete the setup\n\n# Initialize splitters\n# ...\n\n# Initialize converter with YOUR chosen context length\n# ...\n</pre> # TODO: Initialize DataManager and all splitters # This should be familiar from Challenge 1  dm = DataManager(config=config) # ... complete the setup  # Initialize splitters # ...  # Initialize converter with YOUR chosen context length # ... In\u00a0[\u00a0]: Copied! <pre># Get patient IDs for each split\ntraining_patientids = dm.get_all_patientids_in_split(config.train_split_name)\nvalidation_patientids = dm.get_all_patientids_in_split(config.validation_split_name)\n\nprint(f\"Training patients: {len(training_patientids)}\")\nprint(f\"Validation patients: {len(validation_patientids)}\")\n</pre> # Get patient IDs for each split training_patientids = dm.get_all_patientids_in_split(config.train_split_name) validation_patientids = dm.get_all_patientids_in_split(config.validation_split_name)  print(f\"Training patients: {len(training_patientids)}\") print(f\"Validation patients: {len(validation_patientids)}\") In\u00a0[\u00a0]: Copied! <pre># TODO: Implement the dataset generation function\n# This function should:\n# 1. Iterate through each patient\n# 2. Get splits for each patient\n# 3. Convert each split to instruction format\n# 4. Return a DataFrame with 'prompt' and 'completion' columns\n\n\ndef generate_transformers_df(patientids_list):\n    \"\"\"\n    Generate training data from a list of patient IDs.\n\n    Args:\n        patientids_list: List of patient IDs to process\n\n    Returns:\n        pd.DataFrame with columns: prompt, completion, patientid\n    \"\"\"\n    df = []\n\n    # TODO: Implement the function\n    # HINT: Use dm.get_patient_data(), data_splitter.get_splits_from_patient_with_target(),\n    #       and converter.forward_conversion()\n\n    pass\n\n    return pd.DataFrame(df)\n</pre> # TODO: Implement the dataset generation function # This function should: # 1. Iterate through each patient # 2. Get splits for each patient # 3. Convert each split to instruction format # 4. Return a DataFrame with 'prompt' and 'completion' columns   def generate_transformers_df(patientids_list):     \"\"\"     Generate training data from a list of patient IDs.      Args:         patientids_list: List of patient IDs to process      Returns:         pd.DataFrame with columns: prompt, completion, patientid     \"\"\"     df = []      # TODO: Implement the function     # HINT: Use dm.get_patient_data(), data_splitter.get_splits_from_patient_with_target(),     #       and converter.forward_conversion()      pass      return pd.DataFrame(df) In\u00a0[\u00a0]: Copied! <pre># Generate datasets\ndf_train = generate_transformers_df(training_patientids)\ndf_validation = generate_transformers_df(validation_patientids)\n\nprint(f\"Training examples: {len(df_train)}\")\nprint(f\"Validation examples: {len(df_validation)}\")\n</pre> # Generate datasets df_train = generate_transformers_df(training_patientids) df_validation = generate_transformers_df(validation_patientids)  print(f\"Training examples: {len(df_train)}\") print(f\"Validation examples: {len(df_validation)}\") In\u00a0[\u00a0]: Copied! <pre># Validate the generated dataset\ndef validate_dataset(df, name):\n    errors = []\n\n    if df is None or len(df) == 0:\n        errors.append(f\"\u274c {name} is empty\")\n    elif \"prompt\" not in df.columns:\n        errors.append(f\"\u274c {name} missing 'prompt' column\")\n    elif \"completion\" not in df.columns:\n        errors.append(f\"\u274c {name} missing 'completion' column\")\n    else:\n        print(f\"\u2705 {name}: {len(df)} examples\")\n        print(f\"   Avg prompt length: {df['prompt'].str.len().mean():.0f} chars\")\n        print(f\"   Avg completion length: {df['completion'].str.len().mean():.0f} chars\")\n        return True\n\n    for e in errors:\n        print(e)\n    return False\n\n\ntrain_valid = validate_dataset(df_train, \"Training set\")\nval_valid = validate_dataset(df_validation, \"Validation set\")\n\nif train_valid and val_valid:\n    print(\"\\n\ud83c\udf89 Datasets ready for training!\")\n</pre> # Validate the generated dataset def validate_dataset(df, name):     errors = []      if df is None or len(df) == 0:         errors.append(f\"\u274c {name} is empty\")     elif \"prompt\" not in df.columns:         errors.append(f\"\u274c {name} missing 'prompt' column\")     elif \"completion\" not in df.columns:         errors.append(f\"\u274c {name} missing 'completion' column\")     else:         print(f\"\u2705 {name}: {len(df)} examples\")         print(f\"   Avg prompt length: {df['prompt'].str.len().mean():.0f} chars\")         print(f\"   Avg completion length: {df['completion'].str.len().mean():.0f} chars\")         return True      for e in errors:         print(e)     return False   train_valid = validate_dataset(df_train, \"Training set\") val_valid = validate_dataset(df_validation, \"Validation set\")  if train_valid and val_valid:     print(\"\\n\ud83c\udf89 Datasets ready for training!\") In\u00a0[\u00a0]: Copied! <pre># TODO: Load the tokenizer for your chosen model\ntokenizer = None  # Load tokenizer\n\n# TODO: Set the padding token\n# HINT: A common approach is to use the EOS token as the padding token\n</pre> # TODO: Load the tokenizer for your chosen model tokenizer = None  # Load tokenizer  # TODO: Set the padding token # HINT: A common approach is to use the EOS token as the padding token <p>Your Answers:</p> <p>Q2.1:</p> <p>Q2.2:</p> <p>Q2.3:</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Implement the chat formatting function\n# Convert raw prompt/completion to chat message format\n\n\ndef format_chat_template(example):\n    \"\"\"\n    Convert a single example to chat format.\n\n    Args:\n        example: Dict with 'prompt' and 'completion' keys\n\n    Returns:\n        Dict with 'prompt' as list of user messages and 'completion' as list of assistant messages\n    \"\"\"\n    # TODO: Implement this\n    # HINT: Return format should be:\n    # {\"prompt\": [{\"role\": \"user\", \"content\": ...}],\n    #  \"completion\": [{\"role\": \"assistant\", \"content\": ...}]}\n    pass\n</pre> # TODO: Implement the chat formatting function # Convert raw prompt/completion to chat message format   def format_chat_template(example):     \"\"\"     Convert a single example to chat format.      Args:         example: Dict with 'prompt' and 'completion' keys      Returns:         Dict with 'prompt' as list of user messages and 'completion' as list of assistant messages     \"\"\"     # TODO: Implement this     # HINT: Return format should be:     # {\"prompt\": [{\"role\": \"user\", \"content\": ...}],     #  \"completion\": [{\"role\": \"assistant\", \"content\": ...}]}     pass In\u00a0[\u00a0]: Copied! <pre># Convert to HuggingFace datasets and apply formatting\ntrain_dataset = Dataset.from_pandas(df_train)\nvalidation_dataset = Dataset.from_pandas(df_validation)\n\ntrain_dataset = train_dataset.map(format_chat_template)\nvalidation_dataset = validation_dataset.map(format_chat_template)\n</pre> # Convert to HuggingFace datasets and apply formatting train_dataset = Dataset.from_pandas(df_train) validation_dataset = Dataset.from_pandas(df_validation)  train_dataset = train_dataset.map(format_chat_template) validation_dataset = validation_dataset.map(format_chat_template) <p>Your Answers:</p> <p>Q3.1:</p> <p>Q3.2:</p> <p>Q3.3:</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Configure 4-bit quantization\n# Parameters to set:\n# - load_in_4bit: Enable 4-bit loading\n# - bnb_4bit_quant_type: Use \"nf4\" for better quality\n# - bnb_4bit_compute_dtype: Use torch.bfloat16 for modern GPUs\n# - bnb_4bit_use_double_quant: Enable for additional memory savings\n\nbnb_config = BitsAndBytesConfig(\n    # TODO: Fill in the parameters\n)\n</pre> # TODO: Configure 4-bit quantization # Parameters to set: # - load_in_4bit: Enable 4-bit loading # - bnb_4bit_quant_type: Use \"nf4\" for better quality # - bnb_4bit_compute_dtype: Use torch.bfloat16 for modern GPUs # - bnb_4bit_use_double_quant: Enable for additional memory savings  bnb_config = BitsAndBytesConfig(     # TODO: Fill in the parameters ) <p>Your Predictions:</p> <ul> <li><code>r</code> (rank):</li> <li><code>lora_alpha</code>:</li> <li><code>lora_dropout</code>:</li> <li>Number of target modules:</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Configure LoRA\n# Make deliberate choices for each parameter and justify them!\n\npeft_config = LoraConfig(\n    # TODO: Set lora_alpha (scaling factor, common values: 8, 16, 32)\n    lora_alpha=None,\n    # TODO: Set lora_dropout (regularization, common values: 0.05-0.2)\n    lora_dropout=None,\n    # TODO: Set r (rank - higher = more parameters, common values: 4, 8, 16, 32)\n    r=None,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    # TODO: Choose which modules to target\n    # Options:\n    # - Minimal: [\"q_proj\", \"v_proj\"] - faster but less expressive\n    # - Full attention: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n    # - All linear: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    target_modules=None,\n)\n\n# Document your reasoning:\nprint(\"My LoRA configuration choices:\")\nprint(f\"  r={peft_config.r}: [Your reasoning here]\")\nprint(f\"  lora_alpha={peft_config.lora_alpha}: [Your reasoning here]\")\nprint(f\"  target_modules={peft_config.target_modules}: [Your reasoning here]\")\n</pre> # TODO: Configure LoRA # Make deliberate choices for each parameter and justify them!  peft_config = LoraConfig(     # TODO: Set lora_alpha (scaling factor, common values: 8, 16, 32)     lora_alpha=None,     # TODO: Set lora_dropout (regularization, common values: 0.05-0.2)     lora_dropout=None,     # TODO: Set r (rank - higher = more parameters, common values: 4, 8, 16, 32)     r=None,     bias=\"none\",     task_type=\"CAUSAL_LM\",     # TODO: Choose which modules to target     # Options:     # - Minimal: [\"q_proj\", \"v_proj\"] - faster but less expressive     # - Full attention: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]     # - All linear: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]     target_modules=None, )  # Document your reasoning: print(\"My LoRA configuration choices:\") print(f\"  r={peft_config.r}: [Your reasoning here]\") print(f\"  lora_alpha={peft_config.lora_alpha}: [Your reasoning here]\") print(f\"  target_modules={peft_config.target_modules}: [Your reasoning here]\") <p>Your Answers:</p> <p>Q6.1:</p> <p>Q6.2 (too high):</p> <p>Q6.2 (too low):</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Configure training arguments\n# Think carefully about each parameter!\n\ntraining_arguments = SFTConfig(\n    output_dir=\"./results_challenge\",\n    # TODO: Set number of training epochs (consider: small dataset = more epochs OK)\n    num_train_epochs=None,\n    # TODO: Set batch size (limited by GPU memory)\n    per_device_train_batch_size=None,\n    # TODO: Set gradient accumulation (effective_batch = batch_size * grad_accum)\n    gradient_accumulation_steps=None,\n    # Optimizer settings\n    optim=\"paged_adamw_32bit\",\n    # Logging and evaluation\n    save_steps=10,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    per_device_eval_batch_size=1,\n    # TODO: Set learning rate (PEFT typically uses higher LR than full fine-tuning)\n    learning_rate=None,\n    # Precision settings\n    fp16=False,  # Set True for older GPUs (V100, T4)\n    bf16=True,  # Set True for newer GPUs (A100, 3090, 4090)\n    # Regularization\n    max_grad_norm=1.0,\n    # TODO: Set warmup ratio (what fraction of training for warmup?)\n    warmup_ratio=None,\n    group_by_length=True,\n    save_total_limit=1,\n    lr_scheduler_type=\"cosine\",\n    max_length=MAX_CONTEXT_LENGTH,\n    packing=False,\n    completion_only_loss=True,\n)\n</pre> # TODO: Configure training arguments # Think carefully about each parameter!  training_arguments = SFTConfig(     output_dir=\"./results_challenge\",     # TODO: Set number of training epochs (consider: small dataset = more epochs OK)     num_train_epochs=None,     # TODO: Set batch size (limited by GPU memory)     per_device_train_batch_size=None,     # TODO: Set gradient accumulation (effective_batch = batch_size * grad_accum)     gradient_accumulation_steps=None,     # Optimizer settings     optim=\"paged_adamw_32bit\",     # Logging and evaluation     save_steps=10,     logging_steps=10,     eval_strategy=\"steps\",     eval_steps=10,     per_device_eval_batch_size=1,     # TODO: Set learning rate (PEFT typically uses higher LR than full fine-tuning)     learning_rate=None,     # Precision settings     fp16=False,  # Set True for older GPUs (V100, T4)     bf16=True,  # Set True for newer GPUs (A100, 3090, 4090)     # Regularization     max_grad_norm=1.0,     # TODO: Set warmup ratio (what fraction of training for warmup?)     warmup_ratio=None,     group_by_length=True,     save_total_limit=1,     lr_scheduler_type=\"cosine\",     max_length=MAX_CONTEXT_LENGTH,     packing=False,     completion_only_loss=True, ) In\u00a0[\u00a0]: Copied! <pre># TODO: Load the base model with quantization\n# Parameters needed:\n# - Model name (BASE_MODEL)\n# - quantization_config (bnb_config)\n# - device_map=\"auto\"\n# - trust_remote_code=False\n\nmodel = None  # Load the model\n\n# Don't forget: Disable cache for training\n# model.config.use_cache = False\n</pre> # TODO: Load the base model with quantization # Parameters needed: # - Model name (BASE_MODEL) # - quantization_config (bnb_config) # - device_map=\"auto\" # - trust_remote_code=False  model = None  # Load the model  # Don't forget: Disable cache for training # model.config.use_cache = False In\u00a0[\u00a0]: Copied! <pre># TODO: Create the SFTTrainer\n# Parameters needed:\n# - model\n# - train_dataset\n# - processing_class (tokenizer)\n# - args (training_arguments)\n# - eval_dataset (validation_dataset)\n# - peft_config\n\ntrainer = None  # Create trainer\n</pre> # TODO: Create the SFTTrainer # Parameters needed: # - model # - train_dataset # - processing_class (tokenizer) # - args (training_arguments) # - eval_dataset (validation_dataset) # - peft_config  trainer = None  # Create trainer <p>Your Predictions:</p> <p>Q7.1:</p> <p>Q7.2:</p> <p>Q7.3:</p> In\u00a0[\u00a0]: Copied! <pre># Run training!\n# Note: This takes ~5 minutes on a good GPU\n# Watch the training loss and validation loss as it runs\n\ntrainer.train()\n</pre> # Run training! # Note: This takes ~5 minutes on a good GPU # Watch the training loss and validation loss as it runs  trainer.train() In\u00a0[\u00a0]: Copied! <pre># TODO: Analyze the training results\n# Questions to answer:\n# 1. What was the final training loss?\n# 2. What was the final validation loss?\n# 3. Did validation loss ever increase? (sign of overfitting)\n# 4. Did your predictions match reality?\n\nprint(\"Training Analysis:\")\nprint(\"==================\")\n# Your analysis here...\n</pre> # TODO: Analyze the training results # Questions to answer: # 1. What was the final training loss? # 2. What was the final validation loss? # 3. Did validation loss ever increase? (sign of overfitting) # 4. Did your predictions match reality?  print(\"Training Analysis:\") print(\"==================\") # Your analysis here... In\u00a0[\u00a0]: Copied! <pre># Save the adapter\nadapter_path = \"./results_challenge/final_adapter\"\ntrainer.save_model(adapter_path)\nprint(f\"Adapter saved to {adapter_path}\")\n\n# Clean up\ndel trainer\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\n</pre> # Save the adapter adapter_path = \"./results_challenge/final_adapter\" trainer.save_model(adapter_path) print(f\"Adapter saved to {adapter_path}\")  # Clean up del trainer del model gc.collect() torch.cuda.empty_cache() In\u00a0[\u00a0]: Copied! <pre># Get a test patient\ntest_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0]\npatient_data = dm.get_patient_data(test_patientid)\n\n# Get the date of first line of therapy\ndf_events_patient = patient_data[\"events\"].copy()\ndate_of_first_lot = df_events_patient.loc[\n    df_events_patient[\"event_category\"] == config.event_category_lot, \"date\"\n].min()\n\nprint(f\"Test patient: {test_patientid}\")\nprint(f\"First LoT date: {date_of_first_lot}\")\n</pre> # Get a test patient test_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0] patient_data = dm.get_patient_data(test_patientid)  # Get the date of first line of therapy df_events_patient = patient_data[\"events\"].copy() date_of_first_lot = df_events_patient.loc[     df_events_patient[\"event_category\"] == config.event_category_lot, \"date\" ].min()  print(f\"Test patient: {test_patientid}\") print(f\"First LoT date: {date_of_first_lot}\") In\u00a0[\u00a0]: Copied! <pre># TODO: Design your forecasting task\n# What variable do you want to forecast? At what time points?\n\n# Example structure:\n# forecasting_times_to_predict = {\n#     \"variable_name\": [week1, week2, week3, ...]\n# }\n\nforecasting_times_to_predict = {\n    # TODO: Fill in - choose a lab value and time points\n}\n\n# Get inference splits\nforecast_split, events_split = data_splitter.get_splits_from_patient_inference(\n    patient_data,\n    inference_type=\"both\",\n    # TODO: Set the variable(s) to predict\n    forecasting_override_variables_to_predict=None,  # List of variable names\n    # TODO: Set the event to predict\n    events_override_category=None,  # e.g., \"death\"\n    events_override_observation_time_delta=pd.Timedelta(days=52 * 7),  # 1 year\n)\n</pre> # TODO: Design your forecasting task # What variable do you want to forecast? At what time points?  # Example structure: # forecasting_times_to_predict = { #     \"variable_name\": [week1, week2, week3, ...] # }  forecasting_times_to_predict = {     # TODO: Fill in - choose a lab value and time points }  # Get inference splits forecast_split, events_split = data_splitter.get_splits_from_patient_inference(     patient_data,     inference_type=\"both\",     # TODO: Set the variable(s) to predict     forecasting_override_variables_to_predict=None,  # List of variable names     # TODO: Set the event to predict     events_override_category=None,  # e.g., \"death\"     events_override_observation_time_delta=pd.Timedelta(days=52 * 7),  # 1 year ) In\u00a0[\u00a0]: Copied! <pre># TODO: Convert to instruction format for inference\n# Use converter.forward_conversion_inference()\n\nconverted = None  # Your code here\n</pre> # TODO: Convert to instruction format for inference # Use converter.forward_conversion_inference()  converted = None  # Your code here In\u00a0[\u00a0]: Copied! <pre># Print the prompt to see what we're asking the model\nprint(\"=\" * 50)\nprint(\"INFERENCE PROMPT:\")\nprint(\"=\" * 50)\nprint(converted[\"instruction\"][:2000])  # First 2000 chars\nprint(\"\\n... [truncated]\")\n</pre> # Print the prompt to see what we're asking the model print(\"=\" * 50) print(\"INFERENCE PROMPT:\") print(\"=\" * 50) print(converted[\"instruction\"][:2000])  # First 2000 chars print(\"\\n... [truncated]\") <p>Your Predictions:</p> <p>Q8.1:</p> <p>Q8.2:</p> <p>Q8.3:</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Load the base model and adapter for inference\n\n# 1. Load base model with quantization\nbase_model_inference = None  # Your code\n\n# 2. Load the saved adapter\ninference_model = None  # Use PeftModel.from_pretrained()\n\n# 3. Set to evaluation mode\n# inference_model.eval()\n</pre> # TODO: Load the base model and adapter for inference  # 1. Load base model with quantization base_model_inference = None  # Your code  # 2. Load the saved adapter inference_model = None  # Use PeftModel.from_pretrained()  # 3. Set to evaluation mode # inference_model.eval() In\u00a0[\u00a0]: Copied! <pre># TODO: Create text generation pipeline\ninference_model.config.use_cache = True\n\ntext_gen_pipeline = None  # Create pipeline(\"text-generation\", ...)\n</pre> # TODO: Create text generation pipeline inference_model.config.use_cache = True  text_gen_pipeline = None  # Create pipeline(\"text-generation\", ...) In\u00a0[\u00a0]: Copied! <pre># TODO: Generate prediction\n# Use the pipeline with appropriate generation parameters:\n# - max_new_tokens: 128 is usually enough\n# - return_full_text: False (we only want the generated part)\n# - do_sample: True (for nucleus sampling)\n# - temperature: 0.7 (controls randomness)\n# - top_p: 0.9 (nucleus sampling threshold)\n\ngenerated_answer = None  # Your generation code\n</pre> # TODO: Generate prediction # Use the pipeline with appropriate generation parameters: # - max_new_tokens: 128 is usually enough # - return_full_text: False (we only want the generated part) # - do_sample: True (for nucleus sampling) # - temperature: 0.7 (controls randomness) # - top_p: 0.9 (nucleus sampling threshold)  generated_answer = None  # Your generation code In\u00a0[\u00a0]: Copied! <pre># Show the generated answer\nprint(\"=\" * 50)\nprint(\"MODEL PREDICTION:\")\nprint(\"=\" * 50)\nprint(generated_answer)\n</pre> # Show the generated answer print(\"=\" * 50) print(\"MODEL PREDICTION:\") print(\"=\" * 50) print(generated_answer) In\u00a0[\u00a0]: Copied! <pre># TODO: Reverse convert to structured data\nreturn_list = None  # Use converter.reverse_conversion()\n</pre> # TODO: Reverse convert to structured data return_list = None  # Use converter.reverse_conversion() In\u00a0[\u00a0]: Copied! <pre># Display structured results\nfor i, result in enumerate(return_list):\n    print(f\"\\nTask {i + 1}:\")\n    print(result[\"result\"])\n</pre> # Display structured results for i, result in enumerate(return_list):     print(f\"\\nTask {i + 1}:\")     print(result[\"result\"]) In\u00a0[\u00a0]: Copied! <pre># TODO: Write your analysis\n# 1. How did the model's predictions compare to yours?\n# 2. Are the predictions clinically reasonable?\n# 3. What improvements would you suggest?\n\nprint(\"\"\"\nFinal Analysis:\n===============\n\n1. Comparison to my predictions:\n   [Your analysis here]\n\n2. Clinical reasonableness:\n   [Your analysis here]\n\n3. Suggested improvements:\n   [Your analysis here]\n\"\"\")\n</pre> # TODO: Write your analysis # 1. How did the model's predictions compare to yours? # 2. Are the predictions clinically reasonable? # 3. What improvements would you suggest?  print(\"\"\" Final Analysis: ===============  1. Comparison to my predictions:    [Your analysis here]  2. Clinical reasonableness:    [Your analysis here]  3. Suggested improvements:    [Your analysis here] \"\"\") In\u00a0[\u00a0]: Copied! <pre># BONUS: Implement your hyperparameter experiment\n</pre> # BONUS: Implement your hyperparameter experiment  In\u00a0[\u00a0]: Copied! <pre># BONUS: Implement multi-sample inference and analyze variance\n</pre> # BONUS: Implement multi-sample inference and analyze variance  In\u00a0[\u00a0]: Copied! <pre># BONUS: Implement the evaluation framework\n</pre> # BONUS: Implement the evaluation framework"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#challenge-2-end-to-end-llm-fine-tuning","title":"\ud83c\udfc6 Challenge 2: End-to-End LLM Fine-tuning\u00b6","text":"<p>Difficulty: \u2b50\u2b50\u2b50 (Advanced) | Time: 90-120 minutes</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#learning-objectives","title":"\ud83c\udfaf Learning Objectives\u00b6","text":"<p>By completing this challenge, you will:</p> <ol> <li>Generate training datasets from clinical data</li> <li>Configure and understand LoRA/QLoRA parameters</li> <li>Fine-tune an LLM for medical forecasting</li> <li>Run inference and evaluate model predictions</li> </ol>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#prerequisites","title":"\u26a0\ufe0f Prerequisites\u00b6","text":"<ul> <li>Complete Challenge 1 first!</li> <li>GPU with at least 30GB memory</li> <li>Install: <code>pip install twinweaver[fine-tuning-example]</code></li> </ul>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#rules","title":"\ud83d\udccb Rules\u00b6","text":"<ul> <li>Complete all <code># TODO:</code> sections</li> <li>Answer quiz questions before proceeding</li> <li>Make predictions about hyperparameter effects BEFORE running experiments</li> <li>No peeking at the original tutorial!</li> </ul>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-1-decide-on-model-and-context-length","title":"Part 1: Decide on Model and Context Length\u00b6","text":"<p>Before starting, you need to make important decisions about your setup.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#quiz-1-model-selection","title":"\u2753 Quiz 1: Model Selection\u00b6","text":"<p>Q1.1: Why might you choose a smaller model (e.g., Phi-4-mini) over a larger one (e.g., Llama-70B) for this task?</p> <p>Q1.2: What is the trade-off between context length and memory usage?</p> <p>Q1.3: Why do we use an \"instruction-tuned\" base model instead of a base model?</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-2-generate-training-data","title":"Part 2: Generate Training Data\u00b6","text":"<p>Using what you learned in Challenge 1, set up the data pipeline.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#exercise-21-implement-dataset-generator","title":"\ud83d\udd27 Exercise 2.1: Implement Dataset Generator\u00b6","text":"<p>Write a function to generate the training dataset. This is a key skill!</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#checkpoint-21-validate-dataset","title":"\ud83c\udfc1 Checkpoint 2.1: Validate Dataset\u00b6","text":""},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-3-tokenizer-and-data-formatting","title":"Part 3: Tokenizer and Data Formatting\u00b6","text":"<p>LLMs expect data in a specific chat format. Let's set this up.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#quiz-2-chat-templates","title":"\u2753 Quiz 2: Chat Templates\u00b6","text":"<p>Q2.1: What is a \"chat template\" and why do instruction-tuned models need it?</p> <p>Q2.2: What roles are typically used in a chat format?</p> <p>Q2.3: Why do we set <code>completion_only_loss=True</code> during training?</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-4-configure-quantization-qlora","title":"Part 4: Configure Quantization (QLoRA)\u00b6","text":"<p>Quantization allows training large models on limited hardware.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#quiz-3-quantization-understanding","title":"\u2753 Quiz 3: Quantization Understanding\u00b6","text":"<p>Q3.1: What does \"4-bit quantization\" mean? What are we quantizing?</p> <p>Q3.2: What is the trade-off between quantization level (4-bit vs 8-bit vs full precision)?</p> <p>Q3.3: What does \"nf4\" (NormalFloat4) quantization type do differently than regular int4?</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-5-configure-lora","title":"Part 5: Configure LoRA\u00b6","text":"<p>LoRA (Low-Rank Adaptation) enables efficient fine-tuning by training only a small number of parameters.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#experiment-51-lora-hyperparameter-impact","title":"\ud83e\uddea Experiment 5.1: LoRA Hyperparameter Impact\u00b6","text":"<p>Before configuring, make predictions about how these hyperparameters affect training:</p> Parameter Your Prediction: What happens if we INCREASE it? <code>r</code> (rank) <code>lora_alpha</code> <code>lora_dropout</code> Number of target modules"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-6-training-configuration","title":"Part 6: Training Configuration\u00b6","text":"<p>Configure the training hyperparameters. Each choice matters!</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#experiment-61-learning-rate-analysis","title":"\ud83e\uddea Experiment 6.1: Learning Rate Analysis\u00b6","text":"<p>Q6.1: The tutorial uses <code>learning_rate=1e-4</code>. This is higher than typical full fine-tuning (1e-5 to 5e-5) we have used. Why might PEFT methods benefit from higher learning rates?</p> <p>Q6.2: What problems might you see if the learning rate is:</p> <ul> <li>Too high?</li> <li>Too low?</li> </ul>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-7-load-model-and-train","title":"Part 7: Load Model and Train\u00b6","text":"<p>Time to put it all together!</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#experiment-71-training-observation","title":"\ud83e\uddea Experiment 7.1: Training Observation\u00b6","text":"<p>Before running training, predict what you expect to see:</p> <p>Q7.1: What should happen to the training loss over time?</p> <p>Q7.2: What might it mean if validation loss increases while training loss decreases?</p> <p>Q7.3: How long do you expect training to take? (Make a guess!)</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#post-training-analysis","title":"\ud83d\udcca Post-Training Analysis\u00b6","text":"<p>After training completes, analyze what happened.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#part-8-inference-challenge","title":"Part 8: Inference Challenge\u00b6","text":"<p>Now let's test the fine-tuned model!</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#exercise-81-design-your-prediction-task","title":"\ud83d\udd27 Exercise 8.1: Design Your Prediction Task\u00b6","text":"<p>Choose what you want to predict for this patient.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#experiment-82-prediction-before-running","title":"\ud83e\uddea Experiment 8.2: Prediction Before Running\u00b6","text":"<p>Based on the patient's history in the prompt, make your own predictions:</p> <p>Q8.1: What do you predict for the forecasted values?</p> <p>Q8.2: What do you predict for the time-to-event?</p> <p>Q8.3: How confident are you in these predictions? Why?</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#final-analysis","title":"\ud83d\udcca Final Analysis\u00b6","text":"<p>Compare the model's predictions to your own and reflect on the results.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#bonus-challenge-1-hyperparameter-experiment","title":"\ud83c\udf1f Bonus Challenge 1: Hyperparameter Experiment\u00b6","text":"<p>+20 points</p> <p>Train two more models with different LoRA configurations:</p> <ol> <li>Low rank (r=4) with minimal target modules</li> <li>High rank (r=32) with all linear modules</li> </ol> <p>Compare their performance on the same test patient.</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#bonus-challenge-2-multi-sample-inference","title":"\ud83c\udf1f Bonus Challenge 2: Multi-Sample Inference\u00b6","text":"<p>+15 points</p> <p>Generate multiple predictions (N=5) for the same patient using different random seeds. Analyze the variance in predictions. What does this tell you about model confidence?</p>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#bonus-challenge-3-evaluation-framework","title":"\ud83c\udf1f Bonus Challenge 3: Evaluation Framework\u00b6","text":"<p>+25 points</p> <p>Build an evaluation framework that:</p> <ol> <li>Runs inference on all test patients</li> <li>Compares predictions to ground truth</li> <li>Computes metrics (MAE for forecasting, accuracy for events)</li> <li>Generates a summary report</li> </ol>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#challenge-complete","title":"\ud83c\udfc6 Challenge Complete!\u00b6","text":"<p>Congratulations on completing the advanced challenge! You've learned to:</p> <ul> <li>\u2705 Generate training datasets from clinical data</li> <li>\u2705 Configure quantization for memory-efficient training</li> <li>\u2705 Design and justify LoRA hyperparameter choices</li> <li>\u2705 Fine-tune an LLM for medical forecasting</li> <li>\u2705 Run inference and analyze predictions</li> <li>\u2705 Convert model outputs back to structured data</li> </ul>"},{"location":"examples/hackathon/02_llm_finetuning_challenge/#reflection-questions","title":"\ud83d\udcdd Reflection Questions\u00b6","text":"<p>Take a moment to reflect on what you learned:</p> <ol> <li>What was the most challenging part of this challenge?</li> <li>What would you do differently if you had more compute resources?</li> <li>How would you adapt this pipeline for a different clinical task?</li> </ol>"},{"location":"examples/integrations/meds_data_import/","title":"Example on how to import MEDS data format","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nfrom twinweaver import (\n    convert_meds_to_dtc,\n    DataManager,\n    DataSplitterEvents,\n    ConverterInstruction,\n    Config,\n)\n</pre> import pandas as pd import numpy as np from datetime import datetime  from twinweaver import (     convert_meds_to_dtc,     DataManager,     DataSplitterEvents,     ConverterInstruction,     Config, ) In\u00a0[\u00a0]: Copied! <pre>code_metadata_list = [\n    # Static Measurements\n    {\"code\": \"GENDER/Female\", \"description\": \"Female sex\"},\n    {\"code\": \"GENDER/Male\", \"description\": \"Male sex\"},\n    {\"code\": \"GENETIC/BRCA1_pos\", \"description\": \"BRCA1 gene mutation\"},\n    # Visit and Administrative Codes\n    {\n        \"code\": \"ADMISSION/Outpatient\",\n        \"description\": \"Admission for an outpatient clinic visit\",\n    },\n    {\n        \"code\": \"ADMISSION/Inpatient\",\n        \"description\": \"Admission to the hospital for an inpatient stay\",\n    },\n    {\n        \"code\": \"DISCHARGE/Outpatient\",\n        \"description\": \"Discharge from an outpatient clinic visit\",\n    },\n    {\n        \"code\": \"DISCHARGE/Inpatient\",\n        \"description\": \"Discharge from an inpatient hospital stay\",\n    },\n    {\n        \"code\": \"NOTE/FollowUp\",\n        \"description\": \"Clinical note for a follow-up appointment\",\n    },\n    # Diagnosis Codes (ICD-10-CM)\n    {\n        \"code\": \"ICD10CM/C34.90\",\n        \"description\": \"Malignant neoplasm of unspecified part of unspecified bronchus or lung\",\n    },\n    {\"code\": \"ICD10CM/C61\", \"description\": \"Malignant neoplasm of prostate\"},\n    # Symptom Codes\n    {\"code\": \"SYMPTOM/Cough\", \"description\": \"Patient reports a persistent cough\"},\n    # Procedure Codes (CPT)\n    {\n        \"code\": \"CPT/71250\",\n        \"description\": \"Procedure code for a CT scan of the thorax without contrast\",\n    },\n    {\n        \"code\": \"CPT/32408\",\n        \"description\": \"Procedure code for a core needle biopsy of the lung or mediastinum\",\n    },\n    {\n        \"code\": \"CPT/55700\",\n        \"description\": \"Procedure code for a needle biopsy of the prostate\",\n    },\n    {\n        \"code\": \"CPT/55840\",\n        \"description\": \"Procedure code for a radical retropubic prostatectomy\",\n    },\n    # Lab Codes (LOINC)\n    {\n        \"code\": \"LOINC/6690-2\",\n        \"description\": \"Leukocytes [#/volume] in Blood by Automated count (White Blood Cell Count)\",\n    },\n    {\n        \"code\": \"LOINC/2039-6\",\n        \"description\": \"Carcinoembryonic Ag [Mass/volume] in Serum or Plasma (CEA Tumor Marker)\",\n    },\n    {\n        \"code\": \"LOINC/59261-8\",\n        \"description\": \"Comprehensive metabolic 2014 panel - Serum or Plasma\",\n    },\n    {\n        \"code\": \"LOINC/2857-1\",\n        \"description\": \"Prostate specific Ag [Mass/volume] in Serum or Plasma (PSA Test)\",\n    },\n    # Medication Codes\n    {\n        \"code\": \"RX/Cisplatin\",\n        \"description\": \"Administration of Cisplatin chemotherapy agent\",\n    },\n    # Death\n    {\"code\": \"DEATH\", \"description\": \"Death\"},\n]\n\ncode_metadata_df = pd.DataFrame(code_metadata_list)\n\n\n# Patient Events DataFrame\npatient_events_list = [\n    # Patient 101: Jane Doe (Lung Cancer) - Assigned to 'train' split\n    # Static data\n    {\n        \"subject_id\": 101,\n        \"time\": pd.NaT,\n        \"code\": \"GENDER/Female\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Female\",\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": pd.NaT,\n        \"code\": \"GENETIC/BRCA1_pos\",\n        \"numeric_value\": 1,\n        \"text_value\": \"Positive\",\n    },\n    # Visit 1 (Week 2, 2024): Diagnosis\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 8),\n        \"code\": \"ADMISSION/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 8),\n        \"code\": \"SYMPTOM/Cough\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Persistent for 2 months\",\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 8),\n        \"code\": \"LOINC/6690-2\",\n        \"numeric_value\": 12.5,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 8),\n        \"code\": \"CPT/71250\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Nodule found in right lung\",\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 8),\n        \"code\": \"CPT/32408\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 8),\n        \"code\": \"ICD10CM/C34.90\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Primary Diagnosis\",\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 8),\n        \"code\": \"DISCHARGE/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    # Visit 2 (Week 4, 2024): Treatment\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 22),\n        \"code\": \"ADMISSION/Inpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 22),\n        \"code\": \"LOINC/59261-8\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"All values within normal limits\",\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 22),\n        \"code\": \"RX/Cisplatin\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Cisplatin\",\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 1, 22),\n        \"code\": \"DISCHARGE/Inpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    # Visit 3 (Week 8, 2024): Follow-up\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 2, 19),\n        \"code\": \"ADMISSION/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 2, 19),\n        \"code\": \"NOTE/FollowUp\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Patient tolerated first cycle well.\",\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 2, 19),\n        \"code\": \"LOINC/2039-6\",\n        \"numeric_value\": 50.2,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 101,\n        \"time\": datetime(2024, 2, 19),\n        \"code\": \"DISCHARGE/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    # Patient 202: John Smith (Prostate Cancer) - Assigned to 'held_out' split\n    # Static data\n    {\n        \"subject_id\": 202,\n        \"time\": pd.NaT,\n        \"code\": \"GENDER/Male\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Male\",\n    },\n    # Visit 1 (Week 10, 2024): Diagnosis\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 3, 4),\n        \"code\": \"ADMISSION/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 3, 4),\n        \"code\": \"LOINC/2857-1\",\n        \"numeric_value\": 15.1,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 3, 4),\n        \"code\": \"CPT/55700\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Biopsy taken\",\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 3, 4),\n        \"code\": \"ICD10CM/C61\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Primary Diagnosis\",\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 3, 4),\n        \"code\": \"DISCHARGE/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    # Visit 2 (Week 14, 2024): Treatment (Surgery)\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 4, 1),\n        \"code\": \"ADMISSION/Inpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 4, 1),\n        \"code\": \"CPT/55840\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"Surgical procedure completed.\",\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 4, 1),\n        \"code\": \"LOINC/6690-2\",\n        \"numeric_value\": 8.2,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 4, 1),\n        \"code\": \"DISCHARGE/Inpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    # Visit 3 (Week 20, 2024): Follow-up\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 5, 13),\n        \"code\": \"ADMISSION/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 5, 13),\n        \"code\": \"LOINC/2857-1\",\n        \"numeric_value\": 0.1,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 5, 13),\n        \"code\": \"NOTE/FollowUp\",\n        \"numeric_value\": np.nan,\n        \"text_value\": \"PSA levels are undetectable post-op.\",\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2024, 5, 13),\n        \"code\": \"DISCHARGE/Outpatient\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n    {\n        \"subject_id\": 202,\n        \"time\": datetime(2025, 5, 13),\n        \"code\": \"DEATH\",\n        \"numeric_value\": np.nan,\n        \"text_value\": np.nan,\n    },\n]\n\npatient_events_df = pd.DataFrame(patient_events_list)\npatient_events_df[\"time\"] = pd.to_datetime(patient_events_df[\"time\"])\npatient_events_df[\"subject_id\"] = patient_events_df[\"subject_id\"].astype(str)\n\n# Subject Splits DataFrame\nsubject_splits_list = [\n    {\"subject_id\": 101, \"split\": \"train\"},\n    {\n        \"subject_id\": 202,\n        \"split\": \"held_out\",\n    },  # 'held_out' is often used for the final test set\n]\nsubject_splits_df = pd.DataFrame(subject_splits_list)\n</pre> code_metadata_list = [     # Static Measurements     {\"code\": \"GENDER/Female\", \"description\": \"Female sex\"},     {\"code\": \"GENDER/Male\", \"description\": \"Male sex\"},     {\"code\": \"GENETIC/BRCA1_pos\", \"description\": \"BRCA1 gene mutation\"},     # Visit and Administrative Codes     {         \"code\": \"ADMISSION/Outpatient\",         \"description\": \"Admission for an outpatient clinic visit\",     },     {         \"code\": \"ADMISSION/Inpatient\",         \"description\": \"Admission to the hospital for an inpatient stay\",     },     {         \"code\": \"DISCHARGE/Outpatient\",         \"description\": \"Discharge from an outpatient clinic visit\",     },     {         \"code\": \"DISCHARGE/Inpatient\",         \"description\": \"Discharge from an inpatient hospital stay\",     },     {         \"code\": \"NOTE/FollowUp\",         \"description\": \"Clinical note for a follow-up appointment\",     },     # Diagnosis Codes (ICD-10-CM)     {         \"code\": \"ICD10CM/C34.90\",         \"description\": \"Malignant neoplasm of unspecified part of unspecified bronchus or lung\",     },     {\"code\": \"ICD10CM/C61\", \"description\": \"Malignant neoplasm of prostate\"},     # Symptom Codes     {\"code\": \"SYMPTOM/Cough\", \"description\": \"Patient reports a persistent cough\"},     # Procedure Codes (CPT)     {         \"code\": \"CPT/71250\",         \"description\": \"Procedure code for a CT scan of the thorax without contrast\",     },     {         \"code\": \"CPT/32408\",         \"description\": \"Procedure code for a core needle biopsy of the lung or mediastinum\",     },     {         \"code\": \"CPT/55700\",         \"description\": \"Procedure code for a needle biopsy of the prostate\",     },     {         \"code\": \"CPT/55840\",         \"description\": \"Procedure code for a radical retropubic prostatectomy\",     },     # Lab Codes (LOINC)     {         \"code\": \"LOINC/6690-2\",         \"description\": \"Leukocytes [#/volume] in Blood by Automated count (White Blood Cell Count)\",     },     {         \"code\": \"LOINC/2039-6\",         \"description\": \"Carcinoembryonic Ag [Mass/volume] in Serum or Plasma (CEA Tumor Marker)\",     },     {         \"code\": \"LOINC/59261-8\",         \"description\": \"Comprehensive metabolic 2014 panel - Serum or Plasma\",     },     {         \"code\": \"LOINC/2857-1\",         \"description\": \"Prostate specific Ag [Mass/volume] in Serum or Plasma (PSA Test)\",     },     # Medication Codes     {         \"code\": \"RX/Cisplatin\",         \"description\": \"Administration of Cisplatin chemotherapy agent\",     },     # Death     {\"code\": \"DEATH\", \"description\": \"Death\"}, ]  code_metadata_df = pd.DataFrame(code_metadata_list)   # Patient Events DataFrame patient_events_list = [     # Patient 101: Jane Doe (Lung Cancer) - Assigned to 'train' split     # Static data     {         \"subject_id\": 101,         \"time\": pd.NaT,         \"code\": \"GENDER/Female\",         \"numeric_value\": np.nan,         \"text_value\": \"Female\",     },     {         \"subject_id\": 101,         \"time\": pd.NaT,         \"code\": \"GENETIC/BRCA1_pos\",         \"numeric_value\": 1,         \"text_value\": \"Positive\",     },     # Visit 1 (Week 2, 2024): Diagnosis     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 8),         \"code\": \"ADMISSION/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 8),         \"code\": \"SYMPTOM/Cough\",         \"numeric_value\": np.nan,         \"text_value\": \"Persistent for 2 months\",     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 8),         \"code\": \"LOINC/6690-2\",         \"numeric_value\": 12.5,         \"text_value\": np.nan,     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 8),         \"code\": \"CPT/71250\",         \"numeric_value\": np.nan,         \"text_value\": \"Nodule found in right lung\",     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 8),         \"code\": \"CPT/32408\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 8),         \"code\": \"ICD10CM/C34.90\",         \"numeric_value\": np.nan,         \"text_value\": \"Primary Diagnosis\",     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 8),         \"code\": \"DISCHARGE/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     # Visit 2 (Week 4, 2024): Treatment     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 22),         \"code\": \"ADMISSION/Inpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 22),         \"code\": \"LOINC/59261-8\",         \"numeric_value\": np.nan,         \"text_value\": \"All values within normal limits\",     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 22),         \"code\": \"RX/Cisplatin\",         \"numeric_value\": np.nan,         \"text_value\": \"Cisplatin\",     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 1, 22),         \"code\": \"DISCHARGE/Inpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     # Visit 3 (Week 8, 2024): Follow-up     {         \"subject_id\": 101,         \"time\": datetime(2024, 2, 19),         \"code\": \"ADMISSION/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 2, 19),         \"code\": \"NOTE/FollowUp\",         \"numeric_value\": np.nan,         \"text_value\": \"Patient tolerated first cycle well.\",     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 2, 19),         \"code\": \"LOINC/2039-6\",         \"numeric_value\": 50.2,         \"text_value\": np.nan,     },     {         \"subject_id\": 101,         \"time\": datetime(2024, 2, 19),         \"code\": \"DISCHARGE/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     # Patient 202: John Smith (Prostate Cancer) - Assigned to 'held_out' split     # Static data     {         \"subject_id\": 202,         \"time\": pd.NaT,         \"code\": \"GENDER/Male\",         \"numeric_value\": np.nan,         \"text_value\": \"Male\",     },     # Visit 1 (Week 10, 2024): Diagnosis     {         \"subject_id\": 202,         \"time\": datetime(2024, 3, 4),         \"code\": \"ADMISSION/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 3, 4),         \"code\": \"LOINC/2857-1\",         \"numeric_value\": 15.1,         \"text_value\": np.nan,     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 3, 4),         \"code\": \"CPT/55700\",         \"numeric_value\": np.nan,         \"text_value\": \"Biopsy taken\",     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 3, 4),         \"code\": \"ICD10CM/C61\",         \"numeric_value\": np.nan,         \"text_value\": \"Primary Diagnosis\",     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 3, 4),         \"code\": \"DISCHARGE/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     # Visit 2 (Week 14, 2024): Treatment (Surgery)     {         \"subject_id\": 202,         \"time\": datetime(2024, 4, 1),         \"code\": \"ADMISSION/Inpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 4, 1),         \"code\": \"CPT/55840\",         \"numeric_value\": np.nan,         \"text_value\": \"Surgical procedure completed.\",     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 4, 1),         \"code\": \"LOINC/6690-2\",         \"numeric_value\": 8.2,         \"text_value\": np.nan,     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 4, 1),         \"code\": \"DISCHARGE/Inpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     # Visit 3 (Week 20, 2024): Follow-up     {         \"subject_id\": 202,         \"time\": datetime(2024, 5, 13),         \"code\": \"ADMISSION/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 5, 13),         \"code\": \"LOINC/2857-1\",         \"numeric_value\": 0.1,         \"text_value\": np.nan,     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 5, 13),         \"code\": \"NOTE/FollowUp\",         \"numeric_value\": np.nan,         \"text_value\": \"PSA levels are undetectable post-op.\",     },     {         \"subject_id\": 202,         \"time\": datetime(2024, 5, 13),         \"code\": \"DISCHARGE/Outpatient\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     },     {         \"subject_id\": 202,         \"time\": datetime(2025, 5, 13),         \"code\": \"DEATH\",         \"numeric_value\": np.nan,         \"text_value\": np.nan,     }, ]  patient_events_df = pd.DataFrame(patient_events_list) patient_events_df[\"time\"] = pd.to_datetime(patient_events_df[\"time\"]) patient_events_df[\"subject_id\"] = patient_events_df[\"subject_id\"].astype(str)  # Subject Splits DataFrame subject_splits_list = [     {\"subject_id\": 101, \"split\": \"train\"},     {         \"subject_id\": 202,         \"split\": \"held_out\",     },  # 'held_out' is often used for the final test set ] subject_splits_df = pd.DataFrame(subject_splits_list) In\u00a0[\u00a0]: Copied! <pre>patient_events_df\n</pre> patient_events_df In\u00a0[\u00a0]: Copied! <pre># Here we set a demo mapping for the event_category column - if not provided it uses a default\n# This is useful especially for cases when generating custom training data for LLMs\ndemo_mapping = {\n    \"SYMPTOM/Cough\": \"symptom\",\n    \"ICD10CM/C34.90\": \"diagnosis\",\n    \"DEATH\": \"death\",\n    \"RX/Cisplatin\": \"lot\",\n}\n</pre> # Here we set a demo mapping for the event_category column - if not provided it uses a default # This is useful especially for cases when generating custom training data for LLMs demo_mapping = {     \"SYMPTOM/Cough\": \"symptom\",     \"ICD10CM/C34.90\": \"diagnosis\",     \"DEATH\": \"death\",     \"RX/Cisplatin\": \"lot\", } In\u00a0[\u00a0]: Copied! <pre>#: Do actual conversion\ndf_converted_constant, df_converted_constant_description, df_converted_events = convert_meds_to_dtc(\n    df_codes=code_metadata_df,\n    df_data=patient_events_df,\n    df_split=subject_splits_df,\n    prefer_text_value_over_numeric=True,\n    event_category_mapping=demo_mapping,\n    no_value_default=\"observed\",\n)\n</pre> #: Do actual conversion df_converted_constant, df_converted_constant_description, df_converted_events = convert_meds_to_dtc(     df_codes=code_metadata_df,     df_data=patient_events_df,     df_split=subject_splits_df,     prefer_text_value_over_numeric=True,     event_category_mapping=demo_mapping,     no_value_default=\"observed\", ) In\u00a0[\u00a0]: Copied! <pre># Get for future use\nconstant_columns = df_converted_constant.columns.tolist()\nconstant_columns = [x for x in constant_columns if x not in [\"patientid\"]]\n</pre> # Get for future use constant_columns = df_converted_constant.columns.tolist() constant_columns = [x for x in constant_columns if x not in [\"patientid\"]] In\u00a0[\u00a0]: Copied! <pre># Set basics\nindication = \"meds_demo\"\nconfig = Config()  # Override values here to customize pipeline\nconfig.constant_columns_to_use = constant_columns\nconfig.constant_birthdate_column = None  # Not using in demo\nconfig.lot_event_name = None  # Setting for LoTs\nconfig.event_value_lot_start = None\n</pre> # Set basics indication = \"meds_demo\" config = Config()  # Override values here to customize pipeline config.constant_columns_to_use = constant_columns config.constant_birthdate_column = None  # Not using in demo config.lot_event_name = None  # Setting for LoTs config.event_value_lot_start = None In\u00a0[\u00a0]: Copied! <pre># Setup basics\ndm = DataManager(config=config)\n\ndm.load_indication_data(\n    df_events=df_converted_events,\n    df_constant=df_converted_constant,\n    df_constant_description=df_converted_constant_description,\n)\ndm.process_indication_data()\ndm.setup_unique_mapping_of_events()\ndm.setup_dataset_splits()\n\ndata_splitter_events = DataSplitterEvents(dm, config=config)\ndata_splitter_events.setup_variables()\nconverter = ConverterInstruction(\n    nr_tokens_budget_total=8192,\n    config=config,\n    dm=dm,\n)\n</pre> # Setup basics dm = DataManager(config=config)  dm.load_indication_data(     df_events=df_converted_events,     df_constant=df_converted_constant,     df_constant_description=df_converted_constant_description, ) dm.process_indication_data() dm.setup_unique_mapping_of_events() dm.setup_dataset_splits()  data_splitter_events = DataSplitterEvents(dm, config=config) data_splitter_events.setup_variables() converter = ConverterInstruction(     nr_tokens_budget_total=8192,     config=config,     dm=dm, ) In\u00a0[\u00a0]: Copied! <pre># Set example patient\npatientid = \"101\"\n\n# Get data\npatient_data = dm.get_patient_data(patientid)\npatient_data[\"events\"] = patient_data[\"events\"].sort_values(\"date\")\n\n\n# Here then split date\nsplit_date = patient_data[\"events\"][\"date\"].iloc[-1]\n\n\n# Generate splits to predict whether death will occur in the next 52 weeks\nevents_splits = data_splitter_events.get_splits_from_patient(\n    patient_data,\n    max_nr_samples_per_split=1,\n    override_split_dates=[split_date],\n    override_category=\"death\",\n    override_observation_time_delta=pd.Timedelta(weeks=52),\n)\nevents_split = events_splits[0][0]\n\n#: no forecasting split\nforecast_split = None\nforecasting_times_to_predict = None\n\n\n# Convert to instruction\nconverted = converter.forward_conversion_inference(\n    forecasting_split=forecast_split,\n    forecasting_future_weeks_per_variable=forecasting_times_to_predict,\n    event_split=events_split,\n    custom_tasks=None,\n)\n\nprint(converted[\"instruction\"])\n</pre> # Set example patient patientid = \"101\"  # Get data patient_data = dm.get_patient_data(patientid) patient_data[\"events\"] = patient_data[\"events\"].sort_values(\"date\")   # Here then split date split_date = patient_data[\"events\"][\"date\"].iloc[-1]   # Generate splits to predict whether death will occur in the next 52 weeks events_splits = data_splitter_events.get_splits_from_patient(     patient_data,     max_nr_samples_per_split=1,     override_split_dates=[split_date],     override_category=\"death\",     override_observation_time_delta=pd.Timedelta(weeks=52), ) events_split = events_splits[0][0]  #: no forecasting split forecast_split = None forecasting_times_to_predict = None   # Convert to instruction converted = converter.forward_conversion_inference(     forecasting_split=forecast_split,     forecasting_future_weeks_per_variable=forecasting_times_to_predict,     event_split=events_split,     custom_tasks=None, )  print(converted[\"instruction\"])"},{"location":"examples/integrations/meds_data_import/#example-on-how-to-import-meds-data-format","title":"Example on how to import MEDS data format\u00b6","text":""},{"location":"examples/integrations/meds_data_import/#synethetic-example","title":"Synethetic example\u00b6","text":"<p>Here we provide synthetic example data as generated by Gemini.</p>"},{"location":"examples/integrations/meds_data_import/#conversion-to-dtc-format","title":"Conversion to DTC format\u00b6","text":""},{"location":"examples/integrations/meds_data_import/#example-usage-in-digital_twin_converter-package","title":"Example usage in <code>digital_twin_converter</code> package\u00b6","text":"<p>Here we're showing an example for inference (i.e. using a pretrained model), but check out the other examples if you need to e.g. generate training data.</p>"},{"location":"reference/common/config/","title":"Config","text":""},{"location":"reference/common/config/#twinweaver.common.config","title":"twinweaver.common.config","text":""},{"location":"reference/common/config/#twinweaver.common.config-classes","title":"Classes","text":""},{"location":"reference/common/config/#twinweaver.common.config.Config","title":"Config","text":"<p>Centralized configuration repository for data processing, prompt generation, and constants.</p> <p>This class consolidates various configuration settings essential for the data processing pipeline. It defines standardized column names, specific values for event categories (like 'line of therapy', 'death'), data source identifiers, file paths, table names, and text templates (prompts) used for different language model tasks such as text conversion, forecasting (value prediction, time-to-event), quality assurance (QA) via binning, and setting up multi-task prompts. Default values are provided but can be overridden to adapt to specific datasets, model requirements, or experimental setups.</p> <p>Attributes:</p> Name Type Description <code>date_cutoff</code> <code>str | None</code> <p>If set, only use data before this date (format: \"YYYY-MM-DD\"), censored after. Default: None.</p> <code>delta_time_unit</code> <code>str</code> <p>Unit of time used to express intervals between patient visits in the generated text. Options are \"days\" or \"weeks\". Default: \"weeks\".</p> <code>numeric_detect_min_fraction</code> <code>float</code> <p>Fraction of values that must be numeric to classify a variable as numeric. Defaults to 0.99.</p> <code>date_col</code> <code>str</code> <p>Standardized column name for date information across datasets. Default: \"date\".</p> <code>patient_id_col</code> <code>str</code> <p>Standardized column name for unique patient identifiers. Default: \"patientid\".</p> <code>event_category_col</code> <code>str</code> <p>Standardized column name for the category of a recorded event (e.g., 'lab', 'diagnosis'). Default: \"event_category\".</p> <code>event_name_col</code> <code>str</code> <p>Standardized column name for the specific name of an event within its category (e.g., 'Glucose level', 'Type 2 Diabetes'). Default: \"event_name\".</p> <code>event_descriptive_name_col</code> <code>str</code> <p>Standardized column name for a more human-readable or descriptive name of the event. Default: \"event_descriptive_name\".</p> <code>event_value_col</code> <code>str</code> <p>Standardized column name for the value associated with an event (e.g., a lab result, 'present'). Default: \"event_value\".</p> <code>source_col</code> <code>str</code> <p>Standardized column name indicating the origin or type of the data record. Default: \"source\".</p> <code>meta_data_col</code> <code>str</code> <p>Standardized column name for storing additional metadata related to an event. Default: \"meta_data\".</p> <code>constant_split_col</code> <code>str</code> <p>Standardized column name for data split information (train/test/val) in the constant dataframe. Default: \"data_split\".</p> <code>event_category_default_value</code> <code>str</code> <p>Default value to assign to <code>event_category_col</code> if it is missing in the data. Default: \"general\".</p> <code>event_meta_default_value</code> <code>Any</code> <p>Default value to assign to <code>meta_data_col</code> if it is missing. Default: pd.NA.</p> <code>source_col_default_value</code> <code>str</code> <p>Default value to assign to <code>source_col</code> if it is missing. Default: \"events\".</p> <code>split_date_col</code> <code>str</code> <p>Column name specifically used for dates related to line of therapy (LoT) events. Default: \"lot_date\".</p> <code>lot_event_name</code> <code>str</code> <p>Column name for the name or identifier of the line of therapy (e.g., \"First Line\"). Default: \"lot\".</p> <code>event_value_lot_start</code> <code>str</code> <p>Specific string value used in <code>event_value_col</code> to denote the start of a line of therapy. Default: \"LoT Start\".</p> <code>skip_future_lot_filtering</code> <code>bool</code> <p>Flag indicating whether to skip filtering out future line of therapy events. Default: False. Useful in case you accidentially overlap LoTs which are actually the same, use with caution.</p> <code>lot_concatenate_descriptive_and_value</code> <code>bool</code> <p>Flag indicating whether to concatenate the descriptive name and value for line of therapy events. Default: False.</p> <code>lot_concatenate_string</code> <code>str</code> <p>String used to concatenate the descriptive name and value for line of therapy events when <code>lot_concatenate_descriptive_and_value</code> is True. Default: \" - \".</p> <code>warning_for_splitters_patient_without_splits</code> <code>bool</code> <p>Whether to warn if a patient has no split events. Default: True.</p> <code>event_category_lot</code> <code>str</code> <p>Specific string value used in <code>event_category_col</code> to identify 'line of therapy' events. Default: \"lot\".</p> <code>event_category_death</code> <code>str</code> <p>Specific string value used in <code>event_category_col</code> to identify 'death' events. Default: \"death\".</p> <code>event_category_labs</code> <code>str</code> <p>Specific string value used in <code>event_category_col</code> to identify 'lab result' events. Default: \"lab\".</p> <code>event_category_forecast</code> <code>list[str] | None</code> <p>List of event categories to be considered for forecasting tasks. Default: None.</p> <code>split_event_category</code> <code>str | None</code> <p>Event category used for data splitting (e.g., LoT). Default: None.</p> <code>source_genetic</code> <code>str</code> <p>Specific string value used in <code>source_col</code> to identify data originating from genetic testing. Default: \"genetic\".</p> <code>genetic_skip_text_value</code> <code>str</code> <p>A specific event value (often for genetic data) that might be skipped during text generation to avoid redundancy if its presence is implied elsewhere. Default: \"present\".</p> <code>genetic_tag_opening</code> <code>str</code> <p>Opening tag used to demarcate genetic information within generated text. Default: \"\". <code>genetic_tag_closing</code> <code>str</code> <p>Closing tag used to demarcate genetic information within generated text. Default: \"\".</p> <code>event_table_name</code> <code>str</code> <p>The base name (without extension) for the primary file or table containing event data. Default: \"events\".</p> <code>train_split_name</code> <code>str</code> <p>Identifier for the training dataset split (e.g., used in file naming or data loading). Default: \"train\".</p> <code>validation_split_name</code> <code>str</code> <p>Identifier for the validation dataset split. Default: \"validation\".</p> <code>test_split_name</code> <code>str</code> <p>Identifier for the test dataset split. Default: \"test\".</p> <code>bins_split_name</code> <code>str</code> <p>Identifier for a data split used for binning tasks, often related to QA. Default: \"5_equal_sized_bins\".</p> <code>preamble_text</code> <code>str</code> <p>Introductory text inserted at the beginning of the textual representation of a patient's record. Default: Explains structure and LOINC codes.</p> <code>constant_text</code> <code>str</code> <p>Text used to introduce the section containing static demographic data in the textual patient record. Default: \"\\n\\nStarting with demographic data:\\n\".</p> <code>genetic_empty_text</code> <code>str</code> <p>Text to use when no genetic data is available for a patient. Default: \"No genetic data available.\".</p> <code>first_day_text</code> <code>str</code> <p>Text used to introduce the events that occurred on the patient's very first recorded visit day. Default: \"\\nOn the first visit, the patient experienced the following: \\n\".</p> <code>event_day_preamble</code> <code>str</code> <p>Text inserted before the description of events for visits subsequent to the first one. Default: \"\\n\".</p> <code>event_day_text</code> <code>str</code> <p>Template text used to introduce events on subsequent visit days, indicating the time elapsed since the previous visit. Default: \" self.delta_time_unit : later, the patient visited and experienced the following: \\n\".</p> <code>post_event_text</code> <code>str</code> <p>Text appended after listing all events for a specific visit day. Default: \".\\n\".</p> <code>forecasting_fval_prompt_start</code> <code>str</code> <p>Initial text for prompts instructing a language model to predict future numerical values of specified variables over time. Default: Instructs prediction per cumulative week.</p> <code>forecasting_prompt_var_time</code> <code>str</code> <p>Text segment used within forecasting prompts to specify the time frame (e.g., future weeks) for prediction. Default: \" the future weeks \".</p> <code>forecasting_prompt_summarized_start</code> <code>str</code> <p>Initial text for prompts that include a summary of the last known values of variables being forecasted. Default: \"\\nThe last values of the variables in the input data are:\\n\".</p> <code>forecasting_firstday_override</code> <code>str</code> <p>Alternative introductory text for forecasting prompts, possibly used when only a subset of initial data is presented, hinting at omissions. Default: Mentions included events, potential omissions.</p> <code>forecasting_prompt_summarized_genetic</code> <code>str</code> <p>Text used to introduce a summary section listing the last observed genetic event statuses within a forecasting prompt. Default: \"\\n\\n\\n\\nHere we repeat the last observed values of each genetic event in the input data:\\n\".</p> <code>forecasting_prompt_summarized_lot</code> <code>str</code> <p>Text used to introduce a summary section describing the most recent line of therapy within a forecasting prompt. Default: \"\\nThe most recent line of therapy:\\n\".</p> <code>forecasting_tte_prompt_start</code> <code>str</code> <p>Initial text for prompts instructing a language model to predict time-to-event (TTE) outcomes, specifically focusing on whether an event is censored. Default: Asks for censoring prediction.</p> <code>forecasting_tte_prompt_mid</code> <code>str</code> <p>Middle text segment for TTE prompts, specifying the prediction horizon (in weeks) and asking about event occurrence status. Default: Specifies weeks and asks about occurrence.</p> <code>forecasting_tte_prompt_end</code> <code>str</code> <p>Concluding text for TTE prompts, detailing the required output format for the prediction (censoring and occurrence). Default: Specifies format like \"'Here is the prediction: the event () was [not] censored and [did not occur]/[occurred].'\". <code>target_prompt_start</code> <code>str</code> <p>Template string used to begin constructing the target (ground truth) output string for TTE tasks, includes placeholder for event name. Default: \"\\nHere is the prediction: the event ({event_name}) was \".</p> <code>target_prompt_censor_true</code> <code>str</code> <p>Text segment used in the TTE target output to indicate that the event was censored within the observation period. Default: \"censored.\".</p> <code>target_prompt_censor_false</code> <code>str</code> <p>Text segment used in the TTE target output to indicate that the event was not censored. Default: \"not censored \".</p> <code>target_prompt_before_occur</code> <code>str</code> <p>Conjunction used in the TTE target output between the censoring status and the occurrence status. Default: \"and \".</p> <code>target_prompt_occur</code> <code>str</code> <p>Text segment used in the TTE target output to indicate that the event did occur. Default: \"occurred.\".</p> <code>target_prompt_not_occur</code> <code>str</code> <p>Text segment used in the TTE target output to indicate that the event did not occur. Default: \"did not occur.\".</p> <code>qa_prompt_start</code> <code>str</code> <p>Initial text for prompts instructing a model to perform a Quality Assurance (QA) task, specifically predicting value bins for future variable values. Default: Asks for bin prediction per week.</p> <code>qa_bins_start</code> <code>str</code> <p>Text used within QA prompts to introduce the list of possible bins the model should choose from. Default: \"\\tThe possible bins are: \".</p> <code>task_prompt_start</code> <code>str</code> <p>Introductory text for multi-task prompts, explaining that multiple tasks follow and instructing the model on the required response format (e.g., prefixing each answer with 'Task X:'). Default: Explains multi-task format.</p> <code>task_prompt_each_task</code> <code>str</code> <p>Template string used to introduce each individual task within a multi-task prompt, includes placeholder for task number. Default: \"Task {task_nr} is \".</p> <code>task_prompt_end</code> <code>str</code> <p>Concluding text for the overall multi-task prompt setup. Default: \"\" (empty string).</p> <code>task_prompt_forecasting</code> <code>str</code> <p>Identifier text appended to <code>task_prompt_each_task</code> to specify a forecasting sub-task. Default: \"forecasting:\".</p> <code>task_prompt_forecasting_qa</code> <code>str</code> <p>Identifier text appended to <code>task_prompt_each_task</code> to specify a forecasting QA (binning) sub-task. Default: \"forecasting QA:\".</p> <code>task_prompt_events</code> <code>str</code> <p>Identifier text appended to <code>task_prompt_each_task</code> to specify a time-to-event prediction sub-task. Default: \"time to event prediction:\".</p> <code>task_prompt_custom</code> <code>str</code> <p>Identifier text appended to <code>task_prompt_each_task</code> to specify a custom-defined sub-task. Default: \" a custom task:\".</p> <code>task_target_start</code> <code>str</code> <p>Template string used to begin the target (ground truth) output corresponding to a specific task number in a multi-task setting. Default: \"Task {task_nr} is \".</p> <code>task_target_end</code> <code>str</code> <p>Concluding text for the target output of a specific task within a multi-task response. Default: \"\" (empty string).</p> <code>decimal_precision</code> <code>int</code> <p>Number of decimal places to use when rounding numerical values (e.g., lab results) during text conversion. Default: 2.</p> <code>event_category_preamble_mapping_override</code> <code>dict | None</code> <p>Optional dictionary to override the introductory text used before listing events of a specific category on a given day. Structure: <code>{&lt;event_category&gt;: &lt;preamble_string&gt;}</code>. Default: None.</p> <code>event_category_and_name_replace_override</code> <code>dict | None</code> <p>Optional nested dictionary to define specific replacements for event descriptions based on category and name. Allows replacing the entire event string and defining a value for reverse mapping. Structure: <code>{&lt;event_category&gt;: {&lt;event_name&gt;: {\"full_replacement_string\": &lt;str&gt;, \"reverse_string_value\": &lt;str&gt;}}}</code>. Default: None.</p> <code>always_keep_first_visit</code> <code>bool</code> <p>Flag indicating whether the events from the very first visit should always be included in the patient history, regardless of token budget constraints. Default: True.</p> <code>seed</code> <code>int</code> <p>Seed value for random number generators to ensure reproducibility in processes like data splitting or sampling. Default: 768921.</p> <code>nr_tokens_budget_padding</code> <code>int</code> <p>Number of tokens reserved as a buffer when calculating token budgets, ensuring outputs don't exceed limits. May need adjustment based on model/task. Default: 200.</p> <code>tokenizer_to_use</code> <code>str</code> <p>Identifier string for the tokenizer model to be used for counting tokens (e.g., for budget calculations). Should correspond to a model available in the environment (e.g., from Hugging Face). Default: 'microsoft/Phi-4-mini-instruct'.</p> <code>constant_columns_to_use</code> <code>list[str]</code> <p>List of column names from the constant (demographic) data source to be included in the processing and text conversion. Note: Age might be handled separately. Default: [\"race\", \"gender\", \"ethnicity\", \"indication\"].</p> <code>constant_birthdate_column</code> <code>str | None</code> <p>Column name in the constant table representing the patient's birth date or birth year. If provided, age calculation is performed relative to the first event date. Depends on constant_birthdate_column_format for different calculations: - constant_birthdate_column_format=\"age\": The column represents age directly in years. - \"date\": The column represents the birth date (format: \"YYYY-MM-DD\" or \"YYYY\"). Default: None.</p> <code>constant_birthdate_column_format</code> <code>str</code> <p>Format of the birthdate column, either \"date\" or \"age\". Default: \"date\".</p> <code>data_splitter_events_variables_category_mapping</code> <code>dict | None</code> <p>Mapping defining which event categories correspond to specific prediction types in DataSplitterEvents. Keys are event categories (e.g., 'death', 'progression'), values are descriptive names for the target variable. Default: None.</p> <code>data_splitter_events_backup_category_mapping</code> <code>dict</code> <p>Fallback mapping for event categories in DataSplitterEvents. Used if the primary category variables are not found. Keys are the missing categories, values are the backup categories to use. Default: {\"progression\": \"death\"}.</p> Source code in <code>twinweaver/common/config.py</code> <pre><code>class Config:\n    \"\"\"\n    Centralized configuration repository for data processing, prompt generation, and constants.\n\n    This class consolidates various configuration settings essential for the data processing\n    pipeline. It defines standardized column names, specific values for event categories (like\n    'line of therapy', 'death'), data source identifiers, file paths, table names, and text\n    templates (prompts) used for different language model tasks such as text conversion,\n    forecasting (value prediction, time-to-event), quality assurance (QA) via binning, and\n    setting up multi-task prompts. Default values are provided but can be overridden to\n    adapt to specific datasets, model requirements, or experimental setups.\n\n    Attributes\n    ----------\n    date_cutoff : str | None\n        If set, only use data before this date (format: \"YYYY-MM-DD\"), censored after. Default: None.\n    delta_time_unit : str\n        Unit of time used to express intervals between patient visits in the generated text. Options are \"days\" or\n        \"weeks\". Default: \"weeks\".\n    numeric_detect_min_fraction: float\n        Fraction of values that must be numeric to classify a variable as numeric. Defaults to 0.99.\n    date_col : str\n        Standardized column name for date information across datasets. Default: \"date\".\n    patient_id_col : str\n        Standardized column name for unique patient identifiers. Default: \"patientid\".\n    event_category_col : str\n        Standardized column name for the category of a recorded event (e.g., 'lab', 'diagnosis'). Default:\n        \"event_category\".\n    event_name_col : str\n        Standardized column name for the specific name of an event within its category (e.g., 'Glucose level',\n        'Type 2 Diabetes'). Default: \"event_name\".\n    event_descriptive_name_col : str\n        Standardized column name for a more human-readable or descriptive name of the event. Default:\n        \"event_descriptive_name\".\n    event_value_col : str\n        Standardized column name for the value associated with an event (e.g., a lab result, 'present').\n        Default: \"event_value\".\n    source_col : str\n        Standardized column name indicating the origin or type of the data record. Default: \"source\".\n    meta_data_col : str\n        Standardized column name for storing additional metadata related to an event. Default: \"meta_data\".\n    constant_split_col : str\n        Standardized column name for data split information (train/test/val) in the constant dataframe.\n        Default: \"data_split\".\n    event_category_default_value : str\n        Default value to assign to `event_category_col` if it is missing in the data. Default: \"general\".\n    event_meta_default_value : Any\n        Default value to assign to `meta_data_col` if it is missing. Default: pd.NA.\n    source_col_default_value : str\n        Default value to assign to `source_col` if it is missing. Default: \"events\".\n    split_date_col : str\n        Column name specifically used for dates related to line of therapy (LoT) events. Default: \"lot_date\".\n    lot_event_name : str\n        Column name for the name or identifier of the line of therapy (e.g., \"First Line\"). Default: \"lot\".\n    event_value_lot_start : str\n        Specific string value used in `event_value_col` to denote the start of a line of therapy. Default: \"LoT Start\".\n    skip_future_lot_filtering : bool\n        Flag indicating whether to skip filtering out future line of therapy events. Default: False.\n        Useful in case you accidentially overlap LoTs which are actually the same, use with caution.\n    lot_concatenate_descriptive_and_value : bool\n        Flag indicating whether to concatenate the descriptive name and value for line of therapy events.\n        Default: False.\n    lot_concatenate_string : str\n        String used to concatenate the descriptive name and value for line of therapy events when\n        `lot_concatenate_descriptive_and_value` is True. Default: \" - \".\n    warning_for_splitters_patient_without_splits : bool\n        Whether to warn if a patient has no split events. Default: True.\n    event_category_lot : str\n        Specific string value used in `event_category_col` to identify 'line of therapy' events. Default: \"lot\".\n    event_category_death : str\n        Specific string value used in `event_category_col` to identify 'death' events. Default: \"death\".\n    event_category_labs : str\n        Specific string value used in `event_category_col` to identify 'lab result' events. Default: \"lab\".\n    event_category_forecast : list[str] | None\n        List of event categories to be considered for forecasting tasks. Default: None.\n    split_event_category : str | None\n        Event category used for data splitting (e.g., LoT). Default: None.\n    source_genetic : str\n        Specific string value used in `source_col` to identify data originating from genetic testing.\n        Default: \"genetic\".\n    genetic_skip_text_value : str\n        A specific event value (often for genetic data) that might be skipped during text generation to avoid\n        redundancy if its presence is implied elsewhere. Default: \"present\".\n    genetic_tag_opening : str\n        Opening tag used to demarcate genetic information within generated text. Default: \"&lt;genetic&gt;\".\n    genetic_tag_closing : str\n        Closing tag used to demarcate genetic information within generated text. Default: \"&lt;/genetic&gt;\".\n    event_table_name : str\n        The base name (without extension) for the primary file or table containing event data. Default: \"events\".\n    train_split_name : str\n        Identifier for the training dataset split (e.g., used in file naming or data loading). Default: \"train\".\n    validation_split_name : str\n        Identifier for the validation dataset split. Default: \"validation\".\n    test_split_name : str\n        Identifier for the test dataset split. Default: \"test\".\n    bins_split_name : str\n        Identifier for a data split used for binning tasks, often related to QA. Default: \"5_equal_sized_bins\".\n    preamble_text : str\n        Introductory text inserted at the beginning of the textual representation of a patient's record.\n        Default: Explains structure and LOINC codes.\n    constant_text : str\n        Text used to introduce the section containing static demographic data in the textual patient record.\n        Default: \"\\\\n\\\\nStarting with demographic data:\\\\n\".\n    genetic_empty_text : str\n        Text to use when no genetic data is available for a patient. Default: \"No genetic data available.\".\n    first_day_text : str\n        Text used to introduce the events that occurred on the patient's very first recorded visit day.\n        Default: \"\\\\nOn the first visit, the patient experienced the following: \\\\n\".\n    event_day_preamble : str\n        Text inserted before the description of events for visits subsequent to the first one. Default: \"\\\\n\".\n    event_day_text : str\n        Template text used to introduce events on subsequent visit days, indicating the time elapsed since the previous\n        visit. Default: \" self.delta_time_unit : later, the patient visited and experienced the following: \\\\n\".\n    post_event_text : str\n        Text appended after listing all events for a specific visit day. Default: \".\\\\n\".\n    forecasting_fval_prompt_start : str\n        Initial text for prompts instructing a language model to predict future numerical values of specified\n        variables over time. Default: Instructs prediction per cumulative week.\n    forecasting_prompt_var_time : str\n        Text segment used within forecasting prompts to specify the time frame (e.g., future weeks) for prediction.\n        Default: \" the future weeks \".\n    forecasting_prompt_summarized_start : str\n        Initial text for prompts that include a summary of the last known values of variables being forecasted.\n        Default: \"\\\\nThe last values of the variables in the input data are:\\\\n\".\n    forecasting_firstday_override : str\n        Alternative introductory text for forecasting prompts, possibly used when only a subset of initial data is\n        presented, hinting at omissions. Default: Mentions included events, potential omissions.\n    forecasting_prompt_summarized_genetic : str\n        Text used to introduce a summary section listing the last observed genetic event statuses within a forecasting\n        prompt.\n        Default: \"\\\\n\\\\n\\\\n\\\\nHere we repeat the last observed values of each genetic event in the input data:\\\\n\".\n    forecasting_prompt_summarized_lot : str\n        Text used to introduce a summary section describing the most recent line of therapy within a forecasting\n        prompt. Default: \"\\\\nThe most recent line of therapy:\\\\n\".\n    forecasting_tte_prompt_start : str\n        Initial text for prompts instructing a language model to predict time-to-event (TTE) outcomes, specifically\n        focusing on whether an event is censored. Default: Asks for censoring prediction.\n    forecasting_tte_prompt_mid : str\n        Middle text segment for TTE prompts, specifying the prediction horizon (in weeks) and asking about event\n        occurrence status. Default: Specifies weeks and asks about occurrence.\n    forecasting_tte_prompt_end : str\n        Concluding text for TTE prompts, detailing the required output format for the prediction (censoring and\n        occurrence). Default: Specifies format like \"'Here is the prediction: the event (&lt;name&gt;) was [not] censored\n        and [did not occur]/[occurred].'\".\n    target_prompt_start : str\n        Template string used to begin constructing the target (ground truth) output string for TTE tasks, includes\n        placeholder for event name. Default: \"\\\\nHere is the prediction: the event ({event_name}) was \".\n    target_prompt_censor_true : str\n        Text segment used in the TTE target output to indicate that the event *was* censored within the observation\n        period. Default: \"censored.\".\n    target_prompt_censor_false : str\n        Text segment used in the TTE target output to indicate that the event *was not* censored. Default:\n        \"not censored \".\n    target_prompt_before_occur : str\n        Conjunction used in the TTE target output between the censoring status and the occurrence status.\n        Default: \"and \".\n    target_prompt_occur : str\n        Text segment used in the TTE target output to indicate that the event *did* occur. Default: \"occurred.\".\n    target_prompt_not_occur : str\n        Text segment used in the TTE target output to indicate that the event *did not* occur.\n        Default: \"did not occur.\".\n    qa_prompt_start : str\n        Initial text for prompts instructing a model to perform a Quality Assurance (QA) task, specifically predicting\n        value bins for future variable values. Default: Asks for bin prediction per week.\n    qa_bins_start : str\n        Text used within QA prompts to introduce the list of possible bins the model should choose from.\n        Default: \"\\\\tThe possible bins are: \".\n    task_prompt_start : str\n        Introductory text for multi-task prompts, explaining that multiple tasks follow and instructing the model on the\n        required response format (e.g., prefixing each answer with 'Task X:'). Default: Explains multi-task format.\n    task_prompt_each_task : str\n        Template string used to introduce each individual task within a multi-task prompt, includes placeholder for\n        task number. Default: \"Task {task_nr} is \".\n    task_prompt_end : str\n        Concluding text for the overall multi-task prompt setup. Default: \"\" (empty string).\n    task_prompt_forecasting : str\n        Identifier text appended to `task_prompt_each_task` to specify a forecasting sub-task. Default: \"forecasting:\".\n    task_prompt_forecasting_qa : str\n        Identifier text appended to `task_prompt_each_task` to specify a forecasting QA (binning) sub-task.\n        Default: \"forecasting QA:\".\n    task_prompt_events : str\n        Identifier text appended to `task_prompt_each_task` to specify a time-to-event prediction sub-task.\n        Default: \"time to event prediction:\".\n    task_prompt_custom : str\n        Identifier text appended to `task_prompt_each_task` to specify a custom-defined sub-task.\n        Default: \" a custom task:\".\n    task_target_start : str\n        Template string used to begin the target (ground truth) output corresponding to a specific task number in a\n        multi-task setting. Default: \"Task {task_nr} is \".\n    task_target_end : str\n        Concluding text for the target output of a specific task within a multi-task response.\n        Default: \"\" (empty string).\n    decimal_precision : int\n        Number of decimal places to use when rounding numerical values (e.g., lab results) during text conversion.\n        Default: 2.\n    event_category_preamble_mapping_override : dict | None\n        Optional dictionary to override the introductory text used before listing events of a specific category on a\n        given day. Structure: `{&lt;event_category&gt;: &lt;preamble_string&gt;}`. Default: None.\n    event_category_and_name_replace_override : dict | None\n        Optional nested dictionary to define specific replacements for event descriptions based on category and name.\n        Allows replacing the entire event string and defining a value for reverse mapping.\n        Structure:\n        `{&lt;event_category&gt;: {&lt;event_name&gt;: {\"full_replacement_string\": &lt;str&gt;, \"reverse_string_value\": &lt;str&gt;}}}`.\n        Default: None.\n    always_keep_first_visit : bool\n        Flag indicating whether the events from the very first visit should always be included in the patient history,\n        regardless of token budget constraints. Default: True.\n    seed : int\n        Seed value for random number generators to ensure reproducibility in processes like data splitting or sampling.\n        Default: 768921.\n    nr_tokens_budget_padding : int\n        Number of tokens reserved as a buffer when calculating token budgets, ensuring outputs don't exceed limits.\n        May need adjustment based on model/task. Default: 200.\n    tokenizer_to_use : str\n        Identifier string for the tokenizer model to be used for counting tokens (e.g., for budget calculations).\n        Should correspond to a model available in the environment (e.g., from Hugging Face).\n        Default: 'microsoft/Phi-4-mini-instruct'.\n    constant_columns_to_use : list[str]\n        List of column names from the constant (demographic) data source to be included in the processing and text\n        conversion. *Note: Age might be handled separately.* Default: [\"race\", \"gender\", \"ethnicity\", \"indication\"].\n    constant_birthdate_column : str | None\n        Column name in the constant table representing the patient's birth date or birth year.\n        If provided, age calculation is performed relative to the first event date.\n        Depends on constant_birthdate_column_format for different calculations:\n        - constant_birthdate_column_format=\"age\": The column represents age directly in years.\n        - \"date\": The column represents the birth date (format: \"YYYY-MM-DD\" or \"YYYY\").\n        Default: None.\n    constant_birthdate_column_format : str\n        Format of the birthdate column, either \"date\" or \"age\". Default: \"date\".\n    data_splitter_events_variables_category_mapping : dict | None\n        Mapping defining which event categories correspond to specific prediction types in DataSplitterEvents.\n        Keys are event categories (e.g., 'death', 'progression'), values are descriptive names for the target variable.\n        Default: None.\n    data_splitter_events_backup_category_mapping : dict\n         Fallback mapping for event categories in DataSplitterEvents. Used if the primary category variables are not\n         found. Keys are the missing categories, values are the backup categories to use.\n         Default: {\"progression\": \"death\"}.\n    \"\"\"\n\n    def __init__(self):\n        # Critical parameters for instruction mode - need to be set!\n        self.split_event_category: str = None  # e.g. \"lot\" -Event category used for data splitting (e.g., LoT)\n\n        # Needs to be set if using forecasting in instructions!\n        self.event_category_forecast: list = None  # e.g. [\"lab\"] - List of event categories to be used for forecasting\n\n        # Needs to be set if using DataSplitterEvents!\n        # Used to identify which variables correspond to which event categories for\n        # different event types as well as how they should be written down (since based on categories),\n        # for example, based on GDT: { \"death\": \"death\", \"progression\": \"next progression\", \"lot\":\n        # \"next line of therapy\", \"metastasis\": \"next metastasis\"}\n        self.data_splitter_events_variables_category_mapping = None\n\n        # --- Import data parameters ---\n        self.date_cutoff = None  # If set, only use data before this date (format: \"YYYY-MM-DD\"), censored after\n        self.delta_time_unit: str = (\n            \"weeks\"  # Either \"days\" or \"weeks\" - if you change this, you need to call set_delta_time_unit\n        )\n        self.numeric_detect_min_fraction: float = (\n            0.99  # Fraction of numeric values required to consider an event as numeric\n        )\n\n        # --- Core Column Names ---\n        self.date_col: str = \"date\"\n        self.patient_id_col: str = \"patientid\"\n        self.event_category_col: str = \"event_category\"\n        self.event_name_col: str = \"event_name\"\n        self.event_descriptive_name_col: str = \"event_descriptive_name\"\n        self.event_value_col: str = \"event_value\"\n        self.source_col: str = \"source\"\n        self.meta_data_col: str = \"meta_data\"\n        self.constant_split_col: str = \"data_split\"\n\n        # --- Specific Category/Type Column Names ---\n        self.event_category_default_value = \"general\"  # Default value for event category if not present\n        self.event_meta_default_value = pd.NA  # Default value for event meta data if not present\n        self.source_col_default_value: str = \"events\"  # Default value for source column if not present\n        self.split_date_col: str = \"split_date\"\n        self.lot_event_name: str = \"lot\"\n        self.event_value_lot_start: str = \"LoT Start\"\n        self.skip_future_lot_filtering: bool = False  # Whether to skip filtering future LoT events, by default False.\n        self.lot_concatenate_descriptive_and_value: bool = (\n            False  # If true, concatenate descriptive name and value for LoT events, by default False (only event_vale.)\n        )\n        self.lot_concatenate_string: str = (\n            \" - \"  # String used to concatenate descriptive name and value for LoT events, by default \" - \".\n        )\n\n        # Warnings and logs\n        self.warning_for_splitters_patient_without_splits: bool = (\n            True  # Whether to warn if a patient has no LoT events in DataSplitterEvents\n        )\n\n        # --- Specific Event Categories / Values / Sources ---\n        self.event_category_lot: str = \"lot\"\n        self.event_category_death: str = \"death\"\n        self.event_category_labs: str = \"lab\"\n\n        self.source_genetic: str = \"genetic\"\n        self.genetic_skip_text_value: str = \"present\"\n        self.genetic_tag_opening: str = \"&lt;genetic&gt;\"\n        self.genetic_tag_closing: str = \"&lt;/genetic&gt;\"\n\n        # --- Data Paths, Tables, and Splits ---\n        self.event_table_name: str = \"events\"\n        self.train_split_name: str = \"train\"\n        self.validation_split_name: str = \"validation\"\n        self.test_split_name: str = \"test\"\n        self.bins_split_name: str = \"5_equal_sized_bins\"\n\n        # --- Text Conversion Prompts ---\n        self.preamble_text: str = (\n            \"The following is a patient, starting with the demographic data, \"\n            \"following visit by visit everything that the patient experienced. \"\n            \"All lab codes refer to LOINC codes.\"\n        )\n        self.constant_text: str = \"\\n\\nStarting with demographic data:\\n\"\n        self.first_day_text: str = \"\\nOn the first visit, the patient experienced the following: \\n\"\n        self.event_day_preamble: str = \"\\n\"\n        self._event_day_text_template: str = \" {unit} later, the patient visited and experienced the following: \\n\"\n        self.event_day_text: str = self._event_day_text_template.format(unit=self.delta_time_unit)\n        self.post_event_text: str = \".\\n\"\n        self.genetic_empty_text: str = \"No genetic data available.\"\n\n        # --- Forecasting Prompts (General &amp; Summarization) ---\n        self._forecasting_fval_prompt_start_template: str = (\n            \"\\nYour task is to predict the future values of the following variables \"\n            \"for each cumulative {unit} starting from the last visit:\\n\"\n        )\n        self.forecasting_fval_prompt_start: str = self._forecasting_fval_prompt_start_template.format(\n            unit=self.delta_time_unit\n        )\n\n        self._forecasting_prompt_var_time_template: str = \" the future {unit} \"\n        self.forecasting_prompt_var_time: str = self._forecasting_prompt_var_time_template.format(\n            unit=self.delta_time_unit\n        )\n        self.forecasting_prompt_summarized_start: str = \"\\nThe last values of the variables in the input data are:\\n\"\n        self.forecasting_firstday_override: str = (\n            \"\\nThe following events are included in the input data, though \"\n            \"potentially there are more which were omitted. Starting with: \\n\"\n        )\n        self.forecasting_prompt_summarized_genetic: str = (\n            \"\\n\\n\\n\\nHere we repeat the last observed values of each genetic event in the input data:\\n\"\n        )\n        self.forecasting_prompt_summarized_lot: str = \"\\nThe most recent line of therapy:\\n\"\n\n        # --- Forecasting Prompts (Time-to-Event Specific) ---\n        self.forecasting_tte_prompt_start: str = \"\\nYour task is to predict whether the following event was censored \"\n        self._forecasting_tte_prompt_mid_template: str = (\n            \" {unit} from the last clinical visit and whether the event occurred or not: \"\n        )\n        self.forecasting_tte_prompt_mid: str = self._forecasting_tte_prompt_mid_template.format(\n            unit=self.delta_time_unit\n        )\n        self.forecasting_tte_prompt_end: str = (\n            \".\\nPlease provide your prediction in the following format: \"\n            \"'Here is the prediction: the event (&lt;name of event&gt;) was [not] censored \"\n            \"and [did not occur]/[occurred].'\"\n        )\n\n        # --- Target Output Prompts (Time-to-Event) ---\n        self.target_prompt_start: str = \"\\nHere is the prediction: the event ({event_name}) was \"\n        self.target_prompt_censor_true: str = \"censored.\"\n        self.target_prompt_censor_false: str = \"not censored \"\n        self.target_prompt_before_occur: str = \"and \"\n        self.target_prompt_occur: str = \"occurred.\"\n        self.target_prompt_not_occur: str = \"did not occur.\"\n\n        # --- QA Prompts (Binning) ---\n        self._qa_prompt_start_template: str = (\n            \"\\nYour task is to predict the appropriate bins for the future values of \"\n            \"the following variables for each cumulative {unit} starting from the date of the last visit:\"\n        )\n        self.qa_prompt_start = self._qa_prompt_start_template.format(unit=self.delta_time_unit)\n        self.qa_bins_start: str = \"\\tThe possible bins are: \"\n\n        # --- Multi-Task Prompts ---\n        self.task_prompt_start: str = (\n            \"\\nYou will now have multiple tasks to complete. Please answer for each \"\n            \"task in the same order as they are presented. Before every response state the \"\n            \"task nr, e.g. 'Task 2:'.\\n\\n\"\n        )\n        self.task_prompt_each_task: str = \"Task {task_nr} is \"\n        self.task_prompt_end: str = \"\"\n        self.task_prompt_forecasting: str = \"forecasting:\"\n        self.task_prompt_forecasting_qa: str = \"forecasting QA:\"\n        self.task_prompt_events: str = \"time to event prediction:\"\n        self.task_prompt_custom: str = \" a custom task:\"\n        self.task_target_start: str = \"Task {task_nr} is \"\n        self.task_target_end: str = \"\"\n\n        # --- Overrides -----\n        self.decimal_precision = 2  # Number of decimal places to round values to, by default 2.\n        self.event_category_preamble_mapping_override = None\n        # Override for the event category preamble mapping (default is None).\n        # Structure is {&lt;event_category&gt;: &lt;preamble_string&gt;}\n\n        self.event_category_and_name_replace_override = None\n        # dict, optional\n        #    Override for the event category and name replace mapping (default is None).\n        #    Structure is {&lt;event_category&gt;: {\n        #        &lt;event_name&gt;: {\n        #            \"full_replacement_string\": &lt;full_replacement_string&gt;,\n        #            \"reverse_string_value\": &lt;reverse_string_value&gt;\n        #            }\n        #        }\n        #    }\n\n        self.always_keep_first_visit: bool = (\n            True  # Whether to always keep the first visit in the patient history, by default True.\n        )\n\n        # Seeds\n        self._seed = 768921  # I like both of these numbers\n        self._set_all_seeds(self._seed)\n\n        # Token budgets\n        self.nr_tokens_budget_padding: int = 200  # Might need to be set to 500 for pretrain\n\n        # Tokenizers for counting\n        self.tokenizer_to_use: str = \"microsoft/Phi-4-mini-instruct\"\n\n        # --- Processing of constant ---\n        self.constant_columns_to_use: list = [\n            \"race\",\n            \"gender\",\n            \"ethnicity\",\n            \"indication\",\n        ]  # Which columns to use from the constant data\n        self.constant_birthdate_column: str = None  # If set, use this column for age calculation\n        self.constant_birthdate_column_format: str = \"date\"  # Either \"date\" or \"age\"\n\n        # Used to backup event categories for event types if no variables are found\n        # e.g. progression -&gt; death\n        self.data_splitter_events_backup_category_mapping = {\n            \"progression\": \"death\",\n        }\n\n    def set_delta_time_unit(self, unit: str, unit_sing=None):\n        \"\"\"\n        Set the time unit for delta time representation in text conversion. Possible to set either\n        \"days\" (and \"day(s)\") or \"weeks\" (and \"week(s)\"). Optionally, a singular form can be provided\n        for use in specific prompts. If not provided, the plural form will be used.\n        \"\"\"\n        assert unit in (\"days\", \"weeks\", \"day(s)\", \"week(s)\"), \"unit must be either 'days' or 'weeks'\"\n        assert unit_sing in (None, \"day\", \"week\"), \"unit_sing must be either None, 'day' or 'week'\"\n        self.delta_time_unit = unit\n        if unit_sing is None:\n            unit_sing = unit\n\n        self.event_day_text = self._event_day_text_template.format(unit=unit)\n        self.forecasting_fval_prompt_start = self._forecasting_fval_prompt_start_template.format(unit=unit_sing)\n        self.forecasting_prompt_var_time = self._forecasting_prompt_var_time_template.format(unit=unit)\n        self.forecasting_tte_prompt_mid = self._forecasting_tte_prompt_mid_template.format(unit=unit)\n        self.qa_prompt_start = self._qa_prompt_start_template.format(unit=unit_sing)\n\n    @property\n    def seed(self) -&gt; int:\n        \"\"\"Get the current seed value.\"\"\"\n        return self._seed\n\n    @seed.setter\n    def seed(self, value: int):\n        \"\"\"Set the seed value and update all random seeds (numpy, pandas, random).\"\"\"\n        self._seed = value\n        self._set_all_seeds(value)\n\n    def _set_all_seeds(self, seed: int):\n        \"\"\"Set seeds for numpy, pandas, and random modules.\"\"\"\n        np.random.seed(seed)\n        random.seed(seed)\n</code></pre>"},{"location":"reference/common/config/#twinweaver.common.config.Config-attributes","title":"Attributes","text":""},{"location":"reference/common/config/#twinweaver.common.config.Config.seed","title":"seed  <code>property</code> <code>writable</code>","text":"<pre><code>seed\n</code></pre> <p>Get the current seed value.</p>"},{"location":"reference/common/config/#twinweaver.common.config.Config-functions","title":"Functions","text":""},{"location":"reference/common/config/#twinweaver.common.config.Config.set_delta_time_unit","title":"set_delta_time_unit","text":"<pre><code>set_delta_time_unit(unit, unit_sing=None)\n</code></pre> <p>Set the time unit for delta time representation in text conversion. Possible to set either \"days\" (and \"day(s)\") or \"weeks\" (and \"week(s)\"). Optionally, a singular form can be provided for use in specific prompts. If not provided, the plural form will be used.</p> Source code in <code>twinweaver/common/config.py</code> <pre><code>def set_delta_time_unit(self, unit: str, unit_sing=None):\n    \"\"\"\n    Set the time unit for delta time representation in text conversion. Possible to set either\n    \"days\" (and \"day(s)\") or \"weeks\" (and \"week(s)\"). Optionally, a singular form can be provided\n    for use in specific prompts. If not provided, the plural form will be used.\n    \"\"\"\n    assert unit in (\"days\", \"weeks\", \"day(s)\", \"week(s)\"), \"unit must be either 'days' or 'weeks'\"\n    assert unit_sing in (None, \"day\", \"week\"), \"unit_sing must be either None, 'day' or 'week'\"\n    self.delta_time_unit = unit\n    if unit_sing is None:\n        unit_sing = unit\n\n    self.event_day_text = self._event_day_text_template.format(unit=unit)\n    self.forecasting_fval_prompt_start = self._forecasting_fval_prompt_start_template.format(unit=unit_sing)\n    self.forecasting_prompt_var_time = self._forecasting_prompt_var_time_template.format(unit=unit)\n    self.forecasting_tte_prompt_mid = self._forecasting_tte_prompt_mid_template.format(unit=unit)\n    self.qa_prompt_start = self._qa_prompt_start_template.format(unit=unit_sing)\n</code></pre>"},{"location":"reference/common/converter_base/","title":"Converter Base","text":""},{"location":"reference/common/converter_base/#twinweaver.common.converter_base","title":"twinweaver.common.converter_base","text":""},{"location":"reference/common/converter_base/#twinweaver.common.converter_base-classes","title":"Classes","text":""},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.ConverterBase","title":"ConverterBase","text":"<p>Base class for converting structured patient event data into textual representations and vice-versa, using manually defined templates and logic.</p> <p>This class provides the core functionalities for preprocessing constant (demographic) and event data, generating text strings based on configured templates, parsing these strings back into structured data, managing token budgets for text length control, and comparing event datasets. It relies heavily on a <code>Config</code> object for various settings like column names, text snippets, and processing flags.</p> <p>Derived classes are expected to implement specific methods like tokenizer initialization, as well as the abstract methods <code>forward_conversion_inference</code>, <code>generate_target_manual</code>, and <code>aggregate_multiple_responses</code>.</p> Source code in <code>twinweaver/common/converter_base.py</code> <pre><code>class ConverterBase:\n    \"\"\"\n    Base class for converting structured patient event data into textual representations\n    and vice-versa, using manually defined templates and logic.\n\n    This class provides the core functionalities for preprocessing constant (demographic)\n    and event data, generating text strings based on configured templates, parsing these\n    strings back into structured data, managing token budgets for text length control,\n    and comparing event datasets. It relies heavily on a `Config` object for various\n    settings like column names, text snippets, and processing flags.\n\n    Derived classes are expected to implement specific methods like tokenizer initialization,\n    as well as the abstract methods `forward_conversion_inference`, `generate_target_manual`,\n    and `aggregate_multiple_responses`.\n    \"\"\"\n\n    def __init__(self, config: Config) -&gt; None:\n        \"\"\"\n        Initialize the ConverterBase class with configuration settings.\n\n        Sets up internal attributes based on the provided `Config` object, including\n        text templates for conversion, mappings for special event handling (like death or drugs),\n        precision for rounding, and the random seed. Initializes tokenizer-related\n        attributes (`tokenizer`, `nr_tokens_budget_padding`, `always_keep_first_visit`) to None,\n        expecting them to be set by a derived class or later configuration.\n\n        Parameters\n        ----------\n        config : Config\n            A configuration object containing settings like column names, text templates,\n            event category mappings, decimal precision, seed, and paths/flags.\n        \"\"\"\n        # Set up config\n        self.config = config\n\n        # Set decimal precision\n        self.decimal_precision = self.config.decimal_precision\n\n        # Setup all text passages using config defaults if overrides are None\n        self.preamble_text = self.config.preamble_text\n        self.constant_text = self.config.constant_text\n        self.first_day_text = self.config.first_day_text\n        self.genetic_skip_text = self.config.genetic_skip_text_value\n        self.event_day_preamble = self.config.event_day_preamble\n        self.event_day_text = self.config.event_day_text\n        self.post_event_text = self.config.post_event_text\n\n        # Setup special mappings using config defaults if overrides are None\n        self.event_category_preamble_mapping = (\n            self.config.event_category_preamble_mapping_override\n            if self.config.event_category_preamble_mapping_override is not None\n            # Using default 'drug'\n            else {\"drug\": \"drug\"}\n        )\n\n        # Use config constant for 'death' category in the default replacement mapping\n        self.event_category_and_name_replace = (\n            self.config.event_category_and_name_replace_override\n            if self.config.event_category_and_name_replace_override is not None\n            else {\n                self.config.event_category_death: {  # Use config constant\n                    \"death\": {  # Assuming 'death' is the event_name associated with this category\n                        \"full_replacement_string\": \"death\",\n                        \"reverse_string_value\": \"death\",\n                    }\n                }\n            }\n        )\n\n        # These should instantiated from derived class\n        self.tokenizer = None\n        self.nr_tokens_budget_padding = None\n        self.always_keep_first_visit = None\n\n        # Handles time division depending on config\n        if self.config.delta_time_unit in [\"weeks\", \"week(s)\"]:\n            self._time_divisor = 7.0\n        elif self.config.delta_time_unit in [\"days\", \"day(s)\"]:\n            self._time_divisor = 1.0\n        else:\n            self._time_divisor = None\n            raise ValueError(f\"Unsupported delta_time_unit: {self.config.delta_time_unit}\")\n\n    def _preprocess_constant_date(\n        self,\n        events: pd.DataFrame,\n        constant: pd.DataFrame,\n        constant_description: pd.DataFrame,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Preprocesses static patient data (e.g., demographics) for text conversion.\n\n        Applies specific preprocessing steps based on configuration settings.\n\n        Parameters\n        ----------\n        events : pd.DataFrame\n            DataFrame containing the patient's time-series event data\n        constant : pd.DataFrame\n            DataFrame containing the raw static patient data (e.g., birth year, race, gender).\n        constant_description : pd.DataFrame\n            DataFrame describing the variables in the `constant` DataFrame (e.g., variable name and a\n            comment/description).\n\n        Returns\n        -------\n        Tuple[pd.DataFrame, pd.DataFrame]\n            A tuple containing:\n            - The processed `constant` DataFrame, potentially with calculated age and filtered/modified columns.\n            - The potentially updated `constant_description` DataFrame.\n        \"\"\"\n\n        # Extracting corresponding variables\n        constant = constant.copy()[self.config.constant_columns_to_use]\n\n        # Override birthdate (or variations of it) to age\n        if self.config.constant_birthdate_column is not None:\n            if self.config.constant_birthdate_column_format == \"age\":\n                # Handle integer ages - just format them as \"X years\"\n                constant[self.config.constant_birthdate_column] = (\n                    constant[self.config.constant_birthdate_column].astype(int).astype(str) + \" years\"\n                )\n                print(f\"Using provided ages in {self.config.constant_birthdate_column} as age format\")\n            else:\n                # Check if the column contains integer ages (not birthdates)\n                try:\n                    if pd.api.types.is_numeric_dtype(constant[self.config.constant_birthdate_column]):\n                        # Convert year to date (1st of January of that year)\n                        constant[self.config.constant_birthdate_column] = pd.to_datetime(\n                            constant[self.config.constant_birthdate_column].astype(int).astype(str) + \"-01-01\"\n                        )\n                        print(f\"Converted integer ages in {self.config.constant_birthdate_column} to age format\")\n\n                    # Try converting the column to datetime if it is not already, if doesn't work then just keep it\n                    elif not pd.api.types.is_datetime64_any_dtype(constant[self.config.constant_birthdate_column]):\n                        constant[self.config.constant_birthdate_column] = pd.to_datetime(\n                            constant[self.config.constant_birthdate_column]\n                        )\n                except Exception as e:\n                    print(\n                        f\"Warning: Could not convert {self.config.constant_birthdate_column} to datetime. \\n\"\n                        f\"Keeping original values. Error: {e}\"\n                    )\n                    raise e\n\n                # Calculate age in years from the first event date\n                constant[self.config.constant_birthdate_column] = (\n                    events[self.config.date_col].min() - constant[self.config.constant_birthdate_column]\n                ).dt.days // 365\n                constant[self.config.constant_birthdate_column] = (\n                    constant[self.config.constant_birthdate_column].astype(int).astype(str) + \" years\"\n                )\n\n        # Assuming constant_description is stable (copying for future use)\n        constant_description = constant_description.copy()\n\n        #: return constant, constant_description\n        return constant, constant_description\n\n    def _get_constant_string(self, constant: pd.DataFrame, constant_description: pd.DataFrame) -&gt; str:\n        \"\"\"\n        Generates a formatted string representation of the constant (demographic) patient data.\n\n        Removes the columns that are na for that patient/subject.\n        Iterates through the columns of the preprocessed `constant` DataFrame, retrieves the\n        corresponding description from `constant_description`, and constructs a string\n        listing each piece of information (e.g., \"\\t'age of patient' is '65 years',\\n\").\n        Prepends the `self.constant_text` preamble and formats the final string.\n\n        Parameters\n        ----------\n        constant : pd.DataFrame\n            Preprocessed DataFrame containing the constant patient data (typically one row).\n        constant_description : pd.DataFrame\n            DataFrame containing descriptions for the variables in the `constant` DataFrame.\n            Must contain \"variable\" and \"comment\" columns.\n\n        Returns\n        -------\n        str\n            A formatted string summarizing the patient's constant data.\n        \"\"\"\n\n        # +: drop columns that are na for that patient/subject\n        constant = constant.dropna(axis=1, how=\"all\")\n\n        #: create string representation of constant\n        constant_string = self.constant_text\n\n        for col in constant.columns:\n            col_value = constant[col].iloc[0]\n            # Keeping hardcoded column names as they seem specific to this function's input structure\n            col_description = constant_description[constant_description[\"variable\"] == col][\"comment\"].iloc[0]\n            constant_string += f\"\\t{col_description} is {col_value},\\n\"\n\n        # Replace last , with .\n        constant_string = constant_string[:-2] + \".\\n\"\n\n        return constant_string\n\n    def _preprocess_events(self, events: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs initial preprocessing on the time-series event data.\n\n        Sorts the events DataFrame primarily by date, then by event category and name, according\n        to the column names specified in the `config`. It also applies the `round_and_strip`\n        function to the event value column (`config.event_value_col`) using the configured\n        `self.decimal_precision` to standardize numeric representations.\n\n        Parameters\n        ----------\n        events : pd.DataFrame\n            DataFrame containing the raw time-series event data for a patient.\n\n        Returns\n        -------\n        pd.DataFrame\n            The preprocessed events DataFrame, sorted and with rounded/stripped numeric values.\n        \"\"\"\n\n        # First sort using config constants\n        events = events.sort_values(\n            by=[\n                self.config.date_col,\n                self.config.event_category_col,\n                self.config.event_name_col,\n            ]\n        )\n\n        # Use config constant for event_value_col\n        events[self.config.event_value_col] = events[self.config.event_value_col].apply(\n            round_and_strip, args=(self.decimal_precision,)\n        )\n\n        return events\n\n    def _get_event_string(\n        self,\n        events: pd.DataFrame,\n        events_delta_0: int = 0,\n        use_accumulative_dates: bool = False,\n        add_first_day_preamble: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Converts preprocessed time-series event data into a structured textual representation.\n\n        Groups events by visit date, calculates the time delta (in days or weeks, according to\n        `self.config.delta_time_unit`)\n        between consecutive visits, and formats the output string visit by visit. Uses templates from the `config`\n        object (`first_day_text`, `event_day_preamble`, `event_day_text`, `post_event_text`) to structure\n        the text. Handles genetic events separately, enclosing them in tags (`config.genetic_tag_opening`,\n        `config.genetic_tag_closing`) and potentially skipping the value if it matches\n        `config.genetic_skip_text_value`. Applies specific formatting based on event category mappings\n        and replacement overrides defined in the `config`.\n\n        Parameters\n        ----------\n        events : pd.DataFrame\n            Preprocessed DataFrame containing the patient's event data, sorted by date.\n        events_delta_0 : int, optional\n            The initial delta value (in days or weeks) to assume for the first visit date, by default 0.\n        use_accumulative_dates : bool, optional\n            If True, the reported delta for each visit will be the cumulative days or weeks since the\n            very first visit, otherwise it's days or weeks since the *previous* visit, by default False.\n        add_first_day_preamble : bool, optional\n            If True, uses the special `self.first_day_text` preamble for the first visit,\n            otherwise treats it like any other subsequent visit, by default True.\n\n        Returns\n        -------\n        str\n            A formatted string representing the patient's timeline of events.\n        \"\"\"\n\n        #: sort by date using config constant\n        events = events.sort_values(self.config.date_col).reset_index(drop=True)\n\n        #: for every visit get delta to previous in days or weeks, starting with 0 using config constant\n        events_delta = events[self.config.date_col].diff().dt.days / self._time_divisor\n        events_delta[0] = events_delta_0\n        events[\"delta\"] = events_delta\n\n        #: get all unique pairs of dates as well as deltas, then sort by date using config constant\n        all_unique_dates = pd.concat([events.iloc[0:1], events[events[\"delta\"] &gt; 0]], axis=0, ignore_index=True)\n        all_unique_dates = all_unique_dates.drop_duplicates(subset=[self.config.date_col, \"delta\"])[\n            [self.config.date_col, \"delta\"]\n        ]\n        all_unique_dates = all_unique_dates.to_numpy().tolist()\n        all_unique_dates = sorted(all_unique_dates, key=lambda x: x[0])\n\n        #: in case of accumulative dates, accumulate\n        if use_accumulative_dates:\n            all_unique_dates = [\n                (date, sum([delta for _, delta in all_unique_dates[: idx + 1]]))\n                for idx, (date, _) in enumerate(all_unique_dates)\n            ]\n\n        #: setup string\n        events_string = \"\"\n\n        #: replace all \"-( -)*\" with \"-\" in event_descriptive_name and event_value using config constants\n        events[self.config.event_descriptive_name_col] = events[self.config.event_descriptive_name_col].str.replace(\n            r\"(\\s-\\s-)+\", \" - \", regex=True\n        )\n        events[self.config.event_value_col] = events[self.config.event_value_col].str.replace(\n            r\"(\\s-\\s-)+\", \" - \", regex=True\n        )\n\n        #: Go per event\n        for idx, (date, delta) in enumerate(all_unique_dates):\n            #: Get subset using config constant\n            all_events_curr_date = events[events[self.config.date_col] == date]\n\n            #: make alternative text for first event\n            if idx == 0 and add_first_day_preamble:\n                events_string += self.first_day_text\n            else:\n                #: add event text, and delta (if possible, round so 7.0 -&gt; 7, 7.1 -&gt; 7.1)\n                events_string += self.event_day_preamble\n                events_string += f\"{delta:.2f}\".rstrip(\"0\").rstrip(\".\")  # Max 2 post decimal\n                events_string += self.event_day_text\n\n            #: get genetic first using config constants\n            genetic_subset = all_events_curr_date[\n                all_events_curr_date[self.config.source_col] == self.config.source_genetic\n            ]\n\n            if genetic_subset.shape[0] &gt; 0:\n                #: enclose all genetic with &lt;genetic&gt; &lt;/genetic&gt; tags\n                events_string += \"\\t\" + self.config.genetic_tag_opening + \"\\n\"\n\n                #: sort within genetic alphabetically using config constants\n                genetic_subset = genetic_subset.sort_values(\n                    by=[self.config.event_category_col, self.config.event_name_col]\n                )\n\n                for _, row in genetic_subset.iterrows():\n                    #: convert genetic using event_descriptive_name, adding tab before &amp; new line after using config\n                    # constants\n                    event_descriptive_name = row[self.config.event_descriptive_name_col]\n                    event_value = row[self.config.event_value_col]\n\n                    # Skip event_value if it is the default (e.g. \"present\"), using self.genetic_skip_text (initialized\n                    #  from config)\n                    if event_value == self.genetic_skip_text:\n                        events_string += \"\\t\" + event_descriptive_name + \",\\n\"\n                    else:\n                        events_string += \"\\t\" + event_descriptive_name + \" is \" + event_value + \",\\n\"\n\n                #: enclose all genetic with &lt;genetic&gt; &lt;/genetic&gt; tags\n                events_string += \"\\t\" + self.config.genetic_tag_closing\n\n                # Add newline only if there are further events\n                if all_events_curr_date.shape[0] &gt; genetic_subset.shape[0]:\n                    events_string += \",\\n\"\n\n            #: sort rest alphabetically using config constants\n            # Keeping non genetic events\n            event_subset = all_events_curr_date[\n                all_events_curr_date[self.config.source_col] != self.config.source_genetic\n            ]\n            event_subset = event_subset.sort_values(by=[self.config.event_category_col, self.config.event_name_col])\n\n            #: convert rest using event_descriptive_name\n            for idx_inner, (_, row) in enumerate(\n                event_subset.iterrows()\n            ):  # Renamed inner loop index to avoid shadowing\n                #: add tab before &amp; new line after using config constants\n                event_descriptive_name = row[self.config.event_descriptive_name_col]\n                event_value = row[self.config.event_value_col]\n                event_category = row[self.config.event_category_col]\n                event_name = row[self.config.event_name_col]\n\n                #: add custom preamble for event_category\n                if event_category in self.event_category_preamble_mapping:\n                    event_descriptive_name = (\n                        self.event_category_preamble_mapping[event_category] + \" \" + event_descriptive_name\n                    )\n\n                # Convert value to lower case, so it is easier for tokenizer to use\n                event_value = event_value.lower()\n\n                # Add to event string\n                if (\n                    event_category in self.event_category_and_name_replace\n                    and event_name in self.event_category_and_name_replace[event_category]\n                ):\n                    # Allow override for special events for custom strings.\n                    # Need to be careful that they can also do reverse translation\n                    events_string += \"\\t\"\n                    events_string += self.event_category_and_name_replace[event_category][event_name][\n                        \"full_replacement_string\"\n                    ]\n\n                else:\n                    # Default of \"&lt;name&gt; is &lt;value&gt;\"\n                    events_string += \"\\t\" + event_descriptive_name + \" is \" + event_value\n\n                if idx_inner &lt; event_subset.shape[0] - 1:  # Use inner loop index\n                    events_string += \",\\n\"\n\n            #: add self.post_event_text\n            events_string += self.post_event_text\n\n        return events_string\n\n    def _extract_event_data(\n        self,\n        text: str,\n        unique_events: pd.DataFrame,\n        raw_events: pd.DataFrame = None,\n        init_date: datetime = None,\n        only_contains_events: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Parses a textual representation of patient history back into a structured event DataFrame.\n\n        Identifies event sections within the input `text` (either the whole text if\n        `only_contains_events` is True, or the part following `self.first_day_text`).\n        Splits the text into visits based on date delta markers (`self.event_day_preamble`,\n        `self.event_day_text`). Parses each visit's text using `_parse_visit` to extract\n        individual events, calculates their absolute dates based on the deltas and an initial\n        date (`init_date` derived from `raw_events` or provided directly), and reconstructs\n        the event timeline.\n\n        Requires either `raw_events` (to determine the minimum date) or `init_date` to be provided.\n\n        Parameters\n        ----------\n        text : str\n            The textual representation of the patient's event history.\n        unique_events : pd.DataFrame\n            A lookup DataFrame containing information about all possible unique events\n            (mapping descriptive names to categories and original names).\n        raw_events : pd.DataFrame, optional\n            The original raw event DataFrame for the patient, used to determine the `init_date`\n            if `init_date` is not provided directly.\n        init_date : datetime, optional\n            The explicit starting date for the first visit. Used if `raw_events` is None.\n        only_contains_events : bool, optional\n            If True, assumes the entire `text` consists of event descriptions without preamble\n            or constant data sections, by default False.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the events extracted from the text, with columns matching\n            the standard event structure (date, category, name, descriptive_name, value, source).\n\n        Raises\n        ------\n        AssertionError\n            If neither `raw_events` nor `init_date` is provided.\n        \"\"\"\n\n        # Setup up init date using config constant\n        assert raw_events is not None or init_date is not None, \"Either raw_events or init_date must be provided.\"\n        if raw_events is not None:\n            init_date = raw_events[self.config.date_col].min()\n\n        # Initialize an empty list to store event data\n        event_data = []\n\n        # Extract the section of the text that contains events\n        if only_contains_events:\n            event_section = text\n        else:\n            # Use self.first_day_text which is initialized from config\n            event_section = re.search(re.escape(self.first_day_text) + r\"(.*)\", text, re.DOTALL)\n            event_section = event_section.group(1).strip()\n\n        if event_section:\n            # Split the event section into individual visits\n            # Use self.event_day_preamble and self.event_day_text initialized from config\n            visits = re.split(\n                re.escape(self.event_day_preamble) + r\"(\\d+(\\.\\d+)?)\" + re.escape(self.event_day_text),\n                event_section,\n            )\n\n            # The first visit is special and doesn't have a delta\n            first_visit = visits[0].strip()\n            if first_visit:\n                new_events = self._parse_visit(first_visit, 0, unique_events, init_date)\n                event_data.extend(new_events)\n                # Use config constant for date column in the event data dictionary\n                last_visit_date = event_data[-1][self.config.date_col]\n            else:\n                last_visit_date = init_date\n\n            # Process subsequent visits\n            for i in range(1, len(visits), 3):\n                delta = float(visits[i])\n                visit_text = visits[i + 2].strip()\n                new_events = self._parse_visit(visit_text, delta, unique_events, last_visit_date)\n                event_data.extend(new_events)\n                # Use config constant for date column in the event data dictionary\n                last_visit_date = event_data[-1][self.config.date_col]\n\n        # Convert the event data to a DataFrame\n        event_df = pd.DataFrame(event_data)\n\n        return event_df\n\n    def _parse_visit(\n        self,\n        visit_text: str,\n        delta: float,\n        unique_events: pd.DataFrame,\n        last_visit_date,\n    ):\n        \"\"\"\n        Parses the text corresponding to a single visit into a list of event dictionaries.\n\n        Splits the `visit_text` into individual event lines based on \",\\n\". Handles the removal\n        of the trailing `self.post_event_text`. Detects and processes genetic events enclosed\n        in genetic tags (`config.genetic_tag_opening`, `config.genetic_tag_closing`) using\n        `_parse_genetic_event`. Parses standard events using `_parse_standard_event`.\n\n        Parameters\n        ----------\n        visit_text : str\n            The block of text describing the events that occurred during a single visit.\n        delta : float\n            The time delta (in days or weeks) from the previous visit (or `init_date` if the first visit).\n        unique_events : pd.DataFrame\n            Lookup DataFrame for unique event details.\n        last_visit_date : pd.Timestamp\n            The timestamp of the previous visit date (or `init_date` if the first visit).\n\n        Returns\n        -------\n        list[dict]\n            A list where each element is a dictionary representing a single parsed event,\n            containing keys like date, event category, name, value, etc.\n        \"\"\"\n\n        # Split the visit text into individual events\n        events = visit_text.split(\",\\n\")\n        new_events = []\n        genetic_mode = False\n\n        for idx, event in enumerate(events):\n            event = event.strip()\n\n            # If last line, then remove trailing \".\" using self.post_event_text initialized from config\n            if idx == len(events) - 1:\n                # Use self.post_event_text which is initialized from config\n                raw_post_event_text = self.post_event_text.replace(\"\\n\", \"\")\n                event = event[: -len(raw_post_event_text)]\n\n            if event:\n                # Check if the event is genetic\n                if event.startswith(self.config.genetic_tag_closing):\n                    genetic_mode = False\n\n                elif event.startswith(self.config.genetic_tag_opening) or genetic_mode:\n                    genetic_mode = True\n                    # Process out &lt;genetic&gt; if needed\n                    if event.startswith(self.config.genetic_tag_opening):\n                        event = event.split(\"\\n\")[1]\n\n                    # Pass \"unknown - genetic\" as category, consistent with original logic\n                    new_event = self._parse_genetic_event(event, delta, unique_events, last_visit_date)\n                    if new_event:  # Ensure event was parsed correctly\n                        new_events.append(new_event)\n                else:\n                    new_event = self._parse_standard_event(event, delta, unique_events, last_visit_date)\n                    if new_event:  # Ensure event was parsed correctly\n                        new_events.append(new_event)\n\n        return new_events\n\n    def _parse_genetic_event(\n        self,\n        genetic_event: str,\n        delta: float,\n        unique_events: pd.DataFrame,\n        last_visit_date,\n    ):\n        \"\"\"\n        Parses a single line of text representing a genetic event.\n\n        Extracts the descriptive name and value using `_extract_event_details`.\n        Assigns a fixed category \"unknown - genetic\" (based on original implementation logic).\n        Calculates the event date based on `last_visit_date` and `delta`. Uses\n        `_add_event_to_data` to structure the output dictionary.\n\n        Parameters\n        ----------\n        genetic_event : str\n            The text line describing a genetic event (e.g., \"Gene X Mutation is detected\").\n        delta : float\n            Time delta (days or weeks) from the previous visit.\n        unique_events : pd.DataFrame\n            Lookup DataFrame for unique event details.\n        last_visit_date : pd.Timestamp\n            Timestamp of the previous visit date.\n\n        Returns\n        -------\n        dict or None\n            A dictionary representing the parsed genetic event, or None if parsing fails.\n        \"\"\"\n\n        # Split the genetic event into individual lines (although usually just one per entry)\n        lines = genetic_event.split(\n            \",\\n\"\n        )  # Original split logic, may need review if multi-line genetics occur differently\n        for line in lines:\n            line = line.strip()\n\n            if line:\n                event_descriptive_name, event_value = self._extract_event_details(line)\n                # Use \"unknown - genetic\" as the category, consistent with original apparent logic\n                return self._add_event_to_data(\n                    event_descriptive_name,\n                    event_value,\n                    delta,\n                    \"unknown - genetic\",\n                    unique_events,\n                    last_visit_date,\n                )\n        return None  # Return None if no valid line found\n\n    def _parse_standard_event(self, event: str, delta: float, unique_events: pd.DataFrame, last_visit_date):\n        \"\"\"\n        Parses a single line of text representing a standard (non-genetic) event.\n\n        Extracts the descriptive name and value using `_extract_event_details`. Determines the\n        event category by looking up the descriptive name in `unique_events` using\n        `_get_event_category`. Calculates the event date based on `last_visit_date` and `delta`.\n        Uses `_add_event_to_data` to structure the output dictionary.\n\n        Parameters\n        ----------\n        event : str\n            The text line describing a standard event (e.g., \"Hemoglobin is 12.5\").\n        delta : float\n            Time delta (days or weeks) from the previous visit.\n        unique_events : pd.DataFrame\n            Lookup DataFrame for unique event details.\n        last_visit_date : pd.Timestamp\n            Timestamp of the previous visit date.\n\n        Returns\n        -------\n        dict or None\n            A dictionary representing the parsed standard event, or None if parsing fails\n            (e.g., `_extract_event_details` returns None).\n        \"\"\"\n\n        event_descriptive_name, event_value = self._extract_event_details(event)\n        if event_descriptive_name is None:  # Check if extraction failed\n            return None\n        event_category = self._get_event_category(event_descriptive_name, unique_events)\n        return self._add_event_to_data(\n            event_descriptive_name,\n            event_value,\n            delta,\n            event_category,\n            unique_events,\n            last_visit_date,\n        )\n\n    def _extract_event_details(self, event: str) -&gt; Tuple[str, str]:\n        \"\"\"\n        Extracts the descriptive name and value from a single event string.\n\n        Handles different event string formats:\n        1. Checks if the string matches a predefined `full_replacement_string` from the\n           `self.event_category_and_name_replace` override mapping. If so, returns the\n           corresponding original event name (as descriptive name) and `reverse_string_value`.\n        2. If the string contains \" is \", splits it into descriptive name and value.\n        3. Otherwise, assumes the entire string is the descriptive name and assigns\n           `self.genetic_skip_text` (e.g., \"present\") as the value.\n        Removes any category-specific preambles (defined in `self.event_category_preamble_mapping`)\n        from the beginning of the extracted descriptive name.\n\n        Parameters\n        ----------\n        event : str\n            The textual representation of a single event line.\n\n        Returns\n        -------\n        Tuple[str, str] or Tuple[None, None]\n            A tuple containing the extracted (and cleaned) event descriptive name and event value.\n            Returns (None, None) if parsing fails (e.g., due to unexpected format).\n        \"\"\"\n\n        # Check if in manual override:\n        all_manual_overrides = []\n        for cat in self.event_category_and_name_replace:\n            for event_name in self.event_category_and_name_replace[cat]:\n                replace_info = self.event_category_and_name_replace[cat][event_name]\n                full_replacement = replace_info[\"full_replacement_string\"]\n                reverse_value = replace_info[\"reverse_string_value\"]\n                # Store descriptive name (event_name) associated with the full replacement\n                override_tuple = (cat, event_name, full_replacement, reverse_value)\n                all_manual_overrides.append(override_tuple)\n\n        # Find if the event text matches a full replacement string\n        matched_override = next((ov for ov in all_manual_overrides if event == ov[2]), None)\n\n        if matched_override:\n            # Extract the original descriptive name and its associated reverse value\n            event_descriptive_name = matched_override[1]  # The event_name is the descriptive name here\n            event_value = matched_override[3]\n        elif \" is \" in event:\n            try:\n                event_descriptive_name, event_value = event.split(\" is \", 1)\n            except ValueError:\n                print(f\"Warning: Could not parse event string '{event}' using ' is ' split.\")\n                return None, None  # Indicate parsing failure\n        else:\n            # Assume it's an event without a value (like genetic 'present')\n            event_descriptive_name = event\n            # Use self.genetic_skip_text which is initialized from config\n            event_value = self.genetic_skip_text\n\n        # Remove any preamble if possible\n        for category, preamble in self.event_category_preamble_mapping.items():\n            # Check if preamble exists at the beginning of the descriptive name\n            if event_descriptive_name and event_descriptive_name.startswith(preamble + \" \"):\n                event_descriptive_name = event_descriptive_name[len(preamble) :].strip()\n\n        return event_descriptive_name.strip(), event_value.strip()\n\n    def _get_event_category(self, event_descriptive_name: str, unique_events: pd.DataFrame) -&gt; str:\n        \"\"\"\n        Determines the event category corresponding to a given descriptive name.\n\n        Looks up the `event_descriptive_name` in the `unique_events` DataFrame (using the\n        `config.event_descriptive_name_col`). If found, returns the associated category\n        (`config.event_category_col`). If not found directly, it checks if the name matches\n        an event name defined within the `self.event_category_and_name_replace` override;\n        if so, it returns the corresponding category key. If still not found, returns \"unknown\".\n\n        Parameters\n        ----------\n        event_descriptive_name : str\n            The descriptive name of the event (after potential preamble removal).\n        unique_events : pd.DataFrame\n            Lookup DataFrame mapping descriptive names to categories and original names.\n\n        Returns\n        -------\n        str\n            The determined event category string (e.g., \"lab\", \"diagnosis\", \"lot\", \"unknown\").\n        \"\"\"\n\n        # Note: Preamble removal is done in _extract_event_details now.\n\n        # Try matching using config constant for descriptive name column\n        matches_in_unique_events = unique_events[\n            unique_events[self.config.event_descriptive_name_col] == event_descriptive_name\n        ]\n\n        if matches_in_unique_events.shape[0] == 0:\n            # Check if it was a manually replaced event (like death) where descriptive name might be the event_name\n            manual_match = None\n            for cat, name_map in self.event_category_and_name_replace.items():\n                if event_descriptive_name in name_map:\n                    manual_match = cat\n                    break\n            if manual_match:\n                return manual_match\n            else:\n                # print(f\"Warning: Event descriptive name '{event_descriptive_name}' not found in unique events or\n                # manual overrides.\")\n                return \"unknown\"  # Default if not found\n        else:\n            # Use config constant for category column\n            event_category = matches_in_unique_events[self.config.event_category_col].iloc[0]\n            return event_category\n\n    def _add_event_to_data(\n        self,\n        event_descriptive_name: str,\n        event_value: str,\n        delta: float,\n        event_category: str,\n        unique_events: pd.DataFrame,\n        prev_date: pd.Timestamp,\n    ):  # Changed type hint from pd.DatetimeIndex to pd.Timestamp\n        \"\"\"\n        Constructs a dictionary representing a single event record with standardized keys.\n\n        Calculates the event date by adding the `delta` (in days, rounded) to the `prev_date`.\n        Looks up the original `event_name` in `unique_events` based on the `event_descriptive_name`,\n        falling back to the descriptive name itself or checking manual overrides if not found.\n        Determines the `source` based on whether the `event_category` is \"unknown - genetic\"\n        (source = `config.source_genetic`) or not (source = \"events\"). Populates a dictionary\n        using column names defined in the `config` object (`date_col`, `event_category_col`, etc.).\n\n        Parameters\n        ----------\n        event_descriptive_name : str\n            The descriptive name of the event.\n        event_value : str\n            The value associated with the event.\n        delta : float\n            Time delta (in days or weeks) from the previous visit date.\n        event_category : str\n            The category assigned to the event.\n        unique_events : pd.DataFrame\n            Lookup DataFrame for unique event details.\n        prev_date : pd.Timestamp\n            The timestamp of the previous visit date (or `init_date`).\n\n        Returns\n        -------\n        dict\n            A dictionary containing the structured event data with keys corresponding to\n            the configured standard column names (e.g., `config.date_col`, `config.event_name_col`).\n        \"\"\"\n\n        # Get event name, falling back to using the event_descriptive_name as event_name\n        # Use config constant for descriptive name column\n        unique_events_data_matched = unique_events[\n            unique_events[self.config.event_descriptive_name_col] == event_descriptive_name\n        ]\n\n        if unique_events_data_matched.shape[0] == 0:\n            # Check manual overrides again for event_name mapping\n            event_name = event_descriptive_name  # Default fallback\n            for cat, name_map in self.event_category_and_name_replace.items():\n                if cat == event_category and event_descriptive_name in name_map:\n                    # This assumes the key in name_map is the desired event_name\n                    event_name = event_descriptive_name\n                    break\n        else:\n            # Use config constant for event name column\n            event_name = unique_events_data_matched[self.config.event_name_col].iloc[0]\n\n        # Determine source: Use config constant for genetic source. Keep 'events' literal for other source.\n        # The logic `event_category == \"unknown - genetic\"` seems more correct based on _parse_genetic_event\n        source_value = self.config.source_genetic if event_category == \"unknown - genetic\" else \"events\"\n\n        new_event = {\n            self.config.date_col: prev_date + pd.to_timedelta(round(delta * self._time_divisor), unit=\"D\"),\n            self.config.event_category_col: event_category,\n            self.config.event_name_col: event_name,\n            self.config.event_descriptive_name_col: event_descriptive_name,\n            self.config.event_value_col: event_value,\n            self.config.source_col: source_value,\n        }\n        return new_event\n\n    def _estimate_budget_per_variable(self, events: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Estimates the number of tokens required for each event row in a DataFrame.\n\n        Requires `self.tokenizer` to be set. Calculates the tokens for the descriptive name\n        and value separately. Estimates base tokens needed for date preambles (`first_day_text`,\n        `event_day_text`) and per-line structural tokens (\" is \\t ,\\n\"). Distributes the\n        base date preamble tokens across all lines and adds them to the per-line estimate.\n        Adds columns \"nr_tokens_descriptive\", \"nr_tokens_value\", and \"nr_tokens_total\"\n        to the DataFrame.\n\n        Parameters\n        ----------\n        events : pd.DataFrame\n            DataFrame containing event data for which to estimate token counts.\n\n        Returns\n        -------\n        pd.DataFrame\n            The input DataFrame with added columns for estimated token counts per component\n            and in total for each event row.\n\n        Raises\n        ------\n        ValueError\n            If `self.tokenizer` has not been initialized.\n        \"\"\"\n\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer must be set before estimating budget.\")\n\n        def get_nr_tokens(curr_string):\n            return len(self.tokenizer(str(curr_string))[\"input_ids\"])  # Ensure input is string\n\n        #: estimate base tokens\n        # Using self.first_day_text and self.event_day_text initialized from config\n        nr_tokens_date_first = get_nr_tokens(self.first_day_text)\n        nr_tokens_date_all = get_nr_tokens(self.event_day_text) * len(events[self.config.date_col].unique())\n        nr_tokens_base = nr_tokens_date_first + nr_tokens_date_all\n\n        #: estimated tokens per line\n        base_tokens_per_line = get_nr_tokens(\" is \\t ,\\n\")  # Base structure tokens\n        tokens_per_line = (\n            base_tokens_per_line + (nr_tokens_base / len(events)) if len(events) &gt; 0 else base_tokens_per_line\n        )\n\n        #: apply tokenization to each line using config constants\n        events = events.copy()\n        events[\"nr_tokens_descriptive\"] = events[self.config.event_descriptive_name_col].apply(get_nr_tokens)\n        events[\"nr_tokens_value\"] = events[self.config.event_value_col].apply(get_nr_tokens)\n        events[\"nr_tokens_total\"] = events[\"nr_tokens_descriptive\"] + events[\"nr_tokens_value\"] + tokens_per_line\n\n        return events\n\n    def _get_all_most_recent_events_within_budget(self, events: pd.DataFrame, budget_total: int) -&gt; pd.DataFrame:\n        \"\"\"\n        Selects the most recent events that fit within a specified total token budget.\n\n        Requires `self.tokenizer` and `self.always_keep_first_visit` to be set.\n        Estimates the token budget per event row using `_estimate_budget_per_variable`.\n        Sorts events by date descending (most recent first). If `self.always_keep_first_visit`\n        is True, it prioritizes keeping events from the very first visit (earliest date)\n        by moving them to the top of the list before budget calculation.\n        Calculates the cumulative token sum and selects rows where the cumulative sum\n        is within `budget_total`.\n\n        Parameters\n        ----------\n        events : pd.DataFrame\n            DataFrame containing event data, typically preprocessed and sorted.\n        budget_total : int\n            The maximum total number of tokens allowed for the selected events' textual representation.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the subset of events that fit within the budget,\n            resorted chronologically (ascending).\n            If `always_keep_first_visit` is True, it tries to include the first visit events\n            even if they exceed the budget slightly when combined with `other_events`.\n\n        Raises\n        ------\n        ValueError\n            If `self.tokenizer` or `self.always_keep_first_visit` has not been set.\n        \"\"\"\n\n        # Check that we have correct things\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer must be set before calculating budget.\")\n        if self.always_keep_first_visit is None:\n            raise ValueError(\"always_keep_first_visit must be set.\")\n\n        #: estimate budget per row\n        events = self._estimate_budget_per_variable(events)\n\n        #: sort by date descending using config constant\n        events = events.sort_values(self.config.date_col, ascending=False)\n\n        if self.always_keep_first_visit:\n            if not events.empty:  # Check if dataframe is not empty\n                #: make all events with first visit date be moved to the beginning of the dataframe using config\n                # constant\n                first_date = events[self.config.date_col].min()\n                first_date_events = events[events[self.config.date_col] == first_date]\n                other_events = events[events[self.config.date_col] != first_date]\n                # Keep original index for potential debugging, reset later if needed\n                events = pd.concat([first_date_events, other_events], ignore_index=False)  # Keep original index\n\n        # Do calculation using cumulative sum\n        events[\"cumsum\"] = events[\"nr_tokens_total\"].cumsum()\n\n        # Do actual selection based on budget\n        events_within_budget = events[events[\"cumsum\"] &lt;= budget_total]\n\n        # If always_keep_first_visit, ensure first visit events are kept even if budget is tight\n        if self.always_keep_first_visit and not events.empty:\n            first_date = events[self.config.date_col].min()  # Get first date again\n            first_date_events_original = events[events[self.config.date_col] == first_date]\n            # Combine kept first visit events with other events within budget\n            other_events_within_budget = events_within_budget[events_within_budget[self.config.date_col] != first_date]\n            events_final = pd.concat([first_date_events_original, other_events_within_budget]).drop_duplicates()\n        else:\n            events_final = events_within_budget\n\n        # Sort final selection by date ascending using config constant\n        events_final = events_final.sort_values(self.config.date_col)\n\n        #: drop nr tokens columns\n        events_final = events_final.drop(\n            columns=[\n                \"nr_tokens_descriptive\",\n                \"nr_tokens_value\",\n                \"nr_tokens_total\",\n                \"cumsum\",\n            ]\n        )\n\n        #: return events\n        return events_final.reset_index(drop=True)  # Reset index for clean output\n\n    def _generate_summarized_row_string(self, input_event_data, combined_target_meta: dict) -&gt; str:\n        \"\"\"\n        Creates a summary string containing the most recent genetic, LoT, and target variable values.\n\n        Extracts the latest occurrences of genetic events and the most recent Line of Therapy (LoT)\n        start/name event from the `input_event_data`. Formats these using templates from the\n        `config` (`forecasting_prompt_summarized_genetic`, `forecasting_prompt_summarized_lot`).\n        If target variable information is present in `combined_target_meta[\"dates_per_variable\"]`,\n        it finds the last recorded value for each target variable in `input_event_data`,\n        sorts them alphabetically, and adds them to the string using the\n        `config.forecasting_prompt_summarized_start` template.\n\n        Parameters\n        ----------\n        input_event_data : pd.DataFrame\n            DataFrame containing the patient's event history (input side).\n        combined_target_meta : dict\n            A dictionary potentially containing target variable information under the key\n            \"dates_per_variable\" (mapping variable names to something, structure implies keys exist)\n            and optionally \"variable_name_mapping\" (mapping variable names to descriptive names).\n\n        Returns\n        -------\n        str\n            A formatted string summarizing the most recent key information for forecasting tasks.\n        \"\"\"\n\n        # start ret\n        ret_prompt = \"\"\n\n        #: add most recent genetic info using config constants\n        ret_prompt += self.config.forecasting_prompt_summarized_genetic  # Using config attribute\n\n        #: select only genetic info from input side using config constants\n        genetic_info = input_event_data[input_event_data[self.config.source_col] == self.config.source_genetic]\n\n        #: select for each genetic variable occurence the most recent value using config constants\n        genetic_info_processed = genetic_info.sort_values(self.config.date_col).drop_duplicates(\n            subset=[self.config.event_name_col], keep=\"last\"\n        )\n\n        #: call _get_event_string with genetic info\n        # Set add_first_day_preamble to False as this is a summary section\n        if genetic_info_processed.empty:\n            genetic_str = self.config.genetic_empty_text\n        else:\n            genetic_str = self._get_event_string(\n                genetic_info_processed,\n                use_accumulative_dates=False,\n                add_first_day_preamble=False,\n            )  # Don't add first day preamble here\n\n        #: Process the genetic string: remove potential date preamble and keep only event lines\n        genetic_str_lines = genetic_str.strip().split(\"\\n\")\n        processed_genetic_lines = []\n        # Skip lines related to date/delta preamble if present\n        for line in genetic_str_lines:\n            # Simple check: if line starts with a number followed by ' {unit} later', skip it. Adjust regex if needed.\n            if not re.match(\n                r\"^\\s*\\d+(\\.\\d+)?\\s+\" + re.escape(self.config.event_day_text.strip().split(\" \", 1)[1]),\n                line,\n            ):\n                processed_genetic_lines.append(line)\n\n        # Remove &lt;genetic&gt; tags if they are the only content on their lines\n        processed_genetic_lines = [\n            line\n            for line in processed_genetic_lines\n            if line.strip() not in [self.config.genetic_tag_opening, self.config.genetic_tag_closing]\n        ]\n\n        # Re-join and ensure proper indentation (assuming tabs are used in _get_event_string)\n        ret_prompt += \"\\n\" + \"\\n\".join(processed_genetic_lines) + \"\\n\"  # Add newlines for separation\n\n        #: add most recent LoT info using config constants\n        ret_prompt += self.config.forecasting_prompt_summarized_lot  # Using config attribute\n        lot_info = input_event_data[input_event_data[self.config.event_category_col] == self.config.event_category_lot]\n\n        # Ensure lot_info is sorted by date to correctly find the last one\n        lot_info = lot_info.sort_values(self.config.date_col)\n\n        # Create selections based on event name and event value using config constants\n        if self.config.lot_event_name is not None and self.config.event_value_lot_start is not None:\n            lot_selection_1 = lot_info[\n                lot_info[self.config.event_name_col] == self.config.lot_event_name\n            ]  # Using config attribute\n            lot_selection_2 = lot_info[\n                lot_info[self.config.event_value_col] == self.config.event_value_lot_start\n            ]  # Using config attribute\n        else:\n            # Just use all lot_info if no specific columns are defined\n            lot_selection_1 = lot_info\n            lot_selection_2 = pd.DataFrame()  # Empty DataFrame if no specific selection is made\n\n        # Get the most recent entries and their dates using config constant\n        most_recent_lot_1 = lot_selection_1.iloc[-1] if not lot_selection_1.empty else None\n        most_recent_lot_2 = lot_selection_2.iloc[-1] if not lot_selection_2.empty else None\n\n        date_1 = (\n            most_recent_lot_1[self.config.date_col] if most_recent_lot_1 is not None else pd.NaT\n        )  # Using config attribute, use NaT for comparison\n        date_2 = (\n            most_recent_lot_2[self.config.date_col] if most_recent_lot_2 is not None else pd.NaT\n        )  # Using config attribute, use NaT for comparison\n\n        # Determine which lot to use based on the dates\n        most_recent_lot = None\n        if pd.notna(date_1) and pd.notna(date_2):\n            most_recent_lot = (\n                most_recent_lot_2 if date_2 &gt;= date_1 else most_recent_lot_1\n            )  # Use &gt;= to prefer LoT start if dates are same\n        elif pd.notna(date_1):\n            most_recent_lot = most_recent_lot_1\n        elif pd.notna(date_2):\n            most_recent_lot = most_recent_lot_2\n\n        # Append the appropriate information to ret_prompt using config constants\n        if most_recent_lot is not None:\n            # Override\n            if self.config.lot_concatenate_descriptive_and_value:\n                # Use config constant for concatenation\n                ret_prompt += (\n                    \"\\t\"\n                    + most_recent_lot[self.config.event_descriptive_name_col]\n                    + self.config.lot_concatenate_string\n                    + most_recent_lot[self.config.event_value_col]\n                    + \"\\n\"\n                )\n            else:\n                # Prefer showing the LoT start event's descriptive name if it was chosen\n                if most_recent_lot is most_recent_lot_2:\n                    ret_prompt += (\n                        \"\\t\" + most_recent_lot[self.config.event_descriptive_name_col] + \"\\n\"\n                    )  # Using config attribute\n                else:  # Otherwise show the value associated with the line_name event\n                    ret_prompt += \"\\t\" + most_recent_lot[self.config.event_value_col] + \"\\n\"  # Using config attribute\n        else:\n            ret_prompt += \"\\tNo line of therapy start information available.\\n\"  # Adjusted message\n\n        #: if we have target vars, for every target variable, retrieve their last value in input\n        if \"dates_per_variable\" in combined_target_meta and combined_target_meta[\"dates_per_variable\"]:\n            last_vals = {}\n\n            # Ensure input data is sorted by date to get the actual last value\n            input_event_data_sorted = input_event_data.sort_values(self.config.date_col)\n\n            for target_var in combined_target_meta[\"dates_per_variable\"].keys():\n                # Use config constant for event name column\n                curr_var_data = input_event_data_sorted[\n                    input_event_data_sorted[self.config.event_name_col] == target_var\n                ]\n                if not curr_var_data.empty:\n                    # Use config constant for event value column\n                    last_value = curr_var_data[self.config.event_value_col].iloc[-1]\n                    last_value_rounded = round_and_strip(last_value, self.decimal_precision)\n                    last_vals[target_var] = last_value_rounded\n                # else: handle case where target variable isn't in input_event_data? Maybe add a placeholder?\n\n            #: sort alphabetically by variable name\n            sorted_last_vals = sorted(last_vals.items())\n\n            #: then transform into string\n            ret_prompt += self.config.forecasting_prompt_summarized_start  # Using config attribute\n\n            for variable, value in sorted_last_vals:\n                # Use descriptive name from mapping if available, else use variable name\n                var_descriptive_name = combined_target_meta.get(\"variable_name_mapping\", {}).get(variable, variable)\n                ret_prompt += \"\\t\" + var_descriptive_name + \" was \" + str(value) + \"\\n\"\n\n        #: return\n        return ret_prompt\n\n    def get_difference_in_event_dataframes(\n        self,\n        events_1: pd.DataFrame,\n        events_2: pd.DataFrame,\n        skip_genetic: bool = False,\n        skip_vals_list=None,\n    ):\n        \"\"\"\n        Compares two event DataFrames and identifies rows that are not identical based on key columns.\n\n        Compares `events_1` and `events_2` based on `date`, `event_name` (case-insensitive),\n        and `event_value` (case-insensitive). Optionally filters out genetic events (based on\n        category \"unknown - genetic\" or source `config.source_genetic`) and events whose\n        descriptive name contains any substring from `skip_vals_list` (case-insensitive) before\n        comparison. Uses a merge operation with an indicator to find rows present in one DataFrame\n        but not the other.\n\n        Parameters\n        ----------\n        events_1 : pd.DataFrame\n            The first event DataFrame for comparison.\n        events_2 : pd.DataFrame\n            The second event DataFrame for comparison.\n        skip_genetic : bool, optional\n            If True, genetic events are excluded from the comparison, by default False.\n        skip_vals_list : list, optional\n            A list of substrings. Events whose descriptive name contains any of these substrings\n            (case-insensitive) will be excluded from the comparison. Defaults to None.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the rows that differ between the two input DataFrames,\n            including a '_merge' column indicating whether the row was unique to 'left_only' (events_1)\n            or 'right_only' (events_2).\n        \"\"\"\n        if skip_vals_list is None:\n            skip_vals_list = []\n\n        # Make both to lower on event_name and event_value\n        events_1[self.config.event_name_col] = events_1[self.config.event_name_col].str.lower()\n        events_1[self.config.event_value_col] = events_1[self.config.event_value_col].str.lower()\n        events_2[self.config.event_name_col] = events_2[self.config.event_name_col].str.lower()\n        events_2[self.config.event_value_col] = events_2[self.config.event_value_col].str.lower()\n\n        # Skip genetic if needed\n        if skip_genetic:\n            events_1 = events_1[events_1[self.config.event_category_col] != \"unknown - genetic\"]\n            events_2 = events_2[events_2[self.config.event_category_col] != \"unknown - genetic\"]\n            if self.config.source_col in events_1:\n                events_1 = events_1[events_1[self.config.source_col] != self.config.source_genetic]\n            if self.config.source_col in events_2:\n                events_2 = events_2[events_2[self.config.source_col] != self.config.source_genetic]\n\n        # Skip vals list if needed\n        if len(skip_vals_list) &gt; 0:\n            pattern = \"|\".join(skip_vals_list)\n            events_1 = events_1[\n                ~events_1[self.config.event_descriptive_name_col].str.contains(pattern, case=False, na=False)\n            ]\n            events_2 = events_2[\n                ~events_2[self.config.event_descriptive_name_col].str.contains(pattern, case=False, na=False)\n            ]\n\n        # Only keep columns date, event_name and event_value\n        events_1 = events_1[\n            [\n                self.config.date_col,\n                self.config.event_name_col,\n                self.config.event_value_col,\n            ]\n        ]\n        events_2 = events_2[\n            [\n                self.config.date_col,\n                self.config.event_name_col,\n                self.config.event_value_col,\n            ]\n        ]\n\n        # Match then on date, event_name and event_value, and return which rows are not the same\n\n        # Merge the two DataFrames on date, event_name, and event_value with an indicator\n        merged_df = events_1.merge(\n            events_2,\n            on=[\n                self.config.date_col,\n                self.config.event_name_col,\n                self.config.event_value_col,\n            ],\n            how=\"outer\",\n            indicator=True,\n        )\n\n        # Filter rows that are not the same in both DataFrames\n        difference_df = merged_df[merged_df[\"_merge\"] != \"both\"]\n\n        return difference_df\n\n    def forward_conversion_inference(self):\n        \"\"\"\n        Performs the forward conversion process for inference.\n\n        This method is intended to be overridden by derived classes to implement\n        the specific logic for converting input data into the format required\n        for model inference, typically generating prompt strings.\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented in the derived class.\n        \"\"\"\n        raise NotImplementedError(\"forward_conversion_inference not implemented in base class.\")\n\n    def generate_target_manual(self):\n        \"\"\"\n        Manually generates target values from the data.\n\n        This method is intended to be overridden by derived classes to implement\n        logic for deriving target labels or values directly from the dataset,\n        bypassing model generation. This is often used for validation or\n        creating ground truth for evaluation.\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented in the derived class.\n        \"\"\"\n        raise NotImplementedError(\"generate_target_manual not implemented in base class.\")\n\n    def aggregate_multiple_responses(self):\n        \"\"\"\n        Aggregates multiple responses from the model.\n\n        This method is intended to be overridden by derived classes to implement\n        logic for combining multiple outputs (e.g., from sampling or beam search)\n        into a single final result or prediction.\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented in the derived class.\n        \"\"\"\n        raise NotImplementedError(\"aggregate_multiple_responses not implemented in base class.\")\n</code></pre>"},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.ConverterBase-functions","title":"Functions","text":""},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.ConverterBase.__init__","title":"__init__","text":"<pre><code>__init__(config)\n</code></pre> <p>Initialize the ConverterBase class with configuration settings.</p> <p>Sets up internal attributes based on the provided <code>Config</code> object, including text templates for conversion, mappings for special event handling (like death or drugs), precision for rounding, and the random seed. Initializes tokenizer-related attributes (<code>tokenizer</code>, <code>nr_tokens_budget_padding</code>, <code>always_keep_first_visit</code>) to None, expecting them to be set by a derived class or later configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>A configuration object containing settings like column names, text templates, event category mappings, decimal precision, seed, and paths/flags.</p> required Source code in <code>twinweaver/common/converter_base.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    \"\"\"\n    Initialize the ConverterBase class with configuration settings.\n\n    Sets up internal attributes based on the provided `Config` object, including\n    text templates for conversion, mappings for special event handling (like death or drugs),\n    precision for rounding, and the random seed. Initializes tokenizer-related\n    attributes (`tokenizer`, `nr_tokens_budget_padding`, `always_keep_first_visit`) to None,\n    expecting them to be set by a derived class or later configuration.\n\n    Parameters\n    ----------\n    config : Config\n        A configuration object containing settings like column names, text templates,\n        event category mappings, decimal precision, seed, and paths/flags.\n    \"\"\"\n    # Set up config\n    self.config = config\n\n    # Set decimal precision\n    self.decimal_precision = self.config.decimal_precision\n\n    # Setup all text passages using config defaults if overrides are None\n    self.preamble_text = self.config.preamble_text\n    self.constant_text = self.config.constant_text\n    self.first_day_text = self.config.first_day_text\n    self.genetic_skip_text = self.config.genetic_skip_text_value\n    self.event_day_preamble = self.config.event_day_preamble\n    self.event_day_text = self.config.event_day_text\n    self.post_event_text = self.config.post_event_text\n\n    # Setup special mappings using config defaults if overrides are None\n    self.event_category_preamble_mapping = (\n        self.config.event_category_preamble_mapping_override\n        if self.config.event_category_preamble_mapping_override is not None\n        # Using default 'drug'\n        else {\"drug\": \"drug\"}\n    )\n\n    # Use config constant for 'death' category in the default replacement mapping\n    self.event_category_and_name_replace = (\n        self.config.event_category_and_name_replace_override\n        if self.config.event_category_and_name_replace_override is not None\n        else {\n            self.config.event_category_death: {  # Use config constant\n                \"death\": {  # Assuming 'death' is the event_name associated with this category\n                    \"full_replacement_string\": \"death\",\n                    \"reverse_string_value\": \"death\",\n                }\n            }\n        }\n    )\n\n    # These should instantiated from derived class\n    self.tokenizer = None\n    self.nr_tokens_budget_padding = None\n    self.always_keep_first_visit = None\n\n    # Handles time division depending on config\n    if self.config.delta_time_unit in [\"weeks\", \"week(s)\"]:\n        self._time_divisor = 7.0\n    elif self.config.delta_time_unit in [\"days\", \"day(s)\"]:\n        self._time_divisor = 1.0\n    else:\n        self._time_divisor = None\n        raise ValueError(f\"Unsupported delta_time_unit: {self.config.delta_time_unit}\")\n</code></pre>"},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.ConverterBase.aggregate_multiple_responses","title":"aggregate_multiple_responses","text":"<pre><code>aggregate_multiple_responses()\n</code></pre> <p>Aggregates multiple responses from the model.</p> <p>This method is intended to be overridden by derived classes to implement logic for combining multiple outputs (e.g., from sampling or beam search) into a single final result or prediction.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented in the derived class.</p> Source code in <code>twinweaver/common/converter_base.py</code> <pre><code>def aggregate_multiple_responses(self):\n    \"\"\"\n    Aggregates multiple responses from the model.\n\n    This method is intended to be overridden by derived classes to implement\n    logic for combining multiple outputs (e.g., from sampling or beam search)\n    into a single final result or prediction.\n\n    Raises\n    ------\n    NotImplementedError\n        If not implemented in the derived class.\n    \"\"\"\n    raise NotImplementedError(\"aggregate_multiple_responses not implemented in base class.\")\n</code></pre>"},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.ConverterBase.forward_conversion_inference","title":"forward_conversion_inference","text":"<pre><code>forward_conversion_inference()\n</code></pre> <p>Performs the forward conversion process for inference.</p> <p>This method is intended to be overridden by derived classes to implement the specific logic for converting input data into the format required for model inference, typically generating prompt strings.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented in the derived class.</p> Source code in <code>twinweaver/common/converter_base.py</code> <pre><code>def forward_conversion_inference(self):\n    \"\"\"\n    Performs the forward conversion process for inference.\n\n    This method is intended to be overridden by derived classes to implement\n    the specific logic for converting input data into the format required\n    for model inference, typically generating prompt strings.\n\n    Raises\n    ------\n    NotImplementedError\n        If not implemented in the derived class.\n    \"\"\"\n    raise NotImplementedError(\"forward_conversion_inference not implemented in base class.\")\n</code></pre>"},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.ConverterBase.generate_target_manual","title":"generate_target_manual","text":"<pre><code>generate_target_manual()\n</code></pre> <p>Manually generates target values from the data.</p> <p>This method is intended to be overridden by derived classes to implement logic for deriving target labels or values directly from the dataset, bypassing model generation. This is often used for validation or creating ground truth for evaluation.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented in the derived class.</p> Source code in <code>twinweaver/common/converter_base.py</code> <pre><code>def generate_target_manual(self):\n    \"\"\"\n    Manually generates target values from the data.\n\n    This method is intended to be overridden by derived classes to implement\n    logic for deriving target labels or values directly from the dataset,\n    bypassing model generation. This is often used for validation or\n    creating ground truth for evaluation.\n\n    Raises\n    ------\n    NotImplementedError\n        If not implemented in the derived class.\n    \"\"\"\n    raise NotImplementedError(\"generate_target_manual not implemented in base class.\")\n</code></pre>"},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.ConverterBase.get_difference_in_event_dataframes","title":"get_difference_in_event_dataframes","text":"<pre><code>get_difference_in_event_dataframes(\n    events_1,\n    events_2,\n    skip_genetic=False,\n    skip_vals_list=None,\n)\n</code></pre> <p>Compares two event DataFrames and identifies rows that are not identical based on key columns.</p> <p>Compares <code>events_1</code> and <code>events_2</code> based on <code>date</code>, <code>event_name</code> (case-insensitive), and <code>event_value</code> (case-insensitive). Optionally filters out genetic events (based on category \"unknown - genetic\" or source <code>config.source_genetic</code>) and events whose descriptive name contains any substring from <code>skip_vals_list</code> (case-insensitive) before comparison. Uses a merge operation with an indicator to find rows present in one DataFrame but not the other.</p> <p>Parameters:</p> Name Type Description Default <code>events_1</code> <code>DataFrame</code> <p>The first event DataFrame for comparison.</p> required <code>events_2</code> <code>DataFrame</code> <p>The second event DataFrame for comparison.</p> required <code>skip_genetic</code> <code>bool</code> <p>If True, genetic events are excluded from the comparison, by default False.</p> <code>False</code> <code>skip_vals_list</code> <code>list</code> <p>A list of substrings. Events whose descriptive name contains any of these substrings (case-insensitive) will be excluded from the comparison. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the rows that differ between the two input DataFrames, including a '_merge' column indicating whether the row was unique to 'left_only' (events_1) or 'right_only' (events_2).</p> Source code in <code>twinweaver/common/converter_base.py</code> <pre><code>def get_difference_in_event_dataframes(\n    self,\n    events_1: pd.DataFrame,\n    events_2: pd.DataFrame,\n    skip_genetic: bool = False,\n    skip_vals_list=None,\n):\n    \"\"\"\n    Compares two event DataFrames and identifies rows that are not identical based on key columns.\n\n    Compares `events_1` and `events_2` based on `date`, `event_name` (case-insensitive),\n    and `event_value` (case-insensitive). Optionally filters out genetic events (based on\n    category \"unknown - genetic\" or source `config.source_genetic`) and events whose\n    descriptive name contains any substring from `skip_vals_list` (case-insensitive) before\n    comparison. Uses a merge operation with an indicator to find rows present in one DataFrame\n    but not the other.\n\n    Parameters\n    ----------\n    events_1 : pd.DataFrame\n        The first event DataFrame for comparison.\n    events_2 : pd.DataFrame\n        The second event DataFrame for comparison.\n    skip_genetic : bool, optional\n        If True, genetic events are excluded from the comparison, by default False.\n    skip_vals_list : list, optional\n        A list of substrings. Events whose descriptive name contains any of these substrings\n        (case-insensitive) will be excluded from the comparison. Defaults to None.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the rows that differ between the two input DataFrames,\n        including a '_merge' column indicating whether the row was unique to 'left_only' (events_1)\n        or 'right_only' (events_2).\n    \"\"\"\n    if skip_vals_list is None:\n        skip_vals_list = []\n\n    # Make both to lower on event_name and event_value\n    events_1[self.config.event_name_col] = events_1[self.config.event_name_col].str.lower()\n    events_1[self.config.event_value_col] = events_1[self.config.event_value_col].str.lower()\n    events_2[self.config.event_name_col] = events_2[self.config.event_name_col].str.lower()\n    events_2[self.config.event_value_col] = events_2[self.config.event_value_col].str.lower()\n\n    # Skip genetic if needed\n    if skip_genetic:\n        events_1 = events_1[events_1[self.config.event_category_col] != \"unknown - genetic\"]\n        events_2 = events_2[events_2[self.config.event_category_col] != \"unknown - genetic\"]\n        if self.config.source_col in events_1:\n            events_1 = events_1[events_1[self.config.source_col] != self.config.source_genetic]\n        if self.config.source_col in events_2:\n            events_2 = events_2[events_2[self.config.source_col] != self.config.source_genetic]\n\n    # Skip vals list if needed\n    if len(skip_vals_list) &gt; 0:\n        pattern = \"|\".join(skip_vals_list)\n        events_1 = events_1[\n            ~events_1[self.config.event_descriptive_name_col].str.contains(pattern, case=False, na=False)\n        ]\n        events_2 = events_2[\n            ~events_2[self.config.event_descriptive_name_col].str.contains(pattern, case=False, na=False)\n        ]\n\n    # Only keep columns date, event_name and event_value\n    events_1 = events_1[\n        [\n            self.config.date_col,\n            self.config.event_name_col,\n            self.config.event_value_col,\n        ]\n    ]\n    events_2 = events_2[\n        [\n            self.config.date_col,\n            self.config.event_name_col,\n            self.config.event_value_col,\n        ]\n    ]\n\n    # Match then on date, event_name and event_value, and return which rows are not the same\n\n    # Merge the two DataFrames on date, event_name, and event_value with an indicator\n    merged_df = events_1.merge(\n        events_2,\n        on=[\n            self.config.date_col,\n            self.config.event_name_col,\n            self.config.event_value_col,\n        ],\n        how=\"outer\",\n        indicator=True,\n    )\n\n    # Filter rows that are not the same in both DataFrames\n    difference_df = merged_df[merged_df[\"_merge\"] != \"both\"]\n\n    return difference_df\n</code></pre>"},{"location":"reference/common/converter_base/#twinweaver.common.converter_base-functions","title":"Functions","text":""},{"location":"reference/common/converter_base/#twinweaver.common.converter_base.round_and_strip","title":"round_and_strip","text":"<pre><code>round_and_strip(value, decimal_precision)\n</code></pre> <p>Formats a number according to two rules:   - If abs(value) &gt;= 1: keep <code>decimal_precision</code> decimal places.   - If abs(value) &lt; 1: keep the first <code>decimal_precision</code> nonzero decimals.</p> <p>Removes trailing zeros and decimal points for cleaner output.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>any</code> <p>The value to be rounded. Can be a number or string representation of a number.</p> required <code>decimal_precision</code> <code>int</code> <p>The number of decimals (if &gt;=1) or the number of significant decimals (if &lt;1).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Clean string representation, or original value if conversion fails.</p> Source code in <code>twinweaver/common/converter_base.py</code> <pre><code>def round_and_strip(value, decimal_precision):\n    \"\"\"\n    Formats a number according to two rules:\n      - If abs(value) &gt;= 1: keep `decimal_precision` decimal places.\n      - If abs(value) &lt; 1: keep the first `decimal_precision` nonzero decimals.\n\n    Removes trailing zeros and decimal points for cleaner output.\n\n    Parameters\n    ----------\n    value : any\n        The value to be rounded. Can be a number or string representation of a number.\n    decimal_precision : int\n        The number of decimals (if &gt;=1) or the number of significant decimals (if &lt;1).\n\n    Returns\n    -------\n    str\n        Clean string representation, or original value if conversion fails.\n    \"\"\"\n    try:\n        num = float(value)\n        abs_num = abs(num)\n        if abs_num &gt;= 1:\n            rounded_value = round(num, decimal_precision)\n        else:\n            if num == 0:\n                return \"0\"  # to avoid log10(0) issues\n\n            # Find exponent of the first significant digit\n            exponent = -int(math.floor(math.log10(abs_num)))\n\n            # Scale and round to significant digits\n            total_decimals = exponent + decimal_precision - 1\n            # Attempt to convert to float and round\n            rounded_value = round(num, total_decimals)\n\n        # Convert to string and strip trailing zeros\n        return str(rounded_value).rstrip(\"0\").rstrip(\".\")\n    except ValueError:\n        # If conversion fails, return the original value\n        return value\n</code></pre>"},{"location":"reference/common/data_manager/","title":"Data Manager","text":""},{"location":"reference/common/data_manager/#twinweaver.common.data_manager","title":"twinweaver.common.data_manager","text":""},{"location":"reference/common/data_manager/#twinweaver.common.data_manager-classes","title":"Classes","text":""},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager","title":"DataManager","text":"<p>Manages data loading, processing, and splitting for a single indication.</p> <p>This class handles the lifecycle of data for one specific indication, including loading data from files (or using overridden dataframes), performing processing steps like date conversion and cleaning, ensuring unique event naming, and splitting the patient data into training, validation, and test sets based on patient IDs. It utilizes a <code>Config</code> object for various settings and column names.</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>class DataManager:\n    \"\"\"\n    Manages data loading, processing, and splitting for a single indication.\n\n    This class handles the lifecycle of data for one specific indication,\n    including loading data from files (or using overridden dataframes),\n    performing processing steps like date conversion and cleaning, ensuring\n    unique event naming, and splitting the patient data into training,\n    validation, and test sets based on patient IDs. It utilizes a `Config`\n    object for various settings and column names.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Config,  # Added config parameter\n        train_split_min: float = 0.8,\n        validation_split_max: float = 0.1,\n        test_split_max: float = 0.1,\n        max_val_test_nr_patients: int = 500,\n        replace_special_symbols_override: list = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the DataManager for a specific indication.\n\n        Sets up the manager with the configuration, data split\n        parameters, and options for handling special characters in event names.\n\n        Parameters\n        ----------\n        config : Config\n            A configuration object containing paths, column names, category names,\n            and other constants used throughout the data management process.\n        train_split_min : float, optional\n            The minimum proportion of patients to allocate to the training set.\n            Defaults to 0.8. The actual number will be the remainder after\n            allocating validation and test sets.\n        validation_split_max : float, optional\n            The maximum proportion of the total patients to allocate to the\n            validation set. The actual number is capped by\n            `max_val_test_nr_patients`. Defaults to 0.1.\n        test_split_max : float, optional\n            The maximum proportion of the total patients to allocate to the\n            test set. The actual number is capped by `max_val_test_nr_patients`.\n            Defaults to 0.1.\n        max_val_test_nr_patients : int, optional\n            The absolute maximum number of patients to include in the validation\n            and test sets combined. Defaults to 500.\n        replace_special_symbols_override : list, optional\n            A list of tuples to override the default special character replacements\n            in event descriptive names. Each tuple should be in the format\n            `(event_category, (string_to_replace, replacement_string))`. If None,\n            default replacements specified in the method are used. Defaults to None.\n        \"\"\"\n\n        #: initialize the data manager\n        self.config = config  # Store config object\n        self.train_split = train_split_min\n        self.validation_split = validation_split_max\n        self.test_split = test_split_max\n        self.max_val_test_nr_patients = max_val_test_nr_patients\n        self.variable_types = {}  # event_name -&gt; \"numeric\" / \"categorical\"\n\n        # Setup replacing of special symbol, format is event_category : (&lt;string_to_replace&gt;, &lt;replacement_string&gt;)\n        if replace_special_symbols_override is not None:\n            self.replace_special_symbols = replace_special_symbols_override\n        else:\n            # Use config constants for event categories where available\n            self.replace_special_symbols = [\n                (self.config.event_category_labs, (\"/\", \" per \")),\n                (self.config.event_category_labs, (\".\", \" \")),\n                (\n                    \"drug\",\n                    (\"/\", \" \"),\n                ),  # \"drug\" category not explicitly in Config constants provided\n                (\n                    self.config.event_category_lot,\n                    (\"/\", \" \"),\n                ),  # Use config for 'lot' category\n            ]\n\n        # Setup indication\n        self.data_frames = None\n        self.unique_events = None\n        self.all_patientids = None\n\n        # Set seed\n        np.random.seed(self.config.seed)\n        random.seed(self.config.seed)\n\n    def load_indication_data(\n        self, df_events: pd.DataFrame, df_constant: pd.DataFrame, df_constant_description: pd.DataFrame\n    ) -&gt; None:\n        \"\"\"\n        Loads the data tables (as dataframes) for the specified indication.\n        It also removes any columns named \"Unnamed: *\" from the loaded DataFrames.\n\n        Parameters\n        ----------\n        df_events : pd.DataFrame\n            The events dataframe containing time-series data.\n        df_constant : pd.DataFrame\n            The constant dataframe containing static patient data.\n        df_constant_description : pd.DataFrame\n            The dataframe describing the constant variables.\n        \"\"\"\n\n        # Copy over\n        df_events = df_events.copy()\n        df_constant = df_constant.copy()\n        df_constant_description = df_constant_description.copy()\n\n        # Do some basic checks\n        assert df_events.shape[0] &gt; 0, \"df_events is empty\"\n        assert df_constant.shape[0] &gt; 0, \"df_constant is empty\"\n        assert df_constant_description.shape[0] &gt; 0, \"df_constant_description is empty\"\n\n        # Assert cols in events\n        assert self.config.patient_id_col in df_events.columns, (\n            f\"Patient ID column {self.config.patient_id_col} not in events dataframe\"\n        )\n        assert self.config.event_descriptive_name_col in df_events.columns, (\n            f\"Event descriptive name column {self.config.event_descriptive_name_col} not in events dataframe\"\n        )\n        assert self.config.event_value_col in df_events.columns, (\n            f\"Event value column {self.config.event_value_col} not in events dataframe\"\n        )\n        assert self.config.date_col in df_events.columns, f\"Date column {self.config.date_col} not in events dataframe\"\n\n        # Fil in missing columns\n\n        # If no event category, set it to \"unknown\"\n        if self.config.event_category_col not in df_events.columns:\n            df_events[self.config.event_category_col] = self.config.event_category_default_value\n\n        # If no event name, set it to event_descriptive_name\n        if self.config.event_name_col not in df_events.columns:\n            df_events[self.config.event_name_col] = df_events[self.config.event_descriptive_name_col]\n\n        # If not meta column, set to empty\n        if self.config.meta_data_col not in df_events.columns:\n            df_events[self.config.meta_data_col] = self.config.event_meta_default_value\n\n        # If no source columns, set it to \"events\"\n        if self.config.source_col not in df_events.columns:\n            df_events[self.config.source_col] = self.config.source_col_default_value\n\n        # Assert cols in constant\n        assert self.config.patient_id_col in df_constant.columns, (\n            f\"Patient ID column {self.config.patient_id_col} not in constant dataframe\"\n        )\n\n        # assert cols in constant_description\n        assert \"variable\" in df_constant_description.columns, \"Column 'variable' not in constant_description dataframe\"\n        assert \"comment\" in df_constant_description.columns, \"Column 'comment' not in constant_description dataframe\"\n\n        self.data_frames = {}\n        self.data_frames[\"events\"] = df_events\n        self.data_frames[\"constant\"] = df_constant\n        self.data_frames[\"constant_description\"] = df_constant_description\n\n        #: drop all \"Unnamed\" columns\n        def remove_unnamed_columns(df):\n            return df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n\n        for key in self.data_frames.keys():\n            if self.data_frames[key] is not None:\n                self.data_frames[key] = remove_unnamed_columns(self.data_frames[key])\n\n        logging.info(\"Data loaded for indication\")\n\n    def process_indication_data(\n        self, skip_missing_dates: bool = False, drop_missing_event_values: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Performs initial processing on the loaded indication data.\n\n        Requires `load_indication_data` to be called first.\n        This method converts the date columns (specified by `config.date_col`)\n        in the 'events' DataFrame to datetime objects.\n        It also checks for and removes rows with missing dates in these tables,\n        logging an error if any are found, unless `skip_missing_dates` is True.\n\n        Raises\n        ------\n        ValueError\n            If `load_indication_data` has not been successfully called before\n            this method, or if missing dates are found and `skip_missing_dates` is False.\n        \"\"\"\n\n        # Check that we already have self.data_frames\n        if not self.data_frames:\n            raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n        # Use config.date_col and config.event_table_name\n        events_table_key = self.config.event_table_name  # \"events\"\n        date_col = self.config.date_col  # \"date\"\n\n        #: convert for all COL_DATE column in each dataset to datetime\n        self.data_frames[events_table_key][date_col] = pd.to_datetime(self.data_frames[events_table_key][date_col])\n\n        # Check and drop all rows with missing date in events, and print warning if more than 0\n        missing_date_events = self.data_frames[events_table_key][date_col].isnull().sum()\n        total_events = len(self.data_frames[events_table_key])\n\n        def handle_missing_dates(df_key, missing_count, total_count, col_date):\n            if missing_count &gt; 0:\n                warning_msg = f\"Found {missing_count} out of {total_count} missing dates in {df_key} \"\n                if not skip_missing_dates:\n                    raise ValueError(warning_msg + \"- please fix the dates or set skip_missing_dates=True\")\n                self.data_frames[df_key] = self.data_frames[df_key].dropna(subset=[col_date])\n\n        # Use table keys and config.date_col\n        handle_missing_dates(events_table_key, missing_date_events, total_events, date_col)\n\n        # Assert that no missing values in event_value\n        missing_event_values = self.data_frames[events_table_key][self.config.event_value_col].isnull().sum()\n        total_events = len(self.data_frames[events_table_key])\n        if drop_missing_event_values:\n            if missing_event_values &gt; 0:\n                logging.warning(\n                    f\"Dropping {missing_event_values} out of {total_events} rows with missing event \"\n                    \"values in events table\"\n                )\n                self.data_frames[events_table_key] = self.data_frames[events_table_key].dropna(\n                    subset=[self.config.event_value_col]\n                )\n        else:\n            if missing_event_values &gt; 0:\n                raise ValueError(\n                    f\"Found {missing_event_values} out of {total_events} missing event values in events table\"\n                    \"please fix the data or set drop_missing_event_values=True\"\n                )\n\n        # Convert all event values to string\n        self.data_frames[events_table_key][self.config.event_value_col] = self.data_frames[events_table_key][\n            self.config.event_value_col\n        ].astype(str)\n\n        logging.info(\"Data processed\")\n\n    def setup_unique_mapping_of_events(self) -&gt; None:\n        \"\"\"\n        Ensures uniqueness of descriptive event names and applies replacements.\n\n        Requires `load_indication_data` to be called first.\n        This method first identifies `event_descriptive_name` values that map to\n        multiple `event_name` values within the same `event_category`. For these\n        non-unique descriptive names, it appends the corresponding `event_name`\n        to make them unique (e.g., \"Measurement\" becomes \"Measurement - Systolic BP\").\n\n        Secondly, it applies predefined or overridden special character replacements\n        (e.g., replacing \"/\" with \" per \" in lab results) to the\n        `event_descriptive_name` column based on the `event_category`.\n\n        Finally, it rebuilds the `self.unique_events` mapping (containing unique\n        combinations of event_name, event_descriptive_name, and event_category)\n        and asserts that all `event_descriptive_name` values are now unique.\n\n        Raises\n        ------\n        ValueError\n            If `load_indication_data` has not been successfully called before\n            this method.\n        AssertionError\n            If, after processing, the `event_descriptive_name` column still\n            contains duplicate values.\n        \"\"\"\n\n        # Check that we already have self.data_frames\n        if not self.data_frames:\n            raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n        # Use config constants for column names\n        event_name_col = self.config.event_name_col\n        event_desc_name_col = self.config.event_descriptive_name_col\n        event_cat_col = self.config.event_category_col\n        events_table_key = self.config.event_table_name\n\n        #: get all unique pairs of event_name and event_descriptive_name in self.data_frames[\"events\"]\n        self.unique_events = self.data_frames[events_table_key]\n        self.unique_events = self.unique_events[[event_name_col, event_desc_name_col, event_cat_col]]\n        self.unique_events = self.unique_events.copy().drop_duplicates()\n        self.unique_events = self.unique_events.reset_index(drop=True)\n\n        #: get all event_descriptive_name that are not unique\n        non_unique_events = self.unique_events[event_desc_name_col].value_counts()\n        non_unique_events = non_unique_events[non_unique_events &gt; 1]\n\n        # Extract corresponding event_name and event_category\n        filtered_events = self.unique_events[event_desc_name_col]\n        non_unique_events = self.unique_events[filtered_events.isin(non_unique_events.index)].copy()\n\n        # create mapping for all non-unique descriptive names, and\n        # then add event_name to those, and apply across entire dataset\n        # Keep temporary column name as string literal\n        non_unique_events[\"new_descriptive_name\"] = (\n            non_unique_events[event_desc_name_col] + \" - \" + non_unique_events[event_name_col]\n        )\n        # Use config constants for column names\n        non_unique_events = non_unique_events[[\"new_descriptive_name\", event_name_col, event_cat_col]]\n\n        self.data_frames[events_table_key] = pd.merge(\n            self.data_frames[events_table_key],\n            non_unique_events,\n            how=\"left\",\n            on=(event_name_col, event_cat_col),\n        )  # Use config constants\n        events_df = self.data_frames[events_table_key]\n        new_desc_name = \"new_descriptive_name\"  # Keep temporary column name as string literal\n        # Use config constant\n        events_df[event_desc_name_col] = events_df[new_desc_name].fillna(events_df[event_desc_name_col])\n        self.data_frames[events_table_key] = self.data_frames[events_table_key].drop(columns=[\"new_descriptive_name\"])\n\n        #: first convert special symbols in event_descriptive_name to alternatives, using self.replace_special_symbols\n        for event_category, (\n            string_to_replace,\n            replacement_string,\n        ) in self.replace_special_symbols:\n            events_df = self.data_frames[events_table_key]\n            # Use config constants\n            category_mask = events_df[event_cat_col] == event_category\n            desc_name_col = event_desc_name_col\n\n            events_df.loc[category_mask, desc_name_col] = (\n                events_df.loc[category_mask, desc_name_col]\n                .astype(str)  # Ensure string type before replace\n                .str.replace(\n                    string_to_replace, replacement_string, regex=False\n                )  # Added regex=False for literal replacement\n            )\n\n        #: recalculate self.unique_events and ensure no more non-unique event_descriptive_name\n        # Use config constants\n        cols_to_select = [event_name_col, event_desc_name_col, event_cat_col]\n        self.unique_events = self.data_frames[events_table_key][cols_to_select].copy().drop_duplicates()\n        self.unique_events = self.unique_events.reset_index(drop=True)\n\n        # Assert that all unique now\n        # Use config constant\n        assert len(self.unique_events) == len(self.data_frames[events_table_key][event_desc_name_col].unique())\n\n    def setup_dataset_splits(\n        self,\n    ) -&gt; None:\n        \"\"\"\n        Assigns each patient to a data split (train, validation, or test).\n\n        Requires `load_indication_data` to be called first.\n        The method determines the split assignment for each patient.\n        It retrieves all unique patient IDs from the 'constant' data table.\n        It calculates the number of patients for validation and test sets based on\n        the `validation_split_max`, `test_split_max`, and `max_val_test_nr_patients`\n        parameters set during initialization. The remaining patients are assigned to the training set #\n        (calculated as the remainder after validation and test sets are allocated). Patients are randomly\n        shuffled (with a fixed seed for reproducibility) before assignment.\n\n        The resulting mapping (patient ID to split name) is assigned to the\n        constant dataframe. It also stores all patient IDs in\n        `self.all_patientids`. Asserts are performed to ensure the mapping covers\n        all patients without overlap and that the split sizes match calculations.\n\n        Raises\n        ------\n        ValueError\n            If `load_indication_data` has not been successfully called before\n            this method.\n        AssertionError\n            If calculated splits do not match expected counts or if overlaps exist.\n        \"\"\"\n\n        # Check that we already have self.data_frames\n        if not self.data_frames:\n            raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n        # Use config constants\n        patient_id_col = self.config.patient_id_col\n        constant_table_key = \"constant\"  # Key remains \"constant\"\n        train_split_name = self.config.train_split_name  # Use config for \"train\" split name\n\n        # Raise warning if split column already exists in constant table\n        if self.config.constant_split_col in self.data_frames[constant_table_key].columns:\n            logging.warning(\n                f\"Column {self.config.constant_split_col} already exists in constant table. It will be overwritten.\"\n            )\n\n        #: get all patientids from self.data_frames[\"constant\"]\n        all_patients = self.data_frames[constant_table_key][patient_id_col].unique()\n        self.all_patientids = all_patients\n\n        #: get min(self.validation_split * num_patients, self.max_val_test_nr_patients)\n        validation_nr_patients = min(\n            int(self.validation_split * len(all_patients)),\n            self.max_val_test_nr_patients,\n        )\n\n        #: then the same for test\n        test_nr_patients = min(int(self.test_split * len(all_patients)), self.max_val_test_nr_patients)\n\n        #: randomly shuffle with seed and split into train/val/test, using df.sample\n        np.random.seed(self.config.seed)\n        all_patients = np.random.permutation(all_patients)\n        train_nr_patients = len(all_patients) - validation_nr_patients - test_nr_patients\n\n        #: setup mapping so that each patientid returns which split it belongs to\n        patient_to_split_mapping = {}\n        # Use config.train_split_name for the train split key/value\n        # Keep \"validation\" and \"test\" as strings since not defined in config\n        patient_to_split_mapping.update({patient: train_split_name for patient in all_patients[:train_nr_patients]})\n        patient_to_split_mapping.update(\n            {\n                patient: self.config.validation_split_name\n                for patient in all_patients[train_nr_patients : train_nr_patients + validation_nr_patients]\n            }\n        )\n        patient_to_split_mapping.update(\n            {\n                patient: self.config.test_split_name\n                for patient in all_patients[train_nr_patients + validation_nr_patients :]\n            }\n        )\n\n        #: assert that no overlap in patient mappings\n        assert len(patient_to_split_mapping) == len(all_patients)\n\n        #: assert that correct lengths\n        # Use config.train_split_name for checking train split length\n        assert (\n            len([patient for patient, split in patient_to_split_mapping.items() if split == train_split_name])\n            == train_nr_patients\n        )\n        assert (\n            len(\n                [\n                    patient\n                    for patient, split in patient_to_split_mapping.items()\n                    if split == self.config.validation_split_name\n                ]\n            )\n            == validation_nr_patients\n        )\n        assert (\n            len(\n                [patient for patient, split in patient_to_split_mapping.items() if split == self.config.test_split_name]\n            )\n            == test_nr_patients\n        )\n\n        # Assign to constant dataframe\n        self.data_frames[constant_table_key][self.config.constant_split_col] = self.data_frames[constant_table_key][\n            patient_id_col\n        ].map(patient_to_split_mapping)\n\n    def get_all_patientids_in_split(self, split: str) -&gt; str:\n        \"\"\"\n        Retrieves all patient IDs belonging to a specific data split.\n\n        Parameters\n        ----------\n        split : str\n            The name of the split (e.g., \"train\", \"validation\", \"test\").\n\n        Returns\n        -------\n        list\n            A list of patient ID strings belonging to the specified split.\n        \"\"\"\n        # Use config constant for patient ID if needed, but here it's just a key lookup\n        # patientid is the key itself.\n        return (\n            self.data_frames[\"constant\"][self.data_frames[\"constant\"][self.config.constant_split_col] == split][\n                self.config.patient_id_col\n            ]\n            .unique()\n            .tolist()\n        )\n\n    def get_patient_split(self, patientid: str) -&gt; str:\n        \"\"\"\n        Retrieves the split assignment for a specific patient.\n\n        Parameters\n        ----------\n        patientid : str\n            The unique identifier for the patient.\n\n        Returns\n        -------\n        str\n            The name of the split the patient belongs to.\n        \"\"\"\n        return (\n            self.data_frames[\"constant\"]\n            .loc[\n                self.data_frames[\"constant\"][self.config.patient_id_col] == patientid,\n                self.config.constant_split_col,\n            ]\n            .values[0]\n        )\n\n    def get_patient_data(self, patientid: str) -&gt; dict:\n        \"\"\"\n        Retrieves and consolidates all data for a specific patient.\n\n        Requires `load_indication_data` and `process_indication_data` to have\n        been called. It's also recommended to call `setup_unique_mapping_of_events`\n        to ensure consistent event naming.\n\n        This method gathers data from the 'events', and 'constant'\n        DataFrames for the specified `patientid`.\n        - It filters the 'events' tables for the patient.\n        - It filters the 'constant' table for the patient's static data.\n\n        Parameters\n        ----------\n        patientid : str\n            The unique identifier for the patient whose data is to be retrieved.\n\n        Returns\n        -------\n        dict\n            A dictionary containing the patient's data, with two keys:\n            - \"events\": A pandas DataFrame containing all time-series events\n                        (events data and sortedby date).\n            - \"constant\": A pandas DataFrame containing the static (constant)\n                          data for the patient.\n\n        Raises\n        ------\n        ValueError\n            If `load_indication_data` has not been successfully called before\n            this method.\n        KeyError\n            If essential columns specified in the config are missing from the\n            dataframes after loading.\n        \"\"\"\n\n        # Check that we already have self.data_frames\n        if not self.data_frames:\n            raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n        # Use config constants for column names and table keys/sources where applicable\n        patient_id_col = self.config.patient_id_col\n        events_table_key = self.config.event_table_name  # \"events\"\n        constant_table_key = \"constant\"  # Key remains \"constant\"\n\n        #: get all data for a specific patient\n        patient_data = {}\n\n        #: first from events\n        events = self.data_frames[events_table_key][\n            self.data_frames[events_table_key][patient_id_col] == patientid\n        ].copy()\n        patient_data[\"events\"] = events.sort_values(by=self.config.date_col)\n\n        #: then from constant\n        selected_patient = self.data_frames[constant_table_key][patient_id_col] == patientid\n        patient_data[\"constant\"] = self.data_frames[constant_table_key][selected_patient]\n\n        # Remove any duplicates in case they get in events\n        # Keep \"events\" key as string\n        patient_data[\"events\"] = patient_data[\"events\"].drop_duplicates()\n\n        #: return\n        return patient_data\n\n    def infer_var_types(self):\n        \"\"\"\n        Fills self.dm.variable_types for every candidate forecasting variable.\n        Classifies as \"numeric\" if at least `self.config.numeric_detect_min_fraction` of values\n        can be parsed as numeric, otherwise \"categorical\".\n        \"\"\"\n\n        assert self.config.event_category_forecast is not None, (\n            \"event_category_forecast must be set in config, e.g. to ['lab']\"\n        )\n\n        events = self.data_frames[self.config.event_table_name]\n        name_col = self.config.event_name_col\n        value_col = self.config.event_value_col\n\n        # Consider only variables in the configured forecasting categories\n        mask_cat = events[self.config.event_category_col].isin(self.config.event_category_forecast)\n        df = events.loc[mask_cat, [name_col, value_col]].copy()\n\n        for var, sub in df.groupby(name_col):\n            # Try numeric parse\n            v = pd.to_numeric(sub[value_col], errors=\"coerce\")\n            frac_num = v.notna().mean()\n            if frac_num &gt;= self.config.numeric_detect_min_fraction:\n                logging.info(f\"Variable {var} classified as numeric ({frac_num:.2%} numeric values)\")\n                self.variable_types[var] = \"numeric\"\n            else:\n                logging.info(f\"Variable {var} classified as categorical ({frac_num:.2%} numeric values)\")\n                self.variable_types[var] = \"categorical\"\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager-functions","title":"Functions","text":""},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.__init__","title":"__init__","text":"<pre><code>__init__(\n    config,\n    train_split_min=0.8,\n    validation_split_max=0.1,\n    test_split_max=0.1,\n    max_val_test_nr_patients=500,\n    replace_special_symbols_override=None,\n)\n</code></pre> <p>Initializes the DataManager for a specific indication.</p> <p>Sets up the manager with the configuration, data split parameters, and options for handling special characters in event names.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>A configuration object containing paths, column names, category names, and other constants used throughout the data management process.</p> required <code>train_split_min</code> <code>float</code> <p>The minimum proportion of patients to allocate to the training set. Defaults to 0.8. The actual number will be the remainder after allocating validation and test sets.</p> <code>0.8</code> <code>validation_split_max</code> <code>float</code> <p>The maximum proportion of the total patients to allocate to the validation set. The actual number is capped by <code>max_val_test_nr_patients</code>. Defaults to 0.1.</p> <code>0.1</code> <code>test_split_max</code> <code>float</code> <p>The maximum proportion of the total patients to allocate to the test set. The actual number is capped by <code>max_val_test_nr_patients</code>. Defaults to 0.1.</p> <code>0.1</code> <code>max_val_test_nr_patients</code> <code>int</code> <p>The absolute maximum number of patients to include in the validation and test sets combined. Defaults to 500.</p> <code>500</code> <code>replace_special_symbols_override</code> <code>list</code> <p>A list of tuples to override the default special character replacements in event descriptive names. Each tuple should be in the format <code>(event_category, (string_to_replace, replacement_string))</code>. If None, default replacements specified in the method are used. Defaults to None.</p> <code>None</code> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def __init__(\n    self,\n    config: Config,  # Added config parameter\n    train_split_min: float = 0.8,\n    validation_split_max: float = 0.1,\n    test_split_max: float = 0.1,\n    max_val_test_nr_patients: int = 500,\n    replace_special_symbols_override: list = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the DataManager for a specific indication.\n\n    Sets up the manager with the configuration, data split\n    parameters, and options for handling special characters in event names.\n\n    Parameters\n    ----------\n    config : Config\n        A configuration object containing paths, column names, category names,\n        and other constants used throughout the data management process.\n    train_split_min : float, optional\n        The minimum proportion of patients to allocate to the training set.\n        Defaults to 0.8. The actual number will be the remainder after\n        allocating validation and test sets.\n    validation_split_max : float, optional\n        The maximum proportion of the total patients to allocate to the\n        validation set. The actual number is capped by\n        `max_val_test_nr_patients`. Defaults to 0.1.\n    test_split_max : float, optional\n        The maximum proportion of the total patients to allocate to the\n        test set. The actual number is capped by `max_val_test_nr_patients`.\n        Defaults to 0.1.\n    max_val_test_nr_patients : int, optional\n        The absolute maximum number of patients to include in the validation\n        and test sets combined. Defaults to 500.\n    replace_special_symbols_override : list, optional\n        A list of tuples to override the default special character replacements\n        in event descriptive names. Each tuple should be in the format\n        `(event_category, (string_to_replace, replacement_string))`. If None,\n        default replacements specified in the method are used. Defaults to None.\n    \"\"\"\n\n    #: initialize the data manager\n    self.config = config  # Store config object\n    self.train_split = train_split_min\n    self.validation_split = validation_split_max\n    self.test_split = test_split_max\n    self.max_val_test_nr_patients = max_val_test_nr_patients\n    self.variable_types = {}  # event_name -&gt; \"numeric\" / \"categorical\"\n\n    # Setup replacing of special symbol, format is event_category : (&lt;string_to_replace&gt;, &lt;replacement_string&gt;)\n    if replace_special_symbols_override is not None:\n        self.replace_special_symbols = replace_special_symbols_override\n    else:\n        # Use config constants for event categories where available\n        self.replace_special_symbols = [\n            (self.config.event_category_labs, (\"/\", \" per \")),\n            (self.config.event_category_labs, (\".\", \" \")),\n            (\n                \"drug\",\n                (\"/\", \" \"),\n            ),  # \"drug\" category not explicitly in Config constants provided\n            (\n                self.config.event_category_lot,\n                (\"/\", \" \"),\n            ),  # Use config for 'lot' category\n        ]\n\n    # Setup indication\n    self.data_frames = None\n    self.unique_events = None\n    self.all_patientids = None\n\n    # Set seed\n    np.random.seed(self.config.seed)\n    random.seed(self.config.seed)\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.get_all_patientids_in_split","title":"get_all_patientids_in_split","text":"<pre><code>get_all_patientids_in_split(split)\n</code></pre> <p>Retrieves all patient IDs belonging to a specific data split.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>The name of the split (e.g., \"train\", \"validation\", \"test\").</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of patient ID strings belonging to the specified split.</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def get_all_patientids_in_split(self, split: str) -&gt; str:\n    \"\"\"\n    Retrieves all patient IDs belonging to a specific data split.\n\n    Parameters\n    ----------\n    split : str\n        The name of the split (e.g., \"train\", \"validation\", \"test\").\n\n    Returns\n    -------\n    list\n        A list of patient ID strings belonging to the specified split.\n    \"\"\"\n    # Use config constant for patient ID if needed, but here it's just a key lookup\n    # patientid is the key itself.\n    return (\n        self.data_frames[\"constant\"][self.data_frames[\"constant\"][self.config.constant_split_col] == split][\n            self.config.patient_id_col\n        ]\n        .unique()\n        .tolist()\n    )\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.get_patient_data","title":"get_patient_data","text":"<pre><code>get_patient_data(patientid)\n</code></pre> <p>Retrieves and consolidates all data for a specific patient.</p> <p>Requires <code>load_indication_data</code> and <code>process_indication_data</code> to have been called. It's also recommended to call <code>setup_unique_mapping_of_events</code> to ensure consistent event naming.</p> <p>This method gathers data from the 'events', and 'constant' DataFrames for the specified <code>patientid</code>. - It filters the 'events' tables for the patient. - It filters the 'constant' table for the patient's static data.</p> <p>Parameters:</p> Name Type Description Default <code>patientid</code> <code>str</code> <p>The unique identifier for the patient whose data is to be retrieved.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the patient's data, with two keys: - \"events\": A pandas DataFrame containing all time-series events             (events data and sortedby date). - \"constant\": A pandas DataFrame containing the static (constant)               data for the patient.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>load_indication_data</code> has not been successfully called before this method.</p> <code>KeyError</code> <p>If essential columns specified in the config are missing from the dataframes after loading.</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def get_patient_data(self, patientid: str) -&gt; dict:\n    \"\"\"\n    Retrieves and consolidates all data for a specific patient.\n\n    Requires `load_indication_data` and `process_indication_data` to have\n    been called. It's also recommended to call `setup_unique_mapping_of_events`\n    to ensure consistent event naming.\n\n    This method gathers data from the 'events', and 'constant'\n    DataFrames for the specified `patientid`.\n    - It filters the 'events' tables for the patient.\n    - It filters the 'constant' table for the patient's static data.\n\n    Parameters\n    ----------\n    patientid : str\n        The unique identifier for the patient whose data is to be retrieved.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the patient's data, with two keys:\n        - \"events\": A pandas DataFrame containing all time-series events\n                    (events data and sortedby date).\n        - \"constant\": A pandas DataFrame containing the static (constant)\n                      data for the patient.\n\n    Raises\n    ------\n    ValueError\n        If `load_indication_data` has not been successfully called before\n        this method.\n    KeyError\n        If essential columns specified in the config are missing from the\n        dataframes after loading.\n    \"\"\"\n\n    # Check that we already have self.data_frames\n    if not self.data_frames:\n        raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n    # Use config constants for column names and table keys/sources where applicable\n    patient_id_col = self.config.patient_id_col\n    events_table_key = self.config.event_table_name  # \"events\"\n    constant_table_key = \"constant\"  # Key remains \"constant\"\n\n    #: get all data for a specific patient\n    patient_data = {}\n\n    #: first from events\n    events = self.data_frames[events_table_key][\n        self.data_frames[events_table_key][patient_id_col] == patientid\n    ].copy()\n    patient_data[\"events\"] = events.sort_values(by=self.config.date_col)\n\n    #: then from constant\n    selected_patient = self.data_frames[constant_table_key][patient_id_col] == patientid\n    patient_data[\"constant\"] = self.data_frames[constant_table_key][selected_patient]\n\n    # Remove any duplicates in case they get in events\n    # Keep \"events\" key as string\n    patient_data[\"events\"] = patient_data[\"events\"].drop_duplicates()\n\n    #: return\n    return patient_data\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.get_patient_split","title":"get_patient_split","text":"<pre><code>get_patient_split(patientid)\n</code></pre> <p>Retrieves the split assignment for a specific patient.</p> <p>Parameters:</p> Name Type Description Default <code>patientid</code> <code>str</code> <p>The unique identifier for the patient.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the split the patient belongs to.</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def get_patient_split(self, patientid: str) -&gt; str:\n    \"\"\"\n    Retrieves the split assignment for a specific patient.\n\n    Parameters\n    ----------\n    patientid : str\n        The unique identifier for the patient.\n\n    Returns\n    -------\n    str\n        The name of the split the patient belongs to.\n    \"\"\"\n    return (\n        self.data_frames[\"constant\"]\n        .loc[\n            self.data_frames[\"constant\"][self.config.patient_id_col] == patientid,\n            self.config.constant_split_col,\n        ]\n        .values[0]\n    )\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.infer_var_types","title":"infer_var_types","text":"<pre><code>infer_var_types()\n</code></pre> <p>Fills self.dm.variable_types for every candidate forecasting variable. Classifies as \"numeric\" if at least <code>self.config.numeric_detect_min_fraction</code> of values can be parsed as numeric, otherwise \"categorical\".</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def infer_var_types(self):\n    \"\"\"\n    Fills self.dm.variable_types for every candidate forecasting variable.\n    Classifies as \"numeric\" if at least `self.config.numeric_detect_min_fraction` of values\n    can be parsed as numeric, otherwise \"categorical\".\n    \"\"\"\n\n    assert self.config.event_category_forecast is not None, (\n        \"event_category_forecast must be set in config, e.g. to ['lab']\"\n    )\n\n    events = self.data_frames[self.config.event_table_name]\n    name_col = self.config.event_name_col\n    value_col = self.config.event_value_col\n\n    # Consider only variables in the configured forecasting categories\n    mask_cat = events[self.config.event_category_col].isin(self.config.event_category_forecast)\n    df = events.loc[mask_cat, [name_col, value_col]].copy()\n\n    for var, sub in df.groupby(name_col):\n        # Try numeric parse\n        v = pd.to_numeric(sub[value_col], errors=\"coerce\")\n        frac_num = v.notna().mean()\n        if frac_num &gt;= self.config.numeric_detect_min_fraction:\n            logging.info(f\"Variable {var} classified as numeric ({frac_num:.2%} numeric values)\")\n            self.variable_types[var] = \"numeric\"\n        else:\n            logging.info(f\"Variable {var} classified as categorical ({frac_num:.2%} numeric values)\")\n            self.variable_types[var] = \"categorical\"\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.load_indication_data","title":"load_indication_data","text":"<pre><code>load_indication_data(\n    df_events, df_constant, df_constant_description\n)\n</code></pre> <p>Loads the data tables (as dataframes) for the specified indication. It also removes any columns named \"Unnamed: *\" from the loaded DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>df_events</code> <code>DataFrame</code> <p>The events dataframe containing time-series data.</p> required <code>df_constant</code> <code>DataFrame</code> <p>The constant dataframe containing static patient data.</p> required <code>df_constant_description</code> <code>DataFrame</code> <p>The dataframe describing the constant variables.</p> required Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def load_indication_data(\n    self, df_events: pd.DataFrame, df_constant: pd.DataFrame, df_constant_description: pd.DataFrame\n) -&gt; None:\n    \"\"\"\n    Loads the data tables (as dataframes) for the specified indication.\n    It also removes any columns named \"Unnamed: *\" from the loaded DataFrames.\n\n    Parameters\n    ----------\n    df_events : pd.DataFrame\n        The events dataframe containing time-series data.\n    df_constant : pd.DataFrame\n        The constant dataframe containing static patient data.\n    df_constant_description : pd.DataFrame\n        The dataframe describing the constant variables.\n    \"\"\"\n\n    # Copy over\n    df_events = df_events.copy()\n    df_constant = df_constant.copy()\n    df_constant_description = df_constant_description.copy()\n\n    # Do some basic checks\n    assert df_events.shape[0] &gt; 0, \"df_events is empty\"\n    assert df_constant.shape[0] &gt; 0, \"df_constant is empty\"\n    assert df_constant_description.shape[0] &gt; 0, \"df_constant_description is empty\"\n\n    # Assert cols in events\n    assert self.config.patient_id_col in df_events.columns, (\n        f\"Patient ID column {self.config.patient_id_col} not in events dataframe\"\n    )\n    assert self.config.event_descriptive_name_col in df_events.columns, (\n        f\"Event descriptive name column {self.config.event_descriptive_name_col} not in events dataframe\"\n    )\n    assert self.config.event_value_col in df_events.columns, (\n        f\"Event value column {self.config.event_value_col} not in events dataframe\"\n    )\n    assert self.config.date_col in df_events.columns, f\"Date column {self.config.date_col} not in events dataframe\"\n\n    # Fil in missing columns\n\n    # If no event category, set it to \"unknown\"\n    if self.config.event_category_col not in df_events.columns:\n        df_events[self.config.event_category_col] = self.config.event_category_default_value\n\n    # If no event name, set it to event_descriptive_name\n    if self.config.event_name_col not in df_events.columns:\n        df_events[self.config.event_name_col] = df_events[self.config.event_descriptive_name_col]\n\n    # If not meta column, set to empty\n    if self.config.meta_data_col not in df_events.columns:\n        df_events[self.config.meta_data_col] = self.config.event_meta_default_value\n\n    # If no source columns, set it to \"events\"\n    if self.config.source_col not in df_events.columns:\n        df_events[self.config.source_col] = self.config.source_col_default_value\n\n    # Assert cols in constant\n    assert self.config.patient_id_col in df_constant.columns, (\n        f\"Patient ID column {self.config.patient_id_col} not in constant dataframe\"\n    )\n\n    # assert cols in constant_description\n    assert \"variable\" in df_constant_description.columns, \"Column 'variable' not in constant_description dataframe\"\n    assert \"comment\" in df_constant_description.columns, \"Column 'comment' not in constant_description dataframe\"\n\n    self.data_frames = {}\n    self.data_frames[\"events\"] = df_events\n    self.data_frames[\"constant\"] = df_constant\n    self.data_frames[\"constant_description\"] = df_constant_description\n\n    #: drop all \"Unnamed\" columns\n    def remove_unnamed_columns(df):\n        return df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n\n    for key in self.data_frames.keys():\n        if self.data_frames[key] is not None:\n            self.data_frames[key] = remove_unnamed_columns(self.data_frames[key])\n\n    logging.info(\"Data loaded for indication\")\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.process_indication_data","title":"process_indication_data","text":"<pre><code>process_indication_data(\n    skip_missing_dates=False,\n    drop_missing_event_values=False,\n)\n</code></pre> <p>Performs initial processing on the loaded indication data.</p> <p>Requires <code>load_indication_data</code> to be called first. This method converts the date columns (specified by <code>config.date_col</code>) in the 'events' DataFrame to datetime objects. It also checks for and removes rows with missing dates in these tables, logging an error if any are found, unless <code>skip_missing_dates</code> is True.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>load_indication_data</code> has not been successfully called before this method, or if missing dates are found and <code>skip_missing_dates</code> is False.</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def process_indication_data(\n    self, skip_missing_dates: bool = False, drop_missing_event_values: bool = False\n) -&gt; None:\n    \"\"\"\n    Performs initial processing on the loaded indication data.\n\n    Requires `load_indication_data` to be called first.\n    This method converts the date columns (specified by `config.date_col`)\n    in the 'events' DataFrame to datetime objects.\n    It also checks for and removes rows with missing dates in these tables,\n    logging an error if any are found, unless `skip_missing_dates` is True.\n\n    Raises\n    ------\n    ValueError\n        If `load_indication_data` has not been successfully called before\n        this method, or if missing dates are found and `skip_missing_dates` is False.\n    \"\"\"\n\n    # Check that we already have self.data_frames\n    if not self.data_frames:\n        raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n    # Use config.date_col and config.event_table_name\n    events_table_key = self.config.event_table_name  # \"events\"\n    date_col = self.config.date_col  # \"date\"\n\n    #: convert for all COL_DATE column in each dataset to datetime\n    self.data_frames[events_table_key][date_col] = pd.to_datetime(self.data_frames[events_table_key][date_col])\n\n    # Check and drop all rows with missing date in events, and print warning if more than 0\n    missing_date_events = self.data_frames[events_table_key][date_col].isnull().sum()\n    total_events = len(self.data_frames[events_table_key])\n\n    def handle_missing_dates(df_key, missing_count, total_count, col_date):\n        if missing_count &gt; 0:\n            warning_msg = f\"Found {missing_count} out of {total_count} missing dates in {df_key} \"\n            if not skip_missing_dates:\n                raise ValueError(warning_msg + \"- please fix the dates or set skip_missing_dates=True\")\n            self.data_frames[df_key] = self.data_frames[df_key].dropna(subset=[col_date])\n\n    # Use table keys and config.date_col\n    handle_missing_dates(events_table_key, missing_date_events, total_events, date_col)\n\n    # Assert that no missing values in event_value\n    missing_event_values = self.data_frames[events_table_key][self.config.event_value_col].isnull().sum()\n    total_events = len(self.data_frames[events_table_key])\n    if drop_missing_event_values:\n        if missing_event_values &gt; 0:\n            logging.warning(\n                f\"Dropping {missing_event_values} out of {total_events} rows with missing event \"\n                \"values in events table\"\n            )\n            self.data_frames[events_table_key] = self.data_frames[events_table_key].dropna(\n                subset=[self.config.event_value_col]\n            )\n    else:\n        if missing_event_values &gt; 0:\n            raise ValueError(\n                f\"Found {missing_event_values} out of {total_events} missing event values in events table\"\n                \"please fix the data or set drop_missing_event_values=True\"\n            )\n\n    # Convert all event values to string\n    self.data_frames[events_table_key][self.config.event_value_col] = self.data_frames[events_table_key][\n        self.config.event_value_col\n    ].astype(str)\n\n    logging.info(\"Data processed\")\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.setup_dataset_splits","title":"setup_dataset_splits","text":"<pre><code>setup_dataset_splits()\n</code></pre> <p>Assigns each patient to a data split (train, validation, or test).</p> <p>Requires <code>load_indication_data</code> to be called first. The method determines the split assignment for each patient. It retrieves all unique patient IDs from the 'constant' data table. It calculates the number of patients for validation and test sets based on the <code>validation_split_max</code>, <code>test_split_max</code>, and <code>max_val_test_nr_patients</code> parameters set during initialization. The remaining patients are assigned to the training set # (calculated as the remainder after validation and test sets are allocated). Patients are randomly shuffled (with a fixed seed for reproducibility) before assignment.</p> <p>The resulting mapping (patient ID to split name) is assigned to the constant dataframe. It also stores all patient IDs in <code>self.all_patientids</code>. Asserts are performed to ensure the mapping covers all patients without overlap and that the split sizes match calculations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>load_indication_data</code> has not been successfully called before this method.</p> <code>AssertionError</code> <p>If calculated splits do not match expected counts or if overlaps exist.</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def setup_dataset_splits(\n    self,\n) -&gt; None:\n    \"\"\"\n    Assigns each patient to a data split (train, validation, or test).\n\n    Requires `load_indication_data` to be called first.\n    The method determines the split assignment for each patient.\n    It retrieves all unique patient IDs from the 'constant' data table.\n    It calculates the number of patients for validation and test sets based on\n    the `validation_split_max`, `test_split_max`, and `max_val_test_nr_patients`\n    parameters set during initialization. The remaining patients are assigned to the training set #\n    (calculated as the remainder after validation and test sets are allocated). Patients are randomly\n    shuffled (with a fixed seed for reproducibility) before assignment.\n\n    The resulting mapping (patient ID to split name) is assigned to the\n    constant dataframe. It also stores all patient IDs in\n    `self.all_patientids`. Asserts are performed to ensure the mapping covers\n    all patients without overlap and that the split sizes match calculations.\n\n    Raises\n    ------\n    ValueError\n        If `load_indication_data` has not been successfully called before\n        this method.\n    AssertionError\n        If calculated splits do not match expected counts or if overlaps exist.\n    \"\"\"\n\n    # Check that we already have self.data_frames\n    if not self.data_frames:\n        raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n    # Use config constants\n    patient_id_col = self.config.patient_id_col\n    constant_table_key = \"constant\"  # Key remains \"constant\"\n    train_split_name = self.config.train_split_name  # Use config for \"train\" split name\n\n    # Raise warning if split column already exists in constant table\n    if self.config.constant_split_col in self.data_frames[constant_table_key].columns:\n        logging.warning(\n            f\"Column {self.config.constant_split_col} already exists in constant table. It will be overwritten.\"\n        )\n\n    #: get all patientids from self.data_frames[\"constant\"]\n    all_patients = self.data_frames[constant_table_key][patient_id_col].unique()\n    self.all_patientids = all_patients\n\n    #: get min(self.validation_split * num_patients, self.max_val_test_nr_patients)\n    validation_nr_patients = min(\n        int(self.validation_split * len(all_patients)),\n        self.max_val_test_nr_patients,\n    )\n\n    #: then the same for test\n    test_nr_patients = min(int(self.test_split * len(all_patients)), self.max_val_test_nr_patients)\n\n    #: randomly shuffle with seed and split into train/val/test, using df.sample\n    np.random.seed(self.config.seed)\n    all_patients = np.random.permutation(all_patients)\n    train_nr_patients = len(all_patients) - validation_nr_patients - test_nr_patients\n\n    #: setup mapping so that each patientid returns which split it belongs to\n    patient_to_split_mapping = {}\n    # Use config.train_split_name for the train split key/value\n    # Keep \"validation\" and \"test\" as strings since not defined in config\n    patient_to_split_mapping.update({patient: train_split_name for patient in all_patients[:train_nr_patients]})\n    patient_to_split_mapping.update(\n        {\n            patient: self.config.validation_split_name\n            for patient in all_patients[train_nr_patients : train_nr_patients + validation_nr_patients]\n        }\n    )\n    patient_to_split_mapping.update(\n        {\n            patient: self.config.test_split_name\n            for patient in all_patients[train_nr_patients + validation_nr_patients :]\n        }\n    )\n\n    #: assert that no overlap in patient mappings\n    assert len(patient_to_split_mapping) == len(all_patients)\n\n    #: assert that correct lengths\n    # Use config.train_split_name for checking train split length\n    assert (\n        len([patient for patient, split in patient_to_split_mapping.items() if split == train_split_name])\n        == train_nr_patients\n    )\n    assert (\n        len(\n            [\n                patient\n                for patient, split in patient_to_split_mapping.items()\n                if split == self.config.validation_split_name\n            ]\n        )\n        == validation_nr_patients\n    )\n    assert (\n        len(\n            [patient for patient, split in patient_to_split_mapping.items() if split == self.config.test_split_name]\n        )\n        == test_nr_patients\n    )\n\n    # Assign to constant dataframe\n    self.data_frames[constant_table_key][self.config.constant_split_col] = self.data_frames[constant_table_key][\n        patient_id_col\n    ].map(patient_to_split_mapping)\n</code></pre>"},{"location":"reference/common/data_manager/#twinweaver.common.data_manager.DataManager.setup_unique_mapping_of_events","title":"setup_unique_mapping_of_events","text":"<pre><code>setup_unique_mapping_of_events()\n</code></pre> <p>Ensures uniqueness of descriptive event names and applies replacements.</p> <p>Requires <code>load_indication_data</code> to be called first. This method first identifies <code>event_descriptive_name</code> values that map to multiple <code>event_name</code> values within the same <code>event_category</code>. For these non-unique descriptive names, it appends the corresponding <code>event_name</code> to make them unique (e.g., \"Measurement\" becomes \"Measurement - Systolic BP\").</p> <p>Secondly, it applies predefined or overridden special character replacements (e.g., replacing \"/\" with \" per \" in lab results) to the <code>event_descriptive_name</code> column based on the <code>event_category</code>.</p> <p>Finally, it rebuilds the <code>self.unique_events</code> mapping (containing unique combinations of event_name, event_descriptive_name, and event_category) and asserts that all <code>event_descriptive_name</code> values are now unique.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>load_indication_data</code> has not been successfully called before this method.</p> <code>AssertionError</code> <p>If, after processing, the <code>event_descriptive_name</code> column still contains duplicate values.</p> Source code in <code>twinweaver/common/data_manager.py</code> <pre><code>def setup_unique_mapping_of_events(self) -&gt; None:\n    \"\"\"\n    Ensures uniqueness of descriptive event names and applies replacements.\n\n    Requires `load_indication_data` to be called first.\n    This method first identifies `event_descriptive_name` values that map to\n    multiple `event_name` values within the same `event_category`. For these\n    non-unique descriptive names, it appends the corresponding `event_name`\n    to make them unique (e.g., \"Measurement\" becomes \"Measurement - Systolic BP\").\n\n    Secondly, it applies predefined or overridden special character replacements\n    (e.g., replacing \"/\" with \" per \" in lab results) to the\n    `event_descriptive_name` column based on the `event_category`.\n\n    Finally, it rebuilds the `self.unique_events` mapping (containing unique\n    combinations of event_name, event_descriptive_name, and event_category)\n    and asserts that all `event_descriptive_name` values are now unique.\n\n    Raises\n    ------\n    ValueError\n        If `load_indication_data` has not been successfully called before\n        this method.\n    AssertionError\n        If, after processing, the `event_descriptive_name` column still\n        contains duplicate values.\n    \"\"\"\n\n    # Check that we already have self.data_frames\n    if not self.data_frames:\n        raise ValueError(\"Data not loaded yet. Please load data first by calling load_indication_data()\")\n\n    # Use config constants for column names\n    event_name_col = self.config.event_name_col\n    event_desc_name_col = self.config.event_descriptive_name_col\n    event_cat_col = self.config.event_category_col\n    events_table_key = self.config.event_table_name\n\n    #: get all unique pairs of event_name and event_descriptive_name in self.data_frames[\"events\"]\n    self.unique_events = self.data_frames[events_table_key]\n    self.unique_events = self.unique_events[[event_name_col, event_desc_name_col, event_cat_col]]\n    self.unique_events = self.unique_events.copy().drop_duplicates()\n    self.unique_events = self.unique_events.reset_index(drop=True)\n\n    #: get all event_descriptive_name that are not unique\n    non_unique_events = self.unique_events[event_desc_name_col].value_counts()\n    non_unique_events = non_unique_events[non_unique_events &gt; 1]\n\n    # Extract corresponding event_name and event_category\n    filtered_events = self.unique_events[event_desc_name_col]\n    non_unique_events = self.unique_events[filtered_events.isin(non_unique_events.index)].copy()\n\n    # create mapping for all non-unique descriptive names, and\n    # then add event_name to those, and apply across entire dataset\n    # Keep temporary column name as string literal\n    non_unique_events[\"new_descriptive_name\"] = (\n        non_unique_events[event_desc_name_col] + \" - \" + non_unique_events[event_name_col]\n    )\n    # Use config constants for column names\n    non_unique_events = non_unique_events[[\"new_descriptive_name\", event_name_col, event_cat_col]]\n\n    self.data_frames[events_table_key] = pd.merge(\n        self.data_frames[events_table_key],\n        non_unique_events,\n        how=\"left\",\n        on=(event_name_col, event_cat_col),\n    )  # Use config constants\n    events_df = self.data_frames[events_table_key]\n    new_desc_name = \"new_descriptive_name\"  # Keep temporary column name as string literal\n    # Use config constant\n    events_df[event_desc_name_col] = events_df[new_desc_name].fillna(events_df[event_desc_name_col])\n    self.data_frames[events_table_key] = self.data_frames[events_table_key].drop(columns=[\"new_descriptive_name\"])\n\n    #: first convert special symbols in event_descriptive_name to alternatives, using self.replace_special_symbols\n    for event_category, (\n        string_to_replace,\n        replacement_string,\n    ) in self.replace_special_symbols:\n        events_df = self.data_frames[events_table_key]\n        # Use config constants\n        category_mask = events_df[event_cat_col] == event_category\n        desc_name_col = event_desc_name_col\n\n        events_df.loc[category_mask, desc_name_col] = (\n            events_df.loc[category_mask, desc_name_col]\n            .astype(str)  # Ensure string type before replace\n            .str.replace(\n                string_to_replace, replacement_string, regex=False\n            )  # Added regex=False for literal replacement\n        )\n\n    #: recalculate self.unique_events and ensure no more non-unique event_descriptive_name\n    # Use config constants\n    cols_to_select = [event_name_col, event_desc_name_col, event_cat_col]\n    self.unique_events = self.data_frames[events_table_key][cols_to_select].copy().drop_duplicates()\n    self.unique_events = self.unique_events.reset_index(drop=True)\n\n    # Assert that all unique now\n    # Use config constant\n    assert len(self.unique_events) == len(self.data_frames[events_table_key][event_desc_name_col].unique())\n</code></pre>"},{"location":"reference/instruction/converter_events/","title":"Converter Events","text":""},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events","title":"twinweaver.instruction.converter_events","text":""},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events-classes","title":"Classes","text":""},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents","title":"ConverterEvents","text":"<p>               Bases: <code>ConverterBase</code></p> <p>Manages the conversion between structured patient event data and formatted strings suitable for Time-To-Event (TTE) forecasting tasks with language models.</p> <p>This class specializes <code>ConverterBase</code> to handle event-based forecasting. It uses specific prompt templates defined in a <code>Config</code> object to generate input prompts (conditioning on a time duration and event) and target strings (describing event occurrence and censoring status). It also provides methods for reverse conversion (parsing model output strings back to structured data) and utility functions for comparing and aggregating potentially noisy model outputs.</p> Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>class ConverterEvents(ConverterBase):\n    \"\"\"\n    Manages the conversion between structured patient event data and formatted\n    strings suitable for Time-To-Event (TTE) forecasting tasks with language models.\n\n    This class specializes `ConverterBase` to handle event-based forecasting.\n    It uses specific prompt templates defined in a `Config` object to generate\n    input prompts (conditioning on a time duration and event) and target strings\n    (describing event occurrence and censoring status). It also provides methods\n    for reverse conversion (parsing model output strings back to structured data)\n    and utility functions for comparing and aggregating potentially noisy model outputs.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Config,\n        constant_description: pd.DataFrame,\n        nr_tokens_budget_total: int,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the ConverterEvents class.\n\n        Sets up the converter with configuration, constant descriptions, token budget,\n        and initializes the tokenizer and specific prompt templates for TTE forecasting\n        tasks using values from the provided Config object.\n\n        Parameters\n        ----------\n        config : Config\n            Configuration object containing settings like tokenizer name, prompt templates\n            (e.g., `forecasting_tte_prompt_start`, `target_prompt_start`), token budget padding, etc.\n        constant_description : pd.DataFrame\n            DataFrame containing descriptions for constant patient features (potentially used\n            in base class or future extensions, currently unused in this subclass's methods).\n        nr_tokens_budget_total : int\n            Total number of tokens budgeted for the input sequence (prompt + context).\n            Used potentially in conjunction with padding settings from config.\n        \"\"\"\n\n        super().__init__(config)\n\n        self.constant_description = constant_description\n        self.nr_tokens_budget_total = nr_tokens_budget_total\n\n        # Use config defaults if overrides are None\n        self.forecasting_prompt_start = self.config.forecasting_tte_prompt_start\n        self.forecasting_prompt_mid = self.config.forecasting_tte_prompt_mid\n        self.forecasting_prompt_end = self.config.forecasting_tte_prompt_end\n        self.forecasting_prompt_summarized_start = self.config.forecasting_prompt_summarized_start\n        self.forecasting_prompt_summarized_genetic = self.config.forecasting_prompt_summarized_genetic\n        self.forecasting_prompt_summarized_lot = self.config.forecasting_prompt_summarized_lot\n\n        self.nr_tokens_budget_padding = self.config.nr_tokens_budget_padding\n        self.always_keep_first_visit = self.config.always_keep_first_visit\n\n    def _generate_target_string(self, patient_split: DataSplitterEventsOption) -&gt; tuple:\n        \"\"\"\n        Generates the target output string and associated metadata for a TTE task.\n\n        Constructs a string describing the outcome of the event being predicted,\n        including whether it was censored and whether it occurred, based on predefined\n        templates from the config object (e.g., `config.target_prompt_start`). Also\n        compiles metadata about the target outcome.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterEventsOption\n            DataSplitterEventsOption containing the data for a single split.\n\n        Returns\n        -------\n        target_str : str\n            The formatted target string (e.g., \"Outcome for event (Event A): Not censored. Event occurred.\\n\").\n        target_meta : dict\n            Metadata dictionary containing details like the raw target string,\n            censoring status (boolean and detail), occurrence status (boolean),\n            target event name/category, relevant dates, and a small DataFrame\n            summarizing the key outcome components ('censoring', 'occurred', 'target_name').\n        \"\"\"\n\n        # This is structured this way to minimize bias\n        # 1. Censoring\n        # 2. Event occurred\n        # This way we can condition the LLM for different scenarios\n\n        #: setup base prompt using config\n        ret_prompt = self.config.target_prompt_start.format(event_name=patient_split.sampled_category_name)\n\n        #: add censoring using config\n        censoring = patient_split.event_censored\n        if censoring is not None:\n            ret_prompt += self.config.target_prompt_censor_true\n            event_occur = None  #: if censored, we don't say whether occurred or not\n        else:\n            ret_prompt += self.config.target_prompt_censor_false\n\n            #: if not censored, add whether occurred or not using config\n            ret_prompt += self.config.target_prompt_before_occur\n            event_occur = patient_split.event_occurred\n            if event_occur is True:\n                ret_prompt += self.config.target_prompt_occur\n            else:\n                ret_prompt += self.config.target_prompt_not_occur\n\n        # Add newline at the end\n        ret_prompt += \"\\n\"\n\n        #: make meta\n        target_meta = {\n            \"target_string\": ret_prompt,\n            \"censoring_detail\": censoring,\n            \"censoring\": censoring is not None,\n            \"occurred\": event_occur,\n            \"split_date_included_in_input\": patient_split.split_date_included_in_input,\n            \"observation_end_date\": patient_split.observation_end_date,\n            \"target_category\": patient_split.sampled_category,\n            \"target_name\": patient_split.sampled_category_name,\n        }\n\n        #: make it as dataframe\n        # Use string constants for column names here as they define the output structure\n        target_meta[\"target_data_processed\"] = pd.DataFrame([target_meta])[[\"censoring\", \"occurred\", \"target_name\"]]\n\n        #: return\n        return ret_prompt, target_meta\n\n    def _generate_prompt(self, patient_split: DataSplitterEventsOption) -&gt; tuple:\n        \"\"\"\n        Generates the input prompt string for a TTE forecasting task.\n\n        Constructs a prompt asking the language model to predict the time until a\n        specific event occurs. It calculates the time difference between the patient's\n        split date (last date included in input) and the actual event date, converts\n        it to weeks if config.delta_time_unit is \"weeks\", rounds it, and formats it into the prompt string using\n        templates from the config (e.g., `self.forecasting_prompt_start`).\n\n        Parameters\n        ----------\n        patient_split : DataSplitterEventsOption\n            DataSplitterEventsOption containing the data for a single split.\n\n        Returns\n        -------\n        prompt_str : str\n            The formatted prompt string, e.g.:\n            \"Predict the time in weeks until event Event A occurs: 12.3 weeks. Input data:\\n\"\n        delta_time_numeric : float\n            The calculated time difference in config.delta_time_unit (numeric, before rounding/formatting).\n        \"\"\"\n\n        #: Get event name descriptive\n        curr_event_name = patient_split.sampled_category_name\n\n        #: get delta in time in config.delta_time_unit, rounded using round_and_strip\n        delta_time_numeric = patient_split.observation_end_date - patient_split.split_date_included_in_input\n\n        delta_time_numeric = delta_time_numeric.days / self._time_divisor\n\n        delta_time = round_and_strip(delta_time_numeric, self.decimal_precision)\n\n        #: construct prompt using config attributes accessed via self\n        ret_prompt = self.forecasting_prompt_start + str(delta_time)\n        ret_prompt += self.forecasting_prompt_mid + curr_event_name\n        ret_prompt += self.forecasting_prompt_end\n\n        #: return\n        return ret_prompt, delta_time_numeric\n\n    def forward_conversion(self, patient_split: DataSplitterEventsOption) -&gt; tuple:\n        \"\"\"\n        Performs the complete forward conversion from structured patient data to prompt/target strings.\n\n        This method orchestrates the generation of both the input prompt and the target\n        output string for a given patient's event prediction scenario, using the\n        `_generate_prompt` and `_generate_target_string` helper methods. It combines\n        the outputs and associated metadata.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterEventsOption\n            DataSplitterEventsOption containing the data for a single split.\n\n        Returns\n        -------\n        prompt_str : str\n            The generated input prompt string.\n        target_str : str\n            The generated target output string.\n        target_meta : dict\n            A metadata dictionary containing combined information from\n            prompt and target generation (including numeric time delta,\n            target details, etc.).\n        \"\"\"\n\n        #: generate target string\n        target_str, target_meta = self._generate_target_string(patient_split)\n\n        #: generate prompt (including when to generate what)\n        prompt_str, delta_time_numeric = self._generate_prompt(patient_split)\n        target_meta[\"delta_time_numeric\"] = delta_time_numeric\n\n        # Return prompt_str, target_str, target_meta (as per function signature hint)\n        return prompt_str, target_str, target_meta\n\n    def forward_conversion_inference(self, patient_split: DataSplitterEventsOption) -&gt; tuple:\n        \"\"\"\n        Performs forward conversion suitable for inference time.\n\n        Generates only the input prompt string and associated metadata, omitting the\n        target string generation. This is useful when preparing input for a model\n        prediction task where the target is unknown or not needed.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterEventsOption\n            DataSplitterEventsOption containing the data for a single split.\n\n        Returns\n        -------\n        prompt_str : str\n            The generated input prompt string.\n        meta : dict\n            The metadata dictionary associated with the prompt generation\n            (contains numeric time delta, target name/category from input, etc.).\n        \"\"\"\n        prompt, _, meta = self.forward_conversion(patient_split)\n        # Return prompt, meta (as per function signature hint)\n        return prompt, meta\n\n    def generate_target_manual(\n        self,\n        target_name: str,\n        event_censored: str,  # Note: type hint was str, assuming it can be None or some indicator\n        event_occurred: bool,\n    ) -&gt; tuple:  # Changed return type hint to tuple based on implementation\n        \"\"\"\n        Manually generates a target string and metadata from specified outcome components.\n\n        This allows creating a target string representation without needing the full\n        `patient_split` dictionary, by directly providing the key outcome details. Useful\n        for testing or specific generation scenarios.\n\n        Parameters\n        ----------\n        target_name : str\n            The descriptive name of the target event.\n        event_censored : str or None\n            The censoring status detail. `None` typically indicates not censored,\n            while a string value might provide details if censored.\n        event_occurred : bool\n            Boolean indicating whether the event occurred.\n\n        Returns\n        -------\n        target_str : str\n            The formatted target string.\n        target_meta : dict\n            The associated metadata dictionary.\n        \"\"\"\n\n        patient_dic = {\n            \"sampled_category_name\": target_name,\n            \"event_censored\": event_censored,\n            \"event_occurred\": event_occurred,\n            \"split_date_included_in_input\": None,\n            \"observation_end_date\": None,\n            \"sampled_category\": None,\n        }\n        # Return type should be tuple as per _generate_target_string\n        return self._generate_target_string(patient_dic)\n\n    def reverse_conversion(self, target_string):\n        \"\"\"\n        Parses a target string to extract structured event outcome information.\n\n        Attempts to reconstruct the censoring status, occurrence status, and target\n        event name from a formatted target string (presumably generated by an LLM or\n        following the format created by `_generate_target_string`). It uses the\n        predefined prompt template strings from the config as delimiters/markers.\n\n        Parameters\n        ----------\n        target_string : str\n            The formatted target string to parse.\n\n        Returns\n        -------\n        pd.DataFrame\n            A single-row DataFrame containing the extracted information with columns:\n            'censoring' (bool or None), 'occurred' (bool or None), 'target_name' (str or None).\n            Returns None for fields that cannot be reliably extracted.\n\n        Raises\n        ------\n        ValueError\n            If no structured data (all fields are None) can be extracted from the string.\n        \"\"\"\n\n        # Initialize the dictionary to store the extracted data\n        # Using string keys as these define the structure of the output DataFrame\n        extracted_data = {\"censoring\": None, \"occurred\": None, \"target_name\": None}\n\n        # Check for sampled_category_name using \"(\" and \")\"\n        if \"(\" in target_string and \")\" in target_string:\n            try:\n                sampled_var_name = target_string.split(\"(\")[1].split(\")\")[0]\n                extracted_data[\"target_name\"] = sampled_var_name\n            except IndexError:\n                # Handle cases where split might fail if format is unexpected\n                pass  # Keep target_name as None\n\n        # Check for censoring information using config constants\n        if self.config.target_prompt_censor_false.strip() in target_string:\n            extracted_data[\"censoring\"] = False\n        elif self.config.target_prompt_censor_true.strip() in target_string:\n            extracted_data[\"censoring\"] = True\n\n        # Check for event occurrence information using config constants\n        # Note: Added check for potential old prompt constant TARGET_PROMPT_OCCUR_OLD\n        # Assuming TARGET_PROMPT_OCCUR_OLD was meant to be handled, added it here.\n        # If TARGET_PROMPT_OCCUR_OLD is not defined or needed, remove the check.\n        if (\n            self.config.target_prompt_occur.strip() in target_string\n        ):  # Removed check for TARGET_PROMPT_OCCUR_OLD as it's not in Config\n            extracted_data[\"occurred\"] = True\n        elif self.config.target_prompt_not_occur.strip() in target_string:\n            extracted_data[\"occurred\"] = False\n\n        # In the case where the model hallucinates the event, make it none\n        # Using hardcoded string as this checks for a specific hallucination pattern\n        if \"did not occur/occurred\" in target_string:\n            extracted_data[\"occurred\"] = None\n\n        # Convert the extracted data to a DataFrame\n        structured_data = pd.DataFrame([extracted_data])\n\n        # Throw error if only nans\n        if structured_data.isna().all().all():\n            raise ValueError(\"No structured data could be extracted from the target string.\")\n\n        return structured_data\n\n    def get_difference_in_event_dataframes(self, df1, df2):\n        \"\"\"\n        Compares two single-row DataFrames representing event outcomes and identifies differences.\n\n        Designed to compare DataFrames generated by `reverse_conversion`. It checks for\n        discrepancies in the 'censoring', 'occurred', and 'target_name' columns between\n        the two DataFrames.\n\n        Parameters\n        ----------\n        df1 : pd.DataFrame\n            The first single-row DataFrame (columns: 'censoring', 'occurred', 'target_name').\n        df2 : pd.DataFrame\n            The second single-row DataFrame (columns: 'censoring', 'occurred', 'target_name').\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the differing values. If the inputs are identical,\n            an empty DataFrame is returned. The output DataFrame has columns like\n            'df1_censoring', 'df2_censoring', etc., showing the differing values side-by-side.\n\n        Raises\n        ------\n        ValueError\n            If input DataFrames are missing expected columns or do not have exactly one row.\n        \"\"\"\n        # Define expected columns using strings, as these relate to the structure created by reverse_conversion\n        cols_to_compare = [\"censoring\", \"occurred\", \"target_name\"]\n\n        # Ensure that the columns are in the same order for both DataFrames\n        try:\n            df1 = df1[cols_to_compare]\n            df2 = df2[cols_to_compare]\n        except KeyError as e:\n            raise ValueError(f\"Input DataFrames are missing expected columns: {e}. Expected: {cols_to_compare}\")\n\n        # Check if both DataFrames have the same shape\n        if df1.shape != df2.shape:\n            # Consider if shape mismatch should raise error or be handled differently (e.g., return info about mismatch)\n            raise ValueError(\"DataFrames do not have the same shape and cannot be compared.\")\n\n        # Find rows that are different\n        # Handle potential NaNs in comparison gracefully\n        diff_mask = (df1.ne(df2) &amp; ~(df1.isna() &amp; df2.isna())).any(axis=1)\n\n        # If there are no differences, return an empty DataFrame\n        if not diff_mask.any():\n            return pd.DataFrame()\n\n        # Create a DataFrame to hold differences, using string keys for new column names\n        differences = pd.DataFrame(\n            {\n                \"df1_censoring\": df1.loc[diff_mask, \"censoring\"],\n                \"df2_censoring\": df2.loc[diff_mask, \"censoring\"],\n                \"df1_occurred\": df1.loc[diff_mask, \"occurred\"],\n                \"df2_occurred\": df2.loc[diff_mask, \"occurred\"],\n                \"df1_target_name\": df1.loc[diff_mask, \"target_name\"],\n                \"df2_target_name\": df2.loc[diff_mask, \"target_name\"],\n            }\n        )\n\n        # Reset index to make it more readable\n        differences.reset_index(drop=True, inplace=True)\n\n        return differences\n\n    def aggregate_multiple_responses(\n        self, responses_dfs: list[pd.DataFrame]\n    ) -&gt; tuple:  # Changed return type hint to tuple\n        \"\"\"\n        Aggregates multiple single-row event outcome DataFrames by majority vote.\n\n        Takes a list of DataFrames (presumably from multiple `reverse_conversion` calls\n        on model outputs for the same input) and determines the most common combination\n        of 'censoring', 'occurred', and 'target_name' values. Ties are broken arbitrarily\n        by `collections.Counter`.\n\n        Parameters\n        ----------\n        responses_dfs : list[pd.DataFrame]\n            A list of single-row pandas DataFrames, each expected to have columns\n            'censoring', 'occurred', and 'target_name'.\n\n        Returns\n        -------\n        ret_df : pd.DataFrame\n            A single-row DataFrame representing the most common response.\n        meta : dict\n            Metadata containing the distribution (percentage) of all unique\n            responses observed, stored under the key 'distribution_of_responses'\n            as a DataFrame.\n\n        Raises\n        ------\n        ValueError\n            If the input list `responses_dfs` is empty or if any DataFrame within the\n            list does not conform to the expected structure (single row, required columns).\n        \"\"\"\n        if not responses_dfs:\n            raise ValueError(\"Input list `responses_dfs` cannot be empty.\")\n\n        # Use string column names consistent with reverse_conversion output\n        original_cols = [\"censoring\", \"occurred\", \"target_name\"]\n        try:\n            # Ensure all DFs have the expected columns before processing\n            responses_as_list = [df[original_cols].values.tolist() for df in responses_dfs]\n        except KeyError as e:\n            raise ValueError(\n                f\"One or more input DataFrames are missing expected columns: {e}. Expected: {original_cols}\"\n            )\n\n        # Flatten list and count occurrences of each unique response tuple\n        # Handle potential nested lists if DataFrames have more than one row (though typically they shouldn't here)\n        element_counts = Counter(tuple(row[0]) for row in responses_as_list if row)  # Ensure row is not empty\n\n        if not element_counts:\n            # This case might occur if all input DFs were empty or had only NaNs that didn't parse correctly.\n            # Return an empty DataFrame or handle as appropriate.\n            # For now, return empty DF and empty meta, but consider logging a warning.\n            empty_df = pd.DataFrame(columns=original_cols)\n            return empty_df, {\"distribution_of_responses\": pd.DataFrame()}\n\n        #: pick the one with highest occurence, or random if equal (Counter.most_common handles ties arbitrarily)\n        most_common_element = element_counts.most_common(1)[0][0]\n\n        #: transform into dictionary using string keys\n        ret_dict = {\n            \"censoring\": most_common_element[0],\n            \"occurred\": most_common_element[1],\n            \"target_name\": most_common_element[2],\n        }\n\n        #: return as dataframe\n        ret_df = pd.DataFrame([ret_dict])\n\n        #: get distribution of responses\n        total_responses = sum(element_counts.values())\n        distribution_dict = {k: round((v / total_responses) * 100, 2) for k, v in element_counts.items()}\n        distribution_list = []\n        for k, v in distribution_dict.items():\n            dist_dict = dict(zip(original_cols, k))\n            dist_dict[\"distribution_percentage\"] = v  # Use string key\n            distribution_list.append(dist_dict)\n\n        distribution_df = pd.DataFrame(distribution_list)\n\n        # Use string key for metadata dictionary\n        meta = {\"distribution_of_responses\": distribution_df}\n\n        return ret_df, meta\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents-functions","title":"Functions","text":""},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents.__init__","title":"__init__","text":"<pre><code>__init__(\n    config, constant_description, nr_tokens_budget_total\n)\n</code></pre> <p>Initializes the ConverterEvents class.</p> <p>Sets up the converter with configuration, constant descriptions, token budget, and initializes the tokenizer and specific prompt templates for TTE forecasting tasks using values from the provided Config object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object containing settings like tokenizer name, prompt templates (e.g., <code>forecasting_tte_prompt_start</code>, <code>target_prompt_start</code>), token budget padding, etc.</p> required <code>constant_description</code> <code>DataFrame</code> <p>DataFrame containing descriptions for constant patient features (potentially used in base class or future extensions, currently unused in this subclass's methods).</p> required <code>nr_tokens_budget_total</code> <code>int</code> <p>Total number of tokens budgeted for the input sequence (prompt + context). Used potentially in conjunction with padding settings from config.</p> required Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n    constant_description: pd.DataFrame,\n    nr_tokens_budget_total: int,\n) -&gt; None:\n    \"\"\"\n    Initializes the ConverterEvents class.\n\n    Sets up the converter with configuration, constant descriptions, token budget,\n    and initializes the tokenizer and specific prompt templates for TTE forecasting\n    tasks using values from the provided Config object.\n\n    Parameters\n    ----------\n    config : Config\n        Configuration object containing settings like tokenizer name, prompt templates\n        (e.g., `forecasting_tte_prompt_start`, `target_prompt_start`), token budget padding, etc.\n    constant_description : pd.DataFrame\n        DataFrame containing descriptions for constant patient features (potentially used\n        in base class or future extensions, currently unused in this subclass's methods).\n    nr_tokens_budget_total : int\n        Total number of tokens budgeted for the input sequence (prompt + context).\n        Used potentially in conjunction with padding settings from config.\n    \"\"\"\n\n    super().__init__(config)\n\n    self.constant_description = constant_description\n    self.nr_tokens_budget_total = nr_tokens_budget_total\n\n    # Use config defaults if overrides are None\n    self.forecasting_prompt_start = self.config.forecasting_tte_prompt_start\n    self.forecasting_prompt_mid = self.config.forecasting_tte_prompt_mid\n    self.forecasting_prompt_end = self.config.forecasting_tte_prompt_end\n    self.forecasting_prompt_summarized_start = self.config.forecasting_prompt_summarized_start\n    self.forecasting_prompt_summarized_genetic = self.config.forecasting_prompt_summarized_genetic\n    self.forecasting_prompt_summarized_lot = self.config.forecasting_prompt_summarized_lot\n\n    self.nr_tokens_budget_padding = self.config.nr_tokens_budget_padding\n    self.always_keep_first_visit = self.config.always_keep_first_visit\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents.aggregate_multiple_responses","title":"aggregate_multiple_responses","text":"<pre><code>aggregate_multiple_responses(responses_dfs)\n</code></pre> <p>Aggregates multiple single-row event outcome DataFrames by majority vote.</p> <p>Takes a list of DataFrames (presumably from multiple <code>reverse_conversion</code> calls on model outputs for the same input) and determines the most common combination of 'censoring', 'occurred', and 'target_name' values. Ties are broken arbitrarily by <code>collections.Counter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>responses_dfs</code> <code>list[DataFrame]</code> <p>A list of single-row pandas DataFrames, each expected to have columns 'censoring', 'occurred', and 'target_name'.</p> required <p>Returns:</p> Name Type Description <code>ret_df</code> <code>DataFrame</code> <p>A single-row DataFrame representing the most common response.</p> <code>meta</code> <code>dict</code> <p>Metadata containing the distribution (percentage) of all unique responses observed, stored under the key 'distribution_of_responses' as a DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input list <code>responses_dfs</code> is empty or if any DataFrame within the list does not conform to the expected structure (single row, required columns).</p> Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>def aggregate_multiple_responses(\n    self, responses_dfs: list[pd.DataFrame]\n) -&gt; tuple:  # Changed return type hint to tuple\n    \"\"\"\n    Aggregates multiple single-row event outcome DataFrames by majority vote.\n\n    Takes a list of DataFrames (presumably from multiple `reverse_conversion` calls\n    on model outputs for the same input) and determines the most common combination\n    of 'censoring', 'occurred', and 'target_name' values. Ties are broken arbitrarily\n    by `collections.Counter`.\n\n    Parameters\n    ----------\n    responses_dfs : list[pd.DataFrame]\n        A list of single-row pandas DataFrames, each expected to have columns\n        'censoring', 'occurred', and 'target_name'.\n\n    Returns\n    -------\n    ret_df : pd.DataFrame\n        A single-row DataFrame representing the most common response.\n    meta : dict\n        Metadata containing the distribution (percentage) of all unique\n        responses observed, stored under the key 'distribution_of_responses'\n        as a DataFrame.\n\n    Raises\n    ------\n    ValueError\n        If the input list `responses_dfs` is empty or if any DataFrame within the\n        list does not conform to the expected structure (single row, required columns).\n    \"\"\"\n    if not responses_dfs:\n        raise ValueError(\"Input list `responses_dfs` cannot be empty.\")\n\n    # Use string column names consistent with reverse_conversion output\n    original_cols = [\"censoring\", \"occurred\", \"target_name\"]\n    try:\n        # Ensure all DFs have the expected columns before processing\n        responses_as_list = [df[original_cols].values.tolist() for df in responses_dfs]\n    except KeyError as e:\n        raise ValueError(\n            f\"One or more input DataFrames are missing expected columns: {e}. Expected: {original_cols}\"\n        )\n\n    # Flatten list and count occurrences of each unique response tuple\n    # Handle potential nested lists if DataFrames have more than one row (though typically they shouldn't here)\n    element_counts = Counter(tuple(row[0]) for row in responses_as_list if row)  # Ensure row is not empty\n\n    if not element_counts:\n        # This case might occur if all input DFs were empty or had only NaNs that didn't parse correctly.\n        # Return an empty DataFrame or handle as appropriate.\n        # For now, return empty DF and empty meta, but consider logging a warning.\n        empty_df = pd.DataFrame(columns=original_cols)\n        return empty_df, {\"distribution_of_responses\": pd.DataFrame()}\n\n    #: pick the one with highest occurence, or random if equal (Counter.most_common handles ties arbitrarily)\n    most_common_element = element_counts.most_common(1)[0][0]\n\n    #: transform into dictionary using string keys\n    ret_dict = {\n        \"censoring\": most_common_element[0],\n        \"occurred\": most_common_element[1],\n        \"target_name\": most_common_element[2],\n    }\n\n    #: return as dataframe\n    ret_df = pd.DataFrame([ret_dict])\n\n    #: get distribution of responses\n    total_responses = sum(element_counts.values())\n    distribution_dict = {k: round((v / total_responses) * 100, 2) for k, v in element_counts.items()}\n    distribution_list = []\n    for k, v in distribution_dict.items():\n        dist_dict = dict(zip(original_cols, k))\n        dist_dict[\"distribution_percentage\"] = v  # Use string key\n        distribution_list.append(dist_dict)\n\n    distribution_df = pd.DataFrame(distribution_list)\n\n    # Use string key for metadata dictionary\n    meta = {\"distribution_of_responses\": distribution_df}\n\n    return ret_df, meta\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents.forward_conversion","title":"forward_conversion","text":"<pre><code>forward_conversion(patient_split)\n</code></pre> <p>Performs the complete forward conversion from structured patient data to prompt/target strings.</p> <p>This method orchestrates the generation of both the input prompt and the target output string for a given patient's event prediction scenario, using the <code>_generate_prompt</code> and <code>_generate_target_string</code> helper methods. It combines the outputs and associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>patient_split</code> <code>DataSplitterEventsOption</code> <p>DataSplitterEventsOption containing the data for a single split.</p> required <p>Returns:</p> Name Type Description <code>prompt_str</code> <code>str</code> <p>The generated input prompt string.</p> <code>target_str</code> <code>str</code> <p>The generated target output string.</p> <code>target_meta</code> <code>dict</code> <p>A metadata dictionary containing combined information from prompt and target generation (including numeric time delta, target details, etc.).</p> Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>def forward_conversion(self, patient_split: DataSplitterEventsOption) -&gt; tuple:\n    \"\"\"\n    Performs the complete forward conversion from structured patient data to prompt/target strings.\n\n    This method orchestrates the generation of both the input prompt and the target\n    output string for a given patient's event prediction scenario, using the\n    `_generate_prompt` and `_generate_target_string` helper methods. It combines\n    the outputs and associated metadata.\n\n    Parameters\n    ----------\n    patient_split : DataSplitterEventsOption\n        DataSplitterEventsOption containing the data for a single split.\n\n    Returns\n    -------\n    prompt_str : str\n        The generated input prompt string.\n    target_str : str\n        The generated target output string.\n    target_meta : dict\n        A metadata dictionary containing combined information from\n        prompt and target generation (including numeric time delta,\n        target details, etc.).\n    \"\"\"\n\n    #: generate target string\n    target_str, target_meta = self._generate_target_string(patient_split)\n\n    #: generate prompt (including when to generate what)\n    prompt_str, delta_time_numeric = self._generate_prompt(patient_split)\n    target_meta[\"delta_time_numeric\"] = delta_time_numeric\n\n    # Return prompt_str, target_str, target_meta (as per function signature hint)\n    return prompt_str, target_str, target_meta\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents.forward_conversion_inference","title":"forward_conversion_inference","text":"<pre><code>forward_conversion_inference(patient_split)\n</code></pre> <p>Performs forward conversion suitable for inference time.</p> <p>Generates only the input prompt string and associated metadata, omitting the target string generation. This is useful when preparing input for a model prediction task where the target is unknown or not needed.</p> <p>Parameters:</p> Name Type Description Default <code>patient_split</code> <code>DataSplitterEventsOption</code> <p>DataSplitterEventsOption containing the data for a single split.</p> required <p>Returns:</p> Name Type Description <code>prompt_str</code> <code>str</code> <p>The generated input prompt string.</p> <code>meta</code> <code>dict</code> <p>The metadata dictionary associated with the prompt generation (contains numeric time delta, target name/category from input, etc.).</p> Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>def forward_conversion_inference(self, patient_split: DataSplitterEventsOption) -&gt; tuple:\n    \"\"\"\n    Performs forward conversion suitable for inference time.\n\n    Generates only the input prompt string and associated metadata, omitting the\n    target string generation. This is useful when preparing input for a model\n    prediction task where the target is unknown or not needed.\n\n    Parameters\n    ----------\n    patient_split : DataSplitterEventsOption\n        DataSplitterEventsOption containing the data for a single split.\n\n    Returns\n    -------\n    prompt_str : str\n        The generated input prompt string.\n    meta : dict\n        The metadata dictionary associated with the prompt generation\n        (contains numeric time delta, target name/category from input, etc.).\n    \"\"\"\n    prompt, _, meta = self.forward_conversion(patient_split)\n    # Return prompt, meta (as per function signature hint)\n    return prompt, meta\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents.generate_target_manual","title":"generate_target_manual","text":"<pre><code>generate_target_manual(\n    target_name, event_censored, event_occurred\n)\n</code></pre> <p>Manually generates a target string and metadata from specified outcome components.</p> <p>This allows creating a target string representation without needing the full <code>patient_split</code> dictionary, by directly providing the key outcome details. Useful for testing or specific generation scenarios.</p> <p>Parameters:</p> Name Type Description Default <code>target_name</code> <code>str</code> <p>The descriptive name of the target event.</p> required <code>event_censored</code> <code>str or None</code> <p>The censoring status detail. <code>None</code> typically indicates not censored, while a string value might provide details if censored.</p> required <code>event_occurred</code> <code>bool</code> <p>Boolean indicating whether the event occurred.</p> required <p>Returns:</p> Name Type Description <code>target_str</code> <code>str</code> <p>The formatted target string.</p> <code>target_meta</code> <code>dict</code> <p>The associated metadata dictionary.</p> Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>def generate_target_manual(\n    self,\n    target_name: str,\n    event_censored: str,  # Note: type hint was str, assuming it can be None or some indicator\n    event_occurred: bool,\n) -&gt; tuple:  # Changed return type hint to tuple based on implementation\n    \"\"\"\n    Manually generates a target string and metadata from specified outcome components.\n\n    This allows creating a target string representation without needing the full\n    `patient_split` dictionary, by directly providing the key outcome details. Useful\n    for testing or specific generation scenarios.\n\n    Parameters\n    ----------\n    target_name : str\n        The descriptive name of the target event.\n    event_censored : str or None\n        The censoring status detail. `None` typically indicates not censored,\n        while a string value might provide details if censored.\n    event_occurred : bool\n        Boolean indicating whether the event occurred.\n\n    Returns\n    -------\n    target_str : str\n        The formatted target string.\n    target_meta : dict\n        The associated metadata dictionary.\n    \"\"\"\n\n    patient_dic = {\n        \"sampled_category_name\": target_name,\n        \"event_censored\": event_censored,\n        \"event_occurred\": event_occurred,\n        \"split_date_included_in_input\": None,\n        \"observation_end_date\": None,\n        \"sampled_category\": None,\n    }\n    # Return type should be tuple as per _generate_target_string\n    return self._generate_target_string(patient_dic)\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents.get_difference_in_event_dataframes","title":"get_difference_in_event_dataframes","text":"<pre><code>get_difference_in_event_dataframes(df1, df2)\n</code></pre> <p>Compares two single-row DataFrames representing event outcomes and identifies differences.</p> <p>Designed to compare DataFrames generated by <code>reverse_conversion</code>. It checks for discrepancies in the 'censoring', 'occurred', and 'target_name' columns between the two DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>DataFrame</code> <p>The first single-row DataFrame (columns: 'censoring', 'occurred', 'target_name').</p> required <code>df2</code> <code>DataFrame</code> <p>The second single-row DataFrame (columns: 'censoring', 'occurred', 'target_name').</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the differing values. If the inputs are identical, an empty DataFrame is returned. The output DataFrame has columns like 'df1_censoring', 'df2_censoring', etc., showing the differing values side-by-side.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input DataFrames are missing expected columns or do not have exactly one row.</p> Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>def get_difference_in_event_dataframes(self, df1, df2):\n    \"\"\"\n    Compares two single-row DataFrames representing event outcomes and identifies differences.\n\n    Designed to compare DataFrames generated by `reverse_conversion`. It checks for\n    discrepancies in the 'censoring', 'occurred', and 'target_name' columns between\n    the two DataFrames.\n\n    Parameters\n    ----------\n    df1 : pd.DataFrame\n        The first single-row DataFrame (columns: 'censoring', 'occurred', 'target_name').\n    df2 : pd.DataFrame\n        The second single-row DataFrame (columns: 'censoring', 'occurred', 'target_name').\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the differing values. If the inputs are identical,\n        an empty DataFrame is returned. The output DataFrame has columns like\n        'df1_censoring', 'df2_censoring', etc., showing the differing values side-by-side.\n\n    Raises\n    ------\n    ValueError\n        If input DataFrames are missing expected columns or do not have exactly one row.\n    \"\"\"\n    # Define expected columns using strings, as these relate to the structure created by reverse_conversion\n    cols_to_compare = [\"censoring\", \"occurred\", \"target_name\"]\n\n    # Ensure that the columns are in the same order for both DataFrames\n    try:\n        df1 = df1[cols_to_compare]\n        df2 = df2[cols_to_compare]\n    except KeyError as e:\n        raise ValueError(f\"Input DataFrames are missing expected columns: {e}. Expected: {cols_to_compare}\")\n\n    # Check if both DataFrames have the same shape\n    if df1.shape != df2.shape:\n        # Consider if shape mismatch should raise error or be handled differently (e.g., return info about mismatch)\n        raise ValueError(\"DataFrames do not have the same shape and cannot be compared.\")\n\n    # Find rows that are different\n    # Handle potential NaNs in comparison gracefully\n    diff_mask = (df1.ne(df2) &amp; ~(df1.isna() &amp; df2.isna())).any(axis=1)\n\n    # If there are no differences, return an empty DataFrame\n    if not diff_mask.any():\n        return pd.DataFrame()\n\n    # Create a DataFrame to hold differences, using string keys for new column names\n    differences = pd.DataFrame(\n        {\n            \"df1_censoring\": df1.loc[diff_mask, \"censoring\"],\n            \"df2_censoring\": df2.loc[diff_mask, \"censoring\"],\n            \"df1_occurred\": df1.loc[diff_mask, \"occurred\"],\n            \"df2_occurred\": df2.loc[diff_mask, \"occurred\"],\n            \"df1_target_name\": df1.loc[diff_mask, \"target_name\"],\n            \"df2_target_name\": df2.loc[diff_mask, \"target_name\"],\n        }\n    )\n\n    # Reset index to make it more readable\n    differences.reset_index(drop=True, inplace=True)\n\n    return differences\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events.ConverterEvents.reverse_conversion","title":"reverse_conversion","text":"<pre><code>reverse_conversion(target_string)\n</code></pre> <p>Parses a target string to extract structured event outcome information.</p> <p>Attempts to reconstruct the censoring status, occurrence status, and target event name from a formatted target string (presumably generated by an LLM or following the format created by <code>_generate_target_string</code>). It uses the predefined prompt template strings from the config as delimiters/markers.</p> <p>Parameters:</p> Name Type Description Default <code>target_string</code> <code>str</code> <p>The formatted target string to parse.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single-row DataFrame containing the extracted information with columns: 'censoring' (bool or None), 'occurred' (bool or None), 'target_name' (str or None). Returns None for fields that cannot be reliably extracted.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no structured data (all fields are None) can be extracted from the string.</p> Source code in <code>twinweaver/instruction/converter_events.py</code> <pre><code>def reverse_conversion(self, target_string):\n    \"\"\"\n    Parses a target string to extract structured event outcome information.\n\n    Attempts to reconstruct the censoring status, occurrence status, and target\n    event name from a formatted target string (presumably generated by an LLM or\n    following the format created by `_generate_target_string`). It uses the\n    predefined prompt template strings from the config as delimiters/markers.\n\n    Parameters\n    ----------\n    target_string : str\n        The formatted target string to parse.\n\n    Returns\n    -------\n    pd.DataFrame\n        A single-row DataFrame containing the extracted information with columns:\n        'censoring' (bool or None), 'occurred' (bool or None), 'target_name' (str or None).\n        Returns None for fields that cannot be reliably extracted.\n\n    Raises\n    ------\n    ValueError\n        If no structured data (all fields are None) can be extracted from the string.\n    \"\"\"\n\n    # Initialize the dictionary to store the extracted data\n    # Using string keys as these define the structure of the output DataFrame\n    extracted_data = {\"censoring\": None, \"occurred\": None, \"target_name\": None}\n\n    # Check for sampled_category_name using \"(\" and \")\"\n    if \"(\" in target_string and \")\" in target_string:\n        try:\n            sampled_var_name = target_string.split(\"(\")[1].split(\")\")[0]\n            extracted_data[\"target_name\"] = sampled_var_name\n        except IndexError:\n            # Handle cases where split might fail if format is unexpected\n            pass  # Keep target_name as None\n\n    # Check for censoring information using config constants\n    if self.config.target_prompt_censor_false.strip() in target_string:\n        extracted_data[\"censoring\"] = False\n    elif self.config.target_prompt_censor_true.strip() in target_string:\n        extracted_data[\"censoring\"] = True\n\n    # Check for event occurrence information using config constants\n    # Note: Added check for potential old prompt constant TARGET_PROMPT_OCCUR_OLD\n    # Assuming TARGET_PROMPT_OCCUR_OLD was meant to be handled, added it here.\n    # If TARGET_PROMPT_OCCUR_OLD is not defined or needed, remove the check.\n    if (\n        self.config.target_prompt_occur.strip() in target_string\n    ):  # Removed check for TARGET_PROMPT_OCCUR_OLD as it's not in Config\n        extracted_data[\"occurred\"] = True\n    elif self.config.target_prompt_not_occur.strip() in target_string:\n        extracted_data[\"occurred\"] = False\n\n    # In the case where the model hallucinates the event, make it none\n    # Using hardcoded string as this checks for a specific hallucination pattern\n    if \"did not occur/occurred\" in target_string:\n        extracted_data[\"occurred\"] = None\n\n    # Convert the extracted data to a DataFrame\n    structured_data = pd.DataFrame([extracted_data])\n\n    # Throw error if only nans\n    if structured_data.isna().all().all():\n        raise ValueError(\"No structured data could be extracted from the target string.\")\n\n    return structured_data\n</code></pre>"},{"location":"reference/instruction/converter_events/#twinweaver.instruction.converter_events-functions","title":"Functions","text":""},{"location":"reference/instruction/converter_forecasting/","title":"Converter Forecasting","text":""},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting","title":"twinweaver.instruction.converter_forecasting","text":""},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting-classes","title":"Classes","text":""},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting","title":"ConverterForecasting","text":"<p>               Bases: <code>ConverterBase</code></p> <p>Handles the conversion between structured patient data splits and text-based formats suitable for forecasting language models.</p> <p>This class focuses specifically on generating prompts that ask a model to predict future values of specific variables (e.g., lab results) at given future time points (days/weeks relative to a split date). It also formats the actual future data (target) into a corresponding text string and handles the reverse conversion from model-generated text back to a structured DataFrame. Warning: variables including \"weeks\" can also mean days if configured so.</p> <p>Attributes:</p> Name Type Description <code>constant_description</code> <code>DataFrame</code> <p>DataFrame holding descriptions for constant variables.</p> <code>nr_tokens_budget_total</code> <code>int</code> <p>The target token budget for the combined input and output strings.</p> <code>forecasting_prompt_start</code> <code>str</code> <p>The initial text segment of the forecasting prompt.</p> <code>forecasting_prompt_var_time</code> <code>str</code> <p>The text segment used in the prompt to link a variable to its prediction times.</p> <code>forecasting_prompt_summarized_start</code> <code>str</code> <p>Starting text for summarized prompts (not used in core methods here).</p> <code>forecasting_prompt_summarized_genetic</code> <code>str</code> <p>Text for genetic info in summarized prompts (not used here).</p> <code>forecasting_prompt_summarized_lot</code> <code>str</code> <p>Text for LoT info in summarized prompts (not used here).</p> <code>tokenizer</code> <code>AutoTokenizer</code> <p>Tokenizer instance for calculating token counts.</p> <code>nr_tokens_budget_padding</code> <code>int</code> <p>Padding added to token budget calculations.</p> <code>always_keep_first_visit</code> <code>bool</code> <p>Flag indicating if the first visit's data should always be kept during token budget trimming.</p> Source code in <code>twinweaver/instruction/converter_forecasting.py</code> <pre><code>class ConverterForecasting(ConverterBase):\n    \"\"\"\n    Handles the conversion between structured patient data splits and\n    text-based formats suitable for forecasting language models.\n\n    This class focuses specifically on generating prompts that ask a model\n    to predict future values of specific variables (e.g., lab results) at\n    given future time points (days/weeks relative to a split date). It also\n    formats the actual future data (target) into a corresponding text string\n    and handles the reverse conversion from model-generated text back to a\n    structured DataFrame.\n    Warning: variables including \"weeks\" can also mean days if configured so.\n\n    Attributes\n    ----------\n    constant_description : pd.DataFrame\n        DataFrame holding descriptions for constant variables.\n    nr_tokens_budget_total : int\n        The target token budget for the combined input and output strings.\n    forecasting_prompt_start : str\n        The initial text segment of the forecasting prompt.\n    forecasting_prompt_var_time : str\n        The text segment used in the prompt to link a variable to its prediction\n        times.\n    forecasting_prompt_summarized_start : str\n        Starting text for summarized prompts (not used in core methods here).\n    forecasting_prompt_summarized_genetic : str\n        Text for genetic info in summarized prompts (not used here).\n    forecasting_prompt_summarized_lot : str\n        Text for LoT info in summarized prompts (not used here).\n    tokenizer : AutoTokenizer\n        Tokenizer instance for calculating token counts.\n    nr_tokens_budget_padding : int\n        Padding added to token budget calculations.\n    always_keep_first_visit : bool\n        Flag indicating if the first visit's data should always be kept during token\n        budget trimming.\n    \"\"\"\n\n    def __init__(\n        self,\n        constant_description: pd.DataFrame,\n        nr_tokens_budget_total: int,\n        config: Config,\n        dm: DataManager,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the ConverterForecasting instance.\n\n        Sets up the converter with necessary configurations, including descriptions\n        for constant patient features, the total token budget for generated text,\n        and various prompt templates defined in the Config object.\n\n        Parameters\n        ----------\n        constant_description : pd.DataFrame\n            DataFrame containing descriptions for constant\n            patient attributes (e.g., 'Sex: Male'). Used potentially by base\n            class methods for input string generation.\n        nr_tokens_budget_total : int\n            The target maximum number of tokens for the\n            combined input (history) and output (forecast) text.\n        config : Config\n            A Config object containing shared configuration settings like\n            prompt templates, column names, tokenizer details, etc.\n        dm : DataManager\n            DataManager object containing the variable types and data frames.\n        \"\"\"\n        # Initialize base class with config and potential overrides\n        super().__init__(config)\n\n        # Store specific attributes for this class\n        self.constant_description = constant_description\n        self.nr_tokens_budget_total = nr_tokens_budget_total\n        self.dm = dm\n\n        # Set forecasting prompts, using overrides or config defaults\n        self.forecasting_prompt_start = self.config.forecasting_fval_prompt_start\n        self.forecasting_prompt_var_time = self.config.forecasting_prompt_var_time\n        self.forecasting_prompt_summarized_start = self.config.forecasting_prompt_summarized_start\n        self.forecasting_prompt_summarized_genetic = self.config.forecasting_prompt_summarized_genetic\n        self.forecasting_prompt_summarized_lot = self.config.forecasting_prompt_summarized_lot\n\n        # Initialize tokenizer and budget padding\n        self.nr_tokens_budget_padding = self.config.nr_tokens_budget_padding\n        self.always_keep_first_visit = self.config.always_keep_first_visit\n\n    def _generate_target_string(self, patient_split: DataSplitterForecastingOption) -&gt; tuple[str, dict]:\n        \"\"\"\n        Generates the target forecast string and associated metadata.\n\n        Takes a patient data split containing future events, processes them,\n        and formats them into a structured text string representing the forecast\n        target. It also computes metadata detailing which variables are forecasted\n        at which future days/weeks relative to the split date, and includes the raw\n        and processed target data.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterForecastingOption\n            A DataSplitterForecastingOption containing the data for a single split.\n\n        Returns\n        -------\n        target_str : str\n            The formatted string representing the target forecast\n            (e.g., \"[2 weeks later]\\\\n\\\\tLab X: 10.5\\\\n[4 weeks later]\\\\n\\\\tLab Y: 2.1\").\n        target_meta : dict\n            A dictionary containing metadata about the target,\n            including: 'dates_to_forecast', 'future_weeks_to_forecast',\n            'dates_per_variable', 'future_prediction_time_per_variable',\n            'variable_name_mapping', 'target_data_raw', 'target_data_processed',\n            'lot_date', 'last_observed_values' (DataFrame of last known values\n            for forecasted variables from the input history).\n        \"\"\"\n\n        #: preprocess:\n        target_data = patient_split.target_events_after_split.copy()\n        target_cleaned = self._preprocess_events(target_data.copy())\n\n        #: get delta between split and first target\n        target_first_day = target_cleaned[self.config.date_col].min()\n        split_date = patient_split.split_date_included_in_input\n        delta_days = (target_first_day - split_date).days / self._time_divisor\n\n        #: convert to string using default approach\n        target_str = self._get_event_string(\n            target_cleaned,\n            events_delta_0=delta_days,\n            use_accumulative_dates=False,\n            add_first_day_preamble=False,\n        )\n\n        #: get meta, i.e. which dates to forecast and the relative future days/weeks grouped by variable\n        dates_per_variable = target_cleaned.groupby(self.config.event_name_col)[self.config.date_col].unique()\n        future_prediction_time_per_variable = {}\n        variable_name_mapping = {}\n        for variable, dates in dates_per_variable.items():\n            # Get days/weeks to predict (relative to split date)\n            # Ensure dates is a pd.Series or pd.DatetimeIndex for subtraction\n            if not isinstance(dates, (pd.Series, pd.DatetimeIndex)):\n                dates = pd.to_datetime(dates)\n            future_prediction_time_per_variable[variable] = (dates - split_date).days / self._time_divisor\n\n            # Get descriptive name\n            curr_var = target_cleaned[target_cleaned[self.config.event_name_col] == variable]\n            # Handle case where variable might not be found or has no descriptive name\n            if not curr_var.empty and self.config.event_descriptive_name_col in curr_var.columns:\n                descriptive_name = curr_var[self.config.event_descriptive_name_col].iloc[0]\n            else:\n                descriptive_name = variable  # Fallback to variable name itself\n            variable_name_mapping[variable] = descriptive_name\n\n        dates_to_forecast = target_cleaned[self.config.date_col].unique()\n        # Ensure dates_to_forecast is pd.DatetimeIndex for subtraction\n        if not isinstance(dates_to_forecast, pd.DatetimeIndex):\n            dates_to_forecast = pd.to_datetime(dates_to_forecast)\n        future_weeks_to_forecast = (dates_to_forecast - split_date).days / self._time_divisor\n\n        #: add last observed values of each variable from input history\n        input_history = patient_split.events_until_split\n        last_observed_values_dict = {}\n        for variable in future_prediction_time_per_variable.keys():\n            # Check if variable is in input history by event_name\n            matches = input_history[input_history[self.config.event_name_col] == variable]\n            if not matches.empty:\n                last_observed = matches.sort_values(by=self.config.date_col).iloc[-1]\n            else:\n                # Fallback to looking at descriptive name if event_name not found\n                descriptive_name_to_match = variable_name_mapping.get(variable, variable)\n                matches = input_history[\n                    input_history[self.config.event_descriptive_name_col] == descriptive_name_to_match\n                ]\n                if not matches.empty:\n                    last_observed = matches.sort_values(by=self.config.date_col).iloc[-1]\n                else:\n                    last_observed = None  # Variable not found in history\n\n            #: add to dict if found\n            if last_observed is not None:\n                last_observed_values_dict[variable] = last_observed\n\n        # Convert dict of Series to DataFrame\n        if last_observed_values_dict:\n            last_observed_values = pd.DataFrame(last_observed_values_dict).T\n        else:\n            # Create an empty DataFrame with expected columns if nothing was observed\n            expected_cols = [\n                self.config.date_col,\n                self.config.event_category_col,\n                self.config.event_name_col,\n                self.config.event_descriptive_name_col,\n                self.config.event_value_col,\n                self.config.source_col,\n            ]\n            last_observed_values = pd.DataFrame(columns=expected_cols)\n\n        #: setup return metadata dictionary\n        target_meta = {\n            \"dates_to_forecast\": dates_to_forecast,\n            \"future_weeks_to_forecast\": future_weeks_to_forecast,\n            \"dates_per_variable\": dates_per_variable,\n            \"future_prediction_time_per_variable\": future_prediction_time_per_variable,\n            \"variable_name_mapping\": variable_name_mapping,\n            \"target_data_raw\": target_data,\n            \"target_data_processed\": target_cleaned,\n            \"lot_date\": patient_split.lot_date,\n            \"last_observed_values\": last_observed_values,\n        }\n\n        return target_str, target_meta\n\n    def _generate_prompt(self, target_meta: dict) -&gt; str:\n        \"\"\"\n        Generates the forecasting prompt based on target metadata.\n\n        Constructs the text prompt that instructs the language model what to\n        forecast. It uses the metadata about which variables need prediction\n        at which future days/weeks.\n\n        Parameters\n        ----------\n        target_meta : dict\n            A dictionary containing metadata about the target forecast,\n            as generated by `_generate_target_string`. Key required elements are:\n            - \"future_prediction_time_per_variable\": Dict mapping variable names to lists\n              of future days/weeks (relative to split date) they should be predicted at.\n            - \"variable_name_mapping\": Dict mapping internal variable names to\n              their descriptive names for use in the prompt.\n\n        Returns\n        -------\n        str\n            The fully constructed prompt string asking the model to perform the\n            specified forecast.\n        \"\"\"\n\n        # start ret\n        ret_prompt = \"\"\n\n        #: make sure to round days/weeks from future_prediction_time_per_variable to predict to specified precision\n        future_prediction_time_per_variable = target_meta[\"future_prediction_time_per_variable\"]\n        future_prediction_time_per_variable_rounded = {}\n        for k, v in future_prediction_time_per_variable.items():\n            # Check if v is iterable, handle potential scalar values if necessary\n            if hasattr(v, \"__iter__\"):\n                future_prediction_time_per_variable_rounded[k] = [\n                    round_and_strip(v2, self.decimal_precision) for v2 in v\n                ]\n            else:\n                # Handle non-iterable case, maybe log a warning or convert to list\n                future_prediction_time_per_variable_rounded[k] = [round_and_strip(v, self.decimal_precision)]\n\n        #: add base prompt start text\n        ret_prompt += self.forecasting_prompt_start\n\n        #: sort alphabetically by variable name for consistent prompt order\n        future_prediction_time_per_variable_sorted = dict(\n            sorted(future_prediction_time_per_variable_rounded.items(), key=lambda item: item[0])\n        )\n\n        #: create prompt for which variables to predict and when\n        for variable, weeks in future_prediction_time_per_variable_sorted.items():\n            #: need event_descriptive_name from mapping\n            variable_desc_name = target_meta[\"variable_name_mapping\"].get(\n                variable, variable\n            )  # Fallback to variable name\n\n            # Create prompt line for this variable\n            ret_prompt += \"\\n\" + \"\\t\" + variable_desc_name\n            ret_prompt += self.forecasting_prompt_var_time\n            # Format the list of days/weeks nicely\n            ret_prompt += \", \".join(map(str, weeks))  # Use map for clean conversion to string\n\n        #: return the fully constructed prompt\n        return ret_prompt\n\n    def forward_conversion(self, patient_split: DataSplitterForecastingOption) -&gt; tuple[str, str, dict]:\n        \"\"\"\n        Performs the primary conversion from a data split to prompt and target strings.\n\n        This method orchestrates the generation of the target string (what the model\n        should predict) and the corresponding prompt string (the instruction asking\n        for the prediction) based on a patient data split.\n\n        Note: This method generates the *prompt* and *target* strings. The generation\n        of the *input* string (patient history) is typically handled separately, often\n        by the base class's `_get_input_string` method, using the \"events_until_split\"\n        and \"constant_data\" from the `patient_split`.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterForecastingOption\n            DataSplitterForecastingOption containing the data for a single split.\n\n        Returns\n        -------\n        prompt_str : str\n            The generated prompt instructing the model on the\n            forecasting task.\n        target_str : str\n            The formatted string representing the target forecast values.\n        target_meta : dict\n            Metadata associated with the generated target string\n            (output from `_generate_target_string`).\n        \"\"\"\n\n        #: generate target string and associated metadata\n        target_str, target_meta = self._generate_target_string(patient_split)\n\n        #: generate prompt string based on the target metadata\n        prompt_str = self._generate_prompt(target_meta)\n\n        # Note: The input string (patient history text) generation is handled by the base class's\n        # `_get_input_string` method, which would typically be called elsewhere\n        # in the pipeline that uses this forward_conversion result along with the input events.\n        # This method focuses on generating the *prompt* and *target* based on the split.\n\n        return prompt_str, target_str, target_meta\n\n    def forward_conversion_inference(\n        self, patient_split: DataSplitterForecastingOption, future_weeks_per_variable: dict\n    ) -&gt; tuple[str, dict]:\n        \"\"\"\n        Generates only the prompt string for inference scenarios.\n\n        Used when the goal is to get a prediction from the model, not for training.\n        It takes the patient's historical data and a specific set of variables and\n        future days/weeks to predict, then constructs the appropriate prompt string.\n        It does not generate a target string, as the target is what the model\n        is expected to produce.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterForecastingOption\n            DataSplitterForecastingOption containing the data for a single split.\n        future_weeks_per_variable : dict\n            Dictionary explicitly defining the forecasting task.\n            Format: {&lt;variable_name&gt;: [&lt;future_week_1&gt;, &lt;future_week_2&gt;, ...], ...}\n            These weeks are relative to the `split_date_included_in_input`.\n\n        Returns\n        -------\n        prompt_str : str\n            The generated prompt for the inference task.\n        target_pseudo_meta : dict\n            A dictionary containing the metadata constructed\n            specifically for generating this inference prompt (includes the provided\n            `future_prediction_time_per_variable`, derived `dates_per_variable`, and looked-up\n            `variable_name_mapping`).\n        \"\"\"\n\n        #: generate target_pseudo meta data needed for prompt generation\n        target_pseudo_meta = {}\n\n        #: Use the provided future_weeks_per_variable directly\n        target_pseudo_meta[\"future_prediction_time_per_variable\"] = future_weeks_per_variable\n\n        # Make dates per variable based on the provided days/weeks and split date\n        dates_per_variable = {}\n        split_date = patient_split.split_date_included_in_input\n        for variable, weeks in future_weeks_per_variable.items():\n            # Ensure weeks is iterable\n            if not hasattr(weeks, \"__iter__\"):\n                weeks = [weeks]\n            dates_per_variable[variable] = [\n                split_date + pd.Timedelta(days=float(w) * self._time_divisor) for w in weeks\n            ]\n        target_pseudo_meta[\"dates_per_variable\"] = dates_per_variable\n\n        #: make target_meta[\"variable_name_mapping\"] by looking up in input events\n        variable_name_mapping = {}\n        input_events = patient_split.events_until_split\n        for variable in future_weeks_per_variable.keys():\n            # Get descriptive name from input history\n            curr_var = input_events[input_events[self.config.event_name_col] == variable]\n            if not curr_var.empty and self.config.event_descriptive_name_col in curr_var.columns:\n                descriptive_name = curr_var[self.config.event_descriptive_name_col].iloc[0]\n            else:\n                # Fallback: If not found by event_name, try descriptive name directly,\n                # or just use the variable name itself. This part might need refinement\n                # depending on how variables are guaranteed to be present/identifiable.\n                descriptive_name = variable  # Simple fallback\n            variable_name_mapping[variable] = descriptive_name\n\n        target_pseudo_meta[\"variable_name_mapping\"] = variable_name_mapping\n\n        #: generate prompt (including when to generate what) based on constructed metadata\n        prompt_str = self._generate_prompt(target_pseudo_meta)\n\n        return prompt_str, target_pseudo_meta\n\n    def generate_target_manual(\n        self,\n        target_events_after_split: pd.DataFrame,\n        split_date_included_in_input: datetime,\n        events_until_split: pd.DataFrame,\n        lot_date: datetime = None,\n    ) -&gt; tuple:  # Added optional lot_date param\n        \"\"\"\n        Manually generates the target string and metadata from explicit components.\n\n        Provides an alternative way to generate the target string and metadata\n        by directly passing the necessary DataFrames and the split date, rather\n        than relying on a pre-packaged `patient_split` dictionary. This might be\n        useful in scenarios where the data components are sourced differently.\n\n        Parameters\n        ----------\n        target_events_after_split : pd.DataFrame\n            DataFrame containing the future events\n            that constitute the target forecast.\n        split_date_included_in_input : datetime\n            The reference date marking the end of\n            the input history and the start of the forecast period.\n        events_until_split : pd.DataFrame\n            DataFrame containing the historical events up to\n            the `split_date_included_in_input`. Used to find the last observed\n            values for context in the metadata.\n        lot_date : datetime, optional\n            The start date of the relevant Line of Therapy, if applicable.\n            Defaults to None.\n\n        Returns\n        -------\n        target_str : str\n            The formatted string representing the target forecast.\n        target_meta : dict\n            Metadata associated with the generated target string\n            (same structure as output from `_generate_target_string`).\n        \"\"\"\n        # Construct the dictionary expected by _generate_target_string\n        patient_dic = {\n            \"target_events_after_split\": target_events_after_split,\n            \"split_date_included_in_input\": split_date_included_in_input,\n            \"events_until_split\": events_until_split,\n            \"lot_date\": lot_date,  # Pass the provided lot_date\n        }\n        # Call the internal method to generate target string and metadata\n        return self._generate_target_string(patient_dic)\n\n    def aggregate_multiple_responses(self, responses_dfs: list[pd.DataFrame]) -&gt; tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Aggregates structured data from multiple model responses (e.g., trajectories).\n\n        Takes a list of DataFrames, where each DataFrame represents the structured\n        output converted from a single model response (e.g., using `reverse_conversion`).\n        It concatenates these DataFrames and calculates the mean of numeric event\n        values, grouping by date and event identifiers. This is useful for combining\n        results from multiple stochastic samples from a model.\n\n        Parameters\n        ----------\n        responses_dfs : list[pd.DataFrame]\n            A list of DataFrames, each converted from a separate\n            model forecast output string. Expected to have consistent columns\n            as defined in the config (date, event name, value, etc.).\n\n        Returns\n        -------\n        resulting_df : pd.DataFrame\n            A DataFrame containing the aggregated\n            results, typically with the event values averaged per date/event.\n        meta : dict\n            A dictionary containing metadata, currently includes\n            \"all_trajectory_data\" which holds the original list of input DataFrames.\n        \"\"\"\n\n        #: concat all response DataFrames together\n        if not responses_dfs:  # Handle empty list case\n            return pd.DataFrame(), {\"all_trajectory_data\": []}\n\n        concatenated = pd.concat(responses_dfs, axis=0, ignore_index=True)\n\n        #: get average of values per date/event identifier, keeping relevant columns\n        # Define columns to group by\n        grouping_cols = [\n            self.config.date_col,\n            self.config.event_name_col,\n            self.config.event_descriptive_name_col,\n            self.config.event_category_col,\n            self.config.patient_id_col,\n            self.config.source_col,\n        ]\n\n        # Ensure all grouping columns exist, handle potential missing ones gracefully\n        valid_grouping_cols = [col for col in grouping_cols if col in concatenated.columns]\n        if not valid_grouping_cols:\n            # Cannot group if no valid columns are present\n            # Consider logging a warning or raising an error\n            return pd.DataFrame(), {\"all_trajectory_data\": responses_dfs.copy()}\n\n        # APPLY DIFFERENT LOGIC FOR NUMERIC VS CATEGORICAL\n        var_types_lookup = self.dm.variable_types\n\n        # Helper function to aggregate based on variable type\n        def _agg_by_precomputed_type(group_df: pd.DataFrame):\n            \"\"\"\n            Helper function for .apply()\n            Aggregates the 'event_value_col' based on the type stored in\n            self.dm.variable_types.\n            \"\"\"\n            event_name = group_df[self.config.event_name_col].iloc[0]\n            var_type = var_types_lookup.get(event_name)\n            value_series = group_df[self.config.event_value_col]\n\n            if var_type == \"numeric\":\n                numeric_values = pd.to_numeric(value_series, errors=\"coerce\")\n                return numeric_values.mean()\n            elif var_type == \"categorical\":  # get set overlap\n                # Get counts of each unique value in the group (unique date, event)\n                value_counts = value_series.value_counts()\n\n                # Return as a dictionary\n                return value_counts.to_dict()\n            else:  # if var_type is None or unrecognized\n                logging.warning(\n                    f\"Variable type for '{event_name}' not found in var_types_lookup \"\n                    f\"(Type: '{var_type}'). Returning NA for this group.\"\n                )\n                return pd.NA\n\n        # Perform the groupby and aggregation\n        # Use observed=True for potentially better performance with categorical data\n        resulting = (\n            concatenated.groupby(valid_grouping_cols, observed=True)[\n                [self.config.event_name_col, self.config.event_value_col]\n            ]\n            .apply(_agg_by_precomputed_type)\n            .reset_index(name=self.config.event_value_col)\n        )\n\n        #: prepare metadata dictionary\n        meta = {\n            \"all_trajectory_data\": responses_dfs.copy(),  # Keep original list\n        }\n        return resulting, meta\n\n    def reverse_conversion(\n        self, text_to_convert: str, unique_events: pd.DataFrame, split_date: datetime\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Converts a formatted forecast text string back into a structured DataFrame.\n\n        Parses a text string (assumed to be generated by a forecasting model in\n        a specific format, e.g., using \"[X] weeks later...\" markers) and extracts\n        the forecasted event data. It uses the `split_date` as the reference point\n        for calculating absolute dates from relative week offsets in the text.\n        Relies on the base class's `_extract_event_data` method for the core parsing logic.\n\n        Parameters\n        ----------\n        text_to_convert : str\n            The text string generated by the model containing the forecast.\n        unique_events : pd.DataFrame\n            A DataFrame defining known event types (names, categories)\n            to help with parsing and validation within the base class method.\n        split_date : datetime\n            The date relative to which the week offsets (e.g., \"[X] weeks later\")\n            in the `text_to_convert` should be interpreted.\n\n        Returns\n        -------\n        converted_data : pd.DataFrame\n            A DataFrame containing the structured event data parsed from the input text,\n            sorted by date, category, and event name for consistency.\n        \"\"\"\n\n        #: Prepend the event day preamble if it's missing at the start, as the base\n        #  method might expect it for splitting days/visits.\n        if self.event_day_preamble and not text_to_convert.startswith(self.event_day_preamble):\n            text_to_convert = self.event_day_preamble + text_to_convert\n\n        #: Call the base class's extraction method. Assuming the forecast output format\n        #  is compatible with the general event extraction logic (e.g., uses the\n        #  same \"[X] weeks later...\" structure). Set `only_contains_events=True`\n        #  as we don't expect demographic data in the forecast output.\n        converted_data = self._extract_event_data(\n            text_to_convert,\n            unique_events,\n            init_date=split_date,\n            only_contains_events=True,\n        )\n\n        #: sort for consistency based on config columns\n        sort_columns = [\n            col\n            for col in [\n                self.config.date_col,\n                self.config.event_category_col,\n                self.config.event_name_col,\n            ]\n            if col in converted_data.columns\n        ]\n        if sort_columns:\n            converted_data = converted_data.sort_values(by=sort_columns).reset_index(drop=True)\n\n        return converted_data\n</code></pre>"},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting-functions","title":"Functions","text":""},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting.__init__","title":"__init__","text":"<pre><code>__init__(\n    constant_description, nr_tokens_budget_total, config, dm\n)\n</code></pre> <p>Initializes the ConverterForecasting instance.</p> <p>Sets up the converter with necessary configurations, including descriptions for constant patient features, the total token budget for generated text, and various prompt templates defined in the Config object.</p> <p>Parameters:</p> Name Type Description Default <code>constant_description</code> <code>DataFrame</code> <p>DataFrame containing descriptions for constant patient attributes (e.g., 'Sex: Male'). Used potentially by base class methods for input string generation.</p> required <code>nr_tokens_budget_total</code> <code>int</code> <p>The target maximum number of tokens for the combined input (history) and output (forecast) text.</p> required <code>config</code> <code>Config</code> <p>A Config object containing shared configuration settings like prompt templates, column names, tokenizer details, etc.</p> required <code>dm</code> <code>DataManager</code> <p>DataManager object containing the variable types and data frames.</p> required Source code in <code>twinweaver/instruction/converter_forecasting.py</code> <pre><code>def __init__(\n    self,\n    constant_description: pd.DataFrame,\n    nr_tokens_budget_total: int,\n    config: Config,\n    dm: DataManager,\n) -&gt; None:\n    \"\"\"\n    Initializes the ConverterForecasting instance.\n\n    Sets up the converter with necessary configurations, including descriptions\n    for constant patient features, the total token budget for generated text,\n    and various prompt templates defined in the Config object.\n\n    Parameters\n    ----------\n    constant_description : pd.DataFrame\n        DataFrame containing descriptions for constant\n        patient attributes (e.g., 'Sex: Male'). Used potentially by base\n        class methods for input string generation.\n    nr_tokens_budget_total : int\n        The target maximum number of tokens for the\n        combined input (history) and output (forecast) text.\n    config : Config\n        A Config object containing shared configuration settings like\n        prompt templates, column names, tokenizer details, etc.\n    dm : DataManager\n        DataManager object containing the variable types and data frames.\n    \"\"\"\n    # Initialize base class with config and potential overrides\n    super().__init__(config)\n\n    # Store specific attributes for this class\n    self.constant_description = constant_description\n    self.nr_tokens_budget_total = nr_tokens_budget_total\n    self.dm = dm\n\n    # Set forecasting prompts, using overrides or config defaults\n    self.forecasting_prompt_start = self.config.forecasting_fval_prompt_start\n    self.forecasting_prompt_var_time = self.config.forecasting_prompt_var_time\n    self.forecasting_prompt_summarized_start = self.config.forecasting_prompt_summarized_start\n    self.forecasting_prompt_summarized_genetic = self.config.forecasting_prompt_summarized_genetic\n    self.forecasting_prompt_summarized_lot = self.config.forecasting_prompt_summarized_lot\n\n    # Initialize tokenizer and budget padding\n    self.nr_tokens_budget_padding = self.config.nr_tokens_budget_padding\n    self.always_keep_first_visit = self.config.always_keep_first_visit\n</code></pre>"},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting.aggregate_multiple_responses","title":"aggregate_multiple_responses","text":"<pre><code>aggregate_multiple_responses(responses_dfs)\n</code></pre> <p>Aggregates structured data from multiple model responses (e.g., trajectories).</p> <p>Takes a list of DataFrames, where each DataFrame represents the structured output converted from a single model response (e.g., using <code>reverse_conversion</code>). It concatenates these DataFrames and calculates the mean of numeric event values, grouping by date and event identifiers. This is useful for combining results from multiple stochastic samples from a model.</p> <p>Parameters:</p> Name Type Description Default <code>responses_dfs</code> <code>list[DataFrame]</code> <p>A list of DataFrames, each converted from a separate model forecast output string. Expected to have consistent columns as defined in the config (date, event name, value, etc.).</p> required <p>Returns:</p> Name Type Description <code>resulting_df</code> <code>DataFrame</code> <p>A DataFrame containing the aggregated results, typically with the event values averaged per date/event.</p> <code>meta</code> <code>dict</code> <p>A dictionary containing metadata, currently includes \"all_trajectory_data\" which holds the original list of input DataFrames.</p> Source code in <code>twinweaver/instruction/converter_forecasting.py</code> <pre><code>def aggregate_multiple_responses(self, responses_dfs: list[pd.DataFrame]) -&gt; tuple[pd.DataFrame, dict]:\n    \"\"\"\n    Aggregates structured data from multiple model responses (e.g., trajectories).\n\n    Takes a list of DataFrames, where each DataFrame represents the structured\n    output converted from a single model response (e.g., using `reverse_conversion`).\n    It concatenates these DataFrames and calculates the mean of numeric event\n    values, grouping by date and event identifiers. This is useful for combining\n    results from multiple stochastic samples from a model.\n\n    Parameters\n    ----------\n    responses_dfs : list[pd.DataFrame]\n        A list of DataFrames, each converted from a separate\n        model forecast output string. Expected to have consistent columns\n        as defined in the config (date, event name, value, etc.).\n\n    Returns\n    -------\n    resulting_df : pd.DataFrame\n        A DataFrame containing the aggregated\n        results, typically with the event values averaged per date/event.\n    meta : dict\n        A dictionary containing metadata, currently includes\n        \"all_trajectory_data\" which holds the original list of input DataFrames.\n    \"\"\"\n\n    #: concat all response DataFrames together\n    if not responses_dfs:  # Handle empty list case\n        return pd.DataFrame(), {\"all_trajectory_data\": []}\n\n    concatenated = pd.concat(responses_dfs, axis=0, ignore_index=True)\n\n    #: get average of values per date/event identifier, keeping relevant columns\n    # Define columns to group by\n    grouping_cols = [\n        self.config.date_col,\n        self.config.event_name_col,\n        self.config.event_descriptive_name_col,\n        self.config.event_category_col,\n        self.config.patient_id_col,\n        self.config.source_col,\n    ]\n\n    # Ensure all grouping columns exist, handle potential missing ones gracefully\n    valid_grouping_cols = [col for col in grouping_cols if col in concatenated.columns]\n    if not valid_grouping_cols:\n        # Cannot group if no valid columns are present\n        # Consider logging a warning or raising an error\n        return pd.DataFrame(), {\"all_trajectory_data\": responses_dfs.copy()}\n\n    # APPLY DIFFERENT LOGIC FOR NUMERIC VS CATEGORICAL\n    var_types_lookup = self.dm.variable_types\n\n    # Helper function to aggregate based on variable type\n    def _agg_by_precomputed_type(group_df: pd.DataFrame):\n        \"\"\"\n        Helper function for .apply()\n        Aggregates the 'event_value_col' based on the type stored in\n        self.dm.variable_types.\n        \"\"\"\n        event_name = group_df[self.config.event_name_col].iloc[0]\n        var_type = var_types_lookup.get(event_name)\n        value_series = group_df[self.config.event_value_col]\n\n        if var_type == \"numeric\":\n            numeric_values = pd.to_numeric(value_series, errors=\"coerce\")\n            return numeric_values.mean()\n        elif var_type == \"categorical\":  # get set overlap\n            # Get counts of each unique value in the group (unique date, event)\n            value_counts = value_series.value_counts()\n\n            # Return as a dictionary\n            return value_counts.to_dict()\n        else:  # if var_type is None or unrecognized\n            logging.warning(\n                f\"Variable type for '{event_name}' not found in var_types_lookup \"\n                f\"(Type: '{var_type}'). Returning NA for this group.\"\n            )\n            return pd.NA\n\n    # Perform the groupby and aggregation\n    # Use observed=True for potentially better performance with categorical data\n    resulting = (\n        concatenated.groupby(valid_grouping_cols, observed=True)[\n            [self.config.event_name_col, self.config.event_value_col]\n        ]\n        .apply(_agg_by_precomputed_type)\n        .reset_index(name=self.config.event_value_col)\n    )\n\n    #: prepare metadata dictionary\n    meta = {\n        \"all_trajectory_data\": responses_dfs.copy(),  # Keep original list\n    }\n    return resulting, meta\n</code></pre>"},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting.forward_conversion","title":"forward_conversion","text":"<pre><code>forward_conversion(patient_split)\n</code></pre> <p>Performs the primary conversion from a data split to prompt and target strings.</p> <p>This method orchestrates the generation of the target string (what the model should predict) and the corresponding prompt string (the instruction asking for the prediction) based on a patient data split.</p> <p>Note: This method generates the prompt and target strings. The generation of the input string (patient history) is typically handled separately, often by the base class's <code>_get_input_string</code> method, using the \"events_until_split\" and \"constant_data\" from the <code>patient_split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>patient_split</code> <code>DataSplitterForecastingOption</code> <p>DataSplitterForecastingOption containing the data for a single split.</p> required <p>Returns:</p> Name Type Description <code>prompt_str</code> <code>str</code> <p>The generated prompt instructing the model on the forecasting task.</p> <code>target_str</code> <code>str</code> <p>The formatted string representing the target forecast values.</p> <code>target_meta</code> <code>dict</code> <p>Metadata associated with the generated target string (output from <code>_generate_target_string</code>).</p> Source code in <code>twinweaver/instruction/converter_forecasting.py</code> <pre><code>def forward_conversion(self, patient_split: DataSplitterForecastingOption) -&gt; tuple[str, str, dict]:\n    \"\"\"\n    Performs the primary conversion from a data split to prompt and target strings.\n\n    This method orchestrates the generation of the target string (what the model\n    should predict) and the corresponding prompt string (the instruction asking\n    for the prediction) based on a patient data split.\n\n    Note: This method generates the *prompt* and *target* strings. The generation\n    of the *input* string (patient history) is typically handled separately, often\n    by the base class's `_get_input_string` method, using the \"events_until_split\"\n    and \"constant_data\" from the `patient_split`.\n\n    Parameters\n    ----------\n    patient_split : DataSplitterForecastingOption\n        DataSplitterForecastingOption containing the data for a single split.\n\n    Returns\n    -------\n    prompt_str : str\n        The generated prompt instructing the model on the\n        forecasting task.\n    target_str : str\n        The formatted string representing the target forecast values.\n    target_meta : dict\n        Metadata associated with the generated target string\n        (output from `_generate_target_string`).\n    \"\"\"\n\n    #: generate target string and associated metadata\n    target_str, target_meta = self._generate_target_string(patient_split)\n\n    #: generate prompt string based on the target metadata\n    prompt_str = self._generate_prompt(target_meta)\n\n    # Note: The input string (patient history text) generation is handled by the base class's\n    # `_get_input_string` method, which would typically be called elsewhere\n    # in the pipeline that uses this forward_conversion result along with the input events.\n    # This method focuses on generating the *prompt* and *target* based on the split.\n\n    return prompt_str, target_str, target_meta\n</code></pre>"},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting.forward_conversion_inference","title":"forward_conversion_inference","text":"<pre><code>forward_conversion_inference(\n    patient_split, future_weeks_per_variable\n)\n</code></pre> <p>Generates only the prompt string for inference scenarios.</p> <p>Used when the goal is to get a prediction from the model, not for training. It takes the patient's historical data and a specific set of variables and future days/weeks to predict, then constructs the appropriate prompt string. It does not generate a target string, as the target is what the model is expected to produce.</p> <p>Parameters:</p> Name Type Description Default <code>patient_split</code> <code>DataSplitterForecastingOption</code> <p>DataSplitterForecastingOption containing the data for a single split.</p> required <code>future_weeks_per_variable</code> <code>dict</code> <p>Dictionary explicitly defining the forecasting task. Format: {: [, , ...], ...} These weeks are relative to the <code>split_date_included_in_input</code>. required <p>Returns:</p> Name Type Description <code>prompt_str</code> <code>str</code> <p>The generated prompt for the inference task.</p> <code>target_pseudo_meta</code> <code>dict</code> <p>A dictionary containing the metadata constructed specifically for generating this inference prompt (includes the provided <code>future_prediction_time_per_variable</code>, derived <code>dates_per_variable</code>, and looked-up <code>variable_name_mapping</code>).</p> Source code in <code>twinweaver/instruction/converter_forecasting.py</code> <pre><code>def forward_conversion_inference(\n    self, patient_split: DataSplitterForecastingOption, future_weeks_per_variable: dict\n) -&gt; tuple[str, dict]:\n    \"\"\"\n    Generates only the prompt string for inference scenarios.\n\n    Used when the goal is to get a prediction from the model, not for training.\n    It takes the patient's historical data and a specific set of variables and\n    future days/weeks to predict, then constructs the appropriate prompt string.\n    It does not generate a target string, as the target is what the model\n    is expected to produce.\n\n    Parameters\n    ----------\n    patient_split : DataSplitterForecastingOption\n        DataSplitterForecastingOption containing the data for a single split.\n    future_weeks_per_variable : dict\n        Dictionary explicitly defining the forecasting task.\n        Format: {&lt;variable_name&gt;: [&lt;future_week_1&gt;, &lt;future_week_2&gt;, ...], ...}\n        These weeks are relative to the `split_date_included_in_input`.\n\n    Returns\n    -------\n    prompt_str : str\n        The generated prompt for the inference task.\n    target_pseudo_meta : dict\n        A dictionary containing the metadata constructed\n        specifically for generating this inference prompt (includes the provided\n        `future_prediction_time_per_variable`, derived `dates_per_variable`, and looked-up\n        `variable_name_mapping`).\n    \"\"\"\n\n    #: generate target_pseudo meta data needed for prompt generation\n    target_pseudo_meta = {}\n\n    #: Use the provided future_weeks_per_variable directly\n    target_pseudo_meta[\"future_prediction_time_per_variable\"] = future_weeks_per_variable\n\n    # Make dates per variable based on the provided days/weeks and split date\n    dates_per_variable = {}\n    split_date = patient_split.split_date_included_in_input\n    for variable, weeks in future_weeks_per_variable.items():\n        # Ensure weeks is iterable\n        if not hasattr(weeks, \"__iter__\"):\n            weeks = [weeks]\n        dates_per_variable[variable] = [\n            split_date + pd.Timedelta(days=float(w) * self._time_divisor) for w in weeks\n        ]\n    target_pseudo_meta[\"dates_per_variable\"] = dates_per_variable\n\n    #: make target_meta[\"variable_name_mapping\"] by looking up in input events\n    variable_name_mapping = {}\n    input_events = patient_split.events_until_split\n    for variable in future_weeks_per_variable.keys():\n        # Get descriptive name from input history\n        curr_var = input_events[input_events[self.config.event_name_col] == variable]\n        if not curr_var.empty and self.config.event_descriptive_name_col in curr_var.columns:\n            descriptive_name = curr_var[self.config.event_descriptive_name_col].iloc[0]\n        else:\n            # Fallback: If not found by event_name, try descriptive name directly,\n            # or just use the variable name itself. This part might need refinement\n            # depending on how variables are guaranteed to be present/identifiable.\n            descriptive_name = variable  # Simple fallback\n        variable_name_mapping[variable] = descriptive_name\n\n    target_pseudo_meta[\"variable_name_mapping\"] = variable_name_mapping\n\n    #: generate prompt (including when to generate what) based on constructed metadata\n    prompt_str = self._generate_prompt(target_pseudo_meta)\n\n    return prompt_str, target_pseudo_meta\n</code></pre>"},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting.generate_target_manual","title":"generate_target_manual","text":"<pre><code>generate_target_manual(\n    target_events_after_split,\n    split_date_included_in_input,\n    events_until_split,\n    lot_date=None,\n)\n</code></pre> <p>Manually generates the target string and metadata from explicit components.</p> <p>Provides an alternative way to generate the target string and metadata by directly passing the necessary DataFrames and the split date, rather than relying on a pre-packaged <code>patient_split</code> dictionary. This might be useful in scenarios where the data components are sourced differently.</p> <p>Parameters:</p> Name Type Description Default <code>target_events_after_split</code> <code>DataFrame</code> <p>DataFrame containing the future events that constitute the target forecast.</p> required <code>split_date_included_in_input</code> <code>datetime</code> <p>The reference date marking the end of the input history and the start of the forecast period.</p> required <code>events_until_split</code> <code>DataFrame</code> <p>DataFrame containing the historical events up to the <code>split_date_included_in_input</code>. Used to find the last observed values for context in the metadata.</p> required <code>lot_date</code> <code>datetime</code> <p>The start date of the relevant Line of Therapy, if applicable. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>target_str</code> <code>str</code> <p>The formatted string representing the target forecast.</p> <code>target_meta</code> <code>dict</code> <p>Metadata associated with the generated target string (same structure as output from <code>_generate_target_string</code>).</p> Source code in <code>twinweaver/instruction/converter_forecasting.py</code> <pre><code>def generate_target_manual(\n    self,\n    target_events_after_split: pd.DataFrame,\n    split_date_included_in_input: datetime,\n    events_until_split: pd.DataFrame,\n    lot_date: datetime = None,\n) -&gt; tuple:  # Added optional lot_date param\n    \"\"\"\n    Manually generates the target string and metadata from explicit components.\n\n    Provides an alternative way to generate the target string and metadata\n    by directly passing the necessary DataFrames and the split date, rather\n    than relying on a pre-packaged `patient_split` dictionary. This might be\n    useful in scenarios where the data components are sourced differently.\n\n    Parameters\n    ----------\n    target_events_after_split : pd.DataFrame\n        DataFrame containing the future events\n        that constitute the target forecast.\n    split_date_included_in_input : datetime\n        The reference date marking the end of\n        the input history and the start of the forecast period.\n    events_until_split : pd.DataFrame\n        DataFrame containing the historical events up to\n        the `split_date_included_in_input`. Used to find the last observed\n        values for context in the metadata.\n    lot_date : datetime, optional\n        The start date of the relevant Line of Therapy, if applicable.\n        Defaults to None.\n\n    Returns\n    -------\n    target_str : str\n        The formatted string representing the target forecast.\n    target_meta : dict\n        Metadata associated with the generated target string\n        (same structure as output from `_generate_target_string`).\n    \"\"\"\n    # Construct the dictionary expected by _generate_target_string\n    patient_dic = {\n        \"target_events_after_split\": target_events_after_split,\n        \"split_date_included_in_input\": split_date_included_in_input,\n        \"events_until_split\": events_until_split,\n        \"lot_date\": lot_date,  # Pass the provided lot_date\n    }\n    # Call the internal method to generate target string and metadata\n    return self._generate_target_string(patient_dic)\n</code></pre>"},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting.ConverterForecasting.reverse_conversion","title":"reverse_conversion","text":"<pre><code>reverse_conversion(\n    text_to_convert, unique_events, split_date\n)\n</code></pre> <p>Converts a formatted forecast text string back into a structured DataFrame.</p> <p>Parses a text string (assumed to be generated by a forecasting model in a specific format, e.g., using \"[X] weeks later...\" markers) and extracts the forecasted event data. It uses the <code>split_date</code> as the reference point for calculating absolute dates from relative week offsets in the text. Relies on the base class's <code>_extract_event_data</code> method for the core parsing logic.</p> <p>Parameters:</p> Name Type Description Default <code>text_to_convert</code> <code>str</code> <p>The text string generated by the model containing the forecast.</p> required <code>unique_events</code> <code>DataFrame</code> <p>A DataFrame defining known event types (names, categories) to help with parsing and validation within the base class method.</p> required <code>split_date</code> <code>datetime</code> <p>The date relative to which the week offsets (e.g., \"[X] weeks later\") in the <code>text_to_convert</code> should be interpreted.</p> required <p>Returns:</p> Name Type Description <code>converted_data</code> <code>DataFrame</code> <p>A DataFrame containing the structured event data parsed from the input text, sorted by date, category, and event name for consistency.</p> Source code in <code>twinweaver/instruction/converter_forecasting.py</code> <pre><code>def reverse_conversion(\n    self, text_to_convert: str, unique_events: pd.DataFrame, split_date: datetime\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Converts a formatted forecast text string back into a structured DataFrame.\n\n    Parses a text string (assumed to be generated by a forecasting model in\n    a specific format, e.g., using \"[X] weeks later...\" markers) and extracts\n    the forecasted event data. It uses the `split_date` as the reference point\n    for calculating absolute dates from relative week offsets in the text.\n    Relies on the base class's `_extract_event_data` method for the core parsing logic.\n\n    Parameters\n    ----------\n    text_to_convert : str\n        The text string generated by the model containing the forecast.\n    unique_events : pd.DataFrame\n        A DataFrame defining known event types (names, categories)\n        to help with parsing and validation within the base class method.\n    split_date : datetime\n        The date relative to which the week offsets (e.g., \"[X] weeks later\")\n        in the `text_to_convert` should be interpreted.\n\n    Returns\n    -------\n    converted_data : pd.DataFrame\n        A DataFrame containing the structured event data parsed from the input text,\n        sorted by date, category, and event name for consistency.\n    \"\"\"\n\n    #: Prepend the event day preamble if it's missing at the start, as the base\n    #  method might expect it for splitting days/visits.\n    if self.event_day_preamble and not text_to_convert.startswith(self.event_day_preamble):\n        text_to_convert = self.event_day_preamble + text_to_convert\n\n    #: Call the base class's extraction method. Assuming the forecast output format\n    #  is compatible with the general event extraction logic (e.g., uses the\n    #  same \"[X] weeks later...\" structure). Set `only_contains_events=True`\n    #  as we don't expect demographic data in the forecast output.\n    converted_data = self._extract_event_data(\n        text_to_convert,\n        unique_events,\n        init_date=split_date,\n        only_contains_events=True,\n    )\n\n    #: sort for consistency based on config columns\n    sort_columns = [\n        col\n        for col in [\n            self.config.date_col,\n            self.config.event_category_col,\n            self.config.event_name_col,\n        ]\n        if col in converted_data.columns\n    ]\n    if sort_columns:\n        converted_data = converted_data.sort_values(by=sort_columns).reset_index(drop=True)\n\n    return converted_data\n</code></pre>"},{"location":"reference/instruction/converter_forecasting/#twinweaver.instruction.converter_forecasting-functions","title":"Functions","text":""},{"location":"reference/instruction/converter_forecasting_qa/","title":"Converter Forecasting QA","text":""},{"location":"reference/instruction/converter_forecasting_qa/#twinweaver.instruction.converter_forecasting_qa","title":"twinweaver.instruction.converter_forecasting_qa","text":""},{"location":"reference/instruction/converter_forecasting_qa/#twinweaver.instruction.converter_forecasting_qa-classes","title":"Classes","text":""},{"location":"reference/instruction/converter_forecasting_qa/#twinweaver.instruction.converter_forecasting_qa.ConverterForecastingQA","title":"ConverterForecastingQA","text":"<p>               Bases: <code>ConverterForecasting</code></p> <p>A class for converting patient data into a format suitable for manual forecasting with a question-answering approach.</p> <p>This class inherits from ConverterForecasting and implements additional methods to preprocess target data into categorical bins and generate prompts for the forecasting model.</p> Source code in <code>twinweaver/instruction/converter_forecasting_qa.py</code> <pre><code>class ConverterForecastingQA(ConverterForecasting):\n    \"\"\"\n    A class for converting patient data into a format suitable for manual forecasting\n    with a question-answering approach.\n\n    This class inherits from ConverterForecasting and implements additional methods\n    to preprocess target data into\n    categorical bins and generate prompts for the forecasting model.\n    \"\"\"\n\n    def __init__(self, variable_stats: pd.DataFrame, **kwargs):\n        super().__init__(**kwargs)\n        self.variable_stats = variable_stats\n\n    def _preprocess_target_into_qa(self, patient_split: DataSplitterForecastingOption) -&gt; DataSplitterForecastingOption:\n        \"\"\"\n        Preprocesses the target data into categories suitable for question-answering format.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterForecastingOption\n            A data splitter object containing patient data split into events and target variables.\n\n        Returns\n        -------\n        DataSplitterForecastingOption\n            The updated patient_split object with target events converted into categorical bins.\n\n        \"\"\"\n\n        assert self.variable_stats is not None, (\n            \"Variable statistics must be set up before preprocessing target into QA format.\"\n            \"Please run setup_statistics() first on the DataSplitterForecasting.\"\n            \"Alternatively, you can disable forecasting QA in the conversion call.\"\n        )\n\n        #: deep copy dict\n        curr_patient_split = copy.deepcopy(patient_split)\n\n        #: loop through all target variables\n        targets = curr_patient_split.target_events_after_split\n        all_variables = targets[self.config.event_name_col].unique().tolist()\n        targets[self.config.event_value_col] = targets[self.config.event_value_col].astype(\"string\")\n        curr_patient_split.category_splits = {}\n\n        for var in all_variables:\n            # Check if variable is numeric or categorical\n            is_numeric = self.variable_stats.loc[\n                self.variable_stats[self.config.event_name_col] == var, \"is_numeric\"\n            ].values[0]\n            if not is_numeric:\n                raise ValueError(f\"Variable {var} is not numeric, cannot bin for QA forecasting.\")\n\n            #: change every variable in target to be a category based on variable stats in event_value\n            #: use e.g. \"a\" (= \"Bin (0,2]\") as new value\n\n            splits = self.variable_stats.loc[\n                self.variable_stats[self.config.event_name_col] == var,\n                self.config.bins_split_name,\n            ].values[0]\n\n            # Replace both edges for splits with -inf/inf\n            splits[0] = -float(\"inf\")\n            splits[-1] = float(\"inf\")\n            bin_ranges = [\n                f\"Bin ({round_and_strip(splits[i], self.decimal_precision)}, \"\n                + f\"{round_and_strip(splits[i + 1], self.decimal_precision)}]\"\n                for i in range(len(splits) - 1)\n            ]\n\n            # Change last bin closing to be exclusive for inf\n            bin_ranges[-1] = bin_ranges[-1].replace(\"]\", \")\")\n\n            #: make bins into A, B, C, etc.\n            bin_names = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n            bin_mapping = {bin_names[i]: bin_ranges[i] for i in range(len(bin_ranges))}\n            all_bins = list(bin_mapping.keys())\n\n            # Apply binning\n            curr_targets = (\n                targets[targets[self.config.event_name_col] == var].loc[:, self.config.event_value_col].copy()\n            )\n            target_selection = targets[self.config.event_name_col] == var\n\n            new_targets = pd.cut(\n                curr_targets.astype(float),\n                bins=splits,\n                labels=all_bins,\n                include_lowest=True,\n            )\n            new_targets = new_targets.astype(\"string\")\n            targets.loc[target_selection, self.config.event_value_col] = new_targets\n\n            #: add in curr_patient_split[\"category_splits\"]\n            curr_patient_split.category_splits[var] = bin_mapping\n\n        #: save &amp; return\n        curr_patient_split.target_events_after_split = targets\n        return curr_patient_split\n\n    def _generate_prompt(self, target_meta: dict) -&gt; str:\n        \"\"\"\n        Generates a prompt string for the forecasting model based on the target\n        metadata and variable statistics.\n\n        Parameters\n        ----------\n        target_meta : dict\n            A dictionary containing metadata about the target variables.\n\n        Returns\n        -------\n        str\n            The generated prompt string for the forecasting model.\n\n        \"\"\"\n\n        # start ret\n        ret_prompt = \"\"\n\n        #: make sure to round weeks from future_prediction_time_per_variable to predict to 2 decimals\n        future_prediction_time_per_variable = target_meta[\"future_prediction_time_per_variable\"]\n        future_prediction_time_per_variable = {\n            k: [round_and_strip(v2, self.decimal_precision) for v2 in v]\n            for k, v in future_prediction_time_per_variable.items()\n        }\n\n        #: add base prompt\n        ret_prompt += self.config.qa_prompt_start\n\n        #: sort alphabetically by variable name\n        future_prediction_time_per_variable = dict(\n            sorted(future_prediction_time_per_variable.items(), key=lambda item: item[0])\n        )\n\n        #: create prompt for which variables to predict\n        for variable, time in future_prediction_time_per_variable.items():\n            #: need event_descriptive_name\n            variable_desc_name = target_meta[\"variable_name_mapping\"][variable]\n\n            # Create prompt\n            ret_prompt += \"\\n\\t\" + variable_desc_name\n            ret_prompt += self.config.forecasting_prompt_var_time\n            ret_prompt += str(time).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n\n            #: add in string for which varaibles have which category values\n            processed_bins = str(target_meta[\"category_splits\"][variable]).lower()\n            processed_bins = processed_bins.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")\n\n            ret_prompt += \"\\n\" + self.config.qa_bins_start\n            ret_prompt += processed_bins\n\n        #: return\n        return ret_prompt\n\n    def forward_conversion(self, patient_split: DataSplitterForecastingOption) -&gt; tuple:\n        \"\"\"\n        Converts the patient data into a format suitable for the forecasting model,\n        including preprocessing and prompt generation.\n\n        Parameters\n        ----------\n        patient_split : DataSplitterForecastingOption\n            A data splitter object containing patient data split into events and target variables.\n\n        Returns\n        -------\n        prompt_str : str\n            The generated prompt string.\n        target_str : str\n            The generated target string.\n        target_meta : dict\n            Metadata associated with the target.\n        \"\"\"\n\n        #: preprocess target data into categories\n        raw_target = patient_split.target_events_after_split.copy()\n        patient_split = self._preprocess_target_into_qa(patient_split)\n\n        #: generate target string\n        target_str, target_meta = self._generate_target_string(patient_split)\n\n        #: make sure that target_meta contains the new targets in categories\n        target_meta[\"category_splits\"] = patient_split.category_splits\n        target_meta[\"target_data_raw\"] = raw_target\n\n        #: generate prompt (including when to generate what)\n        prompt_str = self._generate_prompt(target_meta)\n\n        return prompt_str, target_str, target_meta\n</code></pre>"},{"location":"reference/instruction/converter_forecasting_qa/#twinweaver.instruction.converter_forecasting_qa.ConverterForecastingQA-functions","title":"Functions","text":""},{"location":"reference/instruction/converter_forecasting_qa/#twinweaver.instruction.converter_forecasting_qa.ConverterForecastingQA.forward_conversion","title":"forward_conversion","text":"<pre><code>forward_conversion(patient_split)\n</code></pre> <p>Converts the patient data into a format suitable for the forecasting model, including preprocessing and prompt generation.</p> <p>Parameters:</p> Name Type Description Default <code>patient_split</code> <code>DataSplitterForecastingOption</code> <p>A data splitter object containing patient data split into events and target variables.</p> required <p>Returns:</p> Name Type Description <code>prompt_str</code> <code>str</code> <p>The generated prompt string.</p> <code>target_str</code> <code>str</code> <p>The generated target string.</p> <code>target_meta</code> <code>dict</code> <p>Metadata associated with the target.</p> Source code in <code>twinweaver/instruction/converter_forecasting_qa.py</code> <pre><code>def forward_conversion(self, patient_split: DataSplitterForecastingOption) -&gt; tuple:\n    \"\"\"\n    Converts the patient data into a format suitable for the forecasting model,\n    including preprocessing and prompt generation.\n\n    Parameters\n    ----------\n    patient_split : DataSplitterForecastingOption\n        A data splitter object containing patient data split into events and target variables.\n\n    Returns\n    -------\n    prompt_str : str\n        The generated prompt string.\n    target_str : str\n        The generated target string.\n    target_meta : dict\n        Metadata associated with the target.\n    \"\"\"\n\n    #: preprocess target data into categories\n    raw_target = patient_split.target_events_after_split.copy()\n    patient_split = self._preprocess_target_into_qa(patient_split)\n\n    #: generate target string\n    target_str, target_meta = self._generate_target_string(patient_split)\n\n    #: make sure that target_meta contains the new targets in categories\n    target_meta[\"category_splits\"] = patient_split.category_splits\n    target_meta[\"target_data_raw\"] = raw_target\n\n    #: generate prompt (including when to generate what)\n    prompt_str = self._generate_prompt(target_meta)\n\n    return prompt_str, target_str, target_meta\n</code></pre>"},{"location":"reference/instruction/converter_forecasting_qa/#twinweaver.instruction.converter_forecasting_qa-functions","title":"Functions","text":""},{"location":"reference/instruction/converter_instruction/","title":"Converter Instruction","text":""},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction","title":"twinweaver.instruction.converter_instruction","text":""},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction-classes","title":"Classes","text":""},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction","title":"ConverterInstruction","text":"<p>               Bases: <code>ConverterBase</code></p> <p>Orchestrates the conversion of patient data into multi-task instruction-following prompts and target strings, suitable for training language models.</p> <p>This class acts as a high-level controller, utilizing specialized sub-converters (<code>ConverterForecasting</code>, <code>ConverterEvents</code>, <code>ConverterForecastingQA</code>) to generate prompts and targets for different task types (e.g., value forecasting, event prediction, question answering about forecasting). It then combines these individual task components into a single, structured multi-task instruction prompt and a corresponding multi-task target answer string, using templates defined in the provided <code>Config</code> object. It also handles the reverse conversion of model outputs back into structured data and provides methods for aggregation and comparison.</p> Customization <p>Summarized row generation (the compact \u201clatest values\u201d section inserted before the task prompts) can be overridden:   * Call set_summarized_row_fn(fn) after instantiation.   * fn may have signature:       - (self, events_df, combined_meta)   OR       - (events_df, combined_meta)     combined_meta contains (possibly empty) keys:       - \"dates_per_variable\": dict[var_name] -&gt; list/sequence of future (or target) dates       - \"variable_name_mapping\": optional mapping var_name -&gt; descriptive name   * If the custom function raises an exception, the converter logs a warning and     falls back to the built-in _generate_summarized_row_string.</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>class ConverterInstruction(ConverterBase):\n    \"\"\"\n    Orchestrates the conversion of patient data into multi-task instruction-following\n    prompts and target strings, suitable for training language models.\n\n    This class acts as a high-level controller, utilizing specialized sub-converters\n    (`ConverterForecasting`, `ConverterEvents`, `ConverterForecastingQA`)\n    to generate prompts and targets for different task types (e.g., value forecasting,\n    event prediction, question answering about forecasting). It then combines these\n    individual task components into a single, structured multi-task instruction prompt\n    and a corresponding multi-task target answer string, using templates defined in\n    the provided `Config` object. It also handles the reverse conversion of model\n    outputs back into structured data and provides methods for aggregation and comparison.\n\n    Customization\n    -------------\n    Summarized row generation (the compact \u201clatest values\u201d section inserted before\n    the task prompts) can be overridden:\n      * Call set_summarized_row_fn(fn) after instantiation.\n      * fn may have signature:\n          - (self, events_df, combined_meta)   OR\n          - (events_df, combined_meta)\n        combined_meta contains (possibly empty) keys:\n          - \"dates_per_variable\": dict[var_name] -&gt; list/sequence of future (or target) dates\n          - \"variable_name_mapping\": optional mapping var_name -&gt; descriptive name\n      * If the custom function raises an exception, the converter logs a warning and\n        falls back to the built-in _generate_summarized_row_string.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        nr_tokens_budget_total: int,\n        config: Config,\n        dm: DataManager,\n        variable_stats: pd.DataFrame = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the ConverterInstruction class.\n\n        Sets up the main converter by initializing base class settings, storing essential\n        data descriptions and token budgets, configuring the tokenizer, loading multi-task\n        prompt templates from the config, and initializing instances of the specialized\n        sub-converters required for different task types.\n\n        Parameters\n        ----------\n        nr_tokens_budget_total : int\n            The total token budget available for the generated context + instruction prompt.\n            Used for dynamically selecting history length.\n        config : Config\n            A configuration object supplying tokenizer details, prompt templates (for tasks\n            and context generation), padding values, and other settings.\n        dm : DataManager\n            The data manager instance providing access to data frames and variable types.\n        variable_stats : pd.DataFrame, optional\n            DataFrame containing statistics about variables, used by the forecasting QA converter.\n        \"\"\"\n\n        super().__init__(config)\n\n        # Set vars\n        self.nr_tokens_budget_total = nr_tokens_budget_total\n        self.dm = dm\n        self.constant_description = self.dm.data_frames[\"constant_description\"]\n\n        # Set forecasting prompt texts using overrides or config defaults\n        self.forecasting_prompt_start = config.forecasting_fval_prompt_start\n        self.forecasting_prompt_var_time = config.forecasting_prompt_var_time\n        self.forecasting_prompt_summarized_start = config.forecasting_prompt_summarized_start\n        self.forecasting_prompt_summarized_genetic = config.forecasting_prompt_summarized_genetic\n        self.forecasting_prompt_summarized_lot = config.forecasting_prompt_summarized_lot\n\n        # Initialize tokenizer and related settings\n        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_to_use)\n        self.nr_tokens_budget_padding = self.config.nr_tokens_budget_padding\n        self.always_keep_first_visit = self.config.always_keep_first_visit\n\n        # Set multi-task prompt texts using overrides or config defaults\n        self.task_prompt_start = config.task_prompt_start\n        self.task_prompt_each_task = config.task_prompt_each_task\n        self.task_prompt_end = config.task_prompt_end\n        self.task_target_start = config.task_target_start\n        self.task_target_end = config.task_target_end\n\n        # Initialize converters - can this be done more elegantly?\n        self.converter_forecasting = ConverterForecasting(\n            self.constant_description, nr_tokens_budget_total, config=config, dm=dm\n        )\n\n        self.converter_events = ConverterEvents(\n            config,\n            self.constant_description,\n            nr_tokens_budget_total,\n        )\n\n        self.converter_forecasting_qa = ConverterForecastingQA(\n            variable_stats=variable_stats,\n            constant_description=self.constant_description,\n            nr_tokens_budget_total=nr_tokens_budget_total,\n            config=config,\n            dm=dm,\n        )\n\n        # Default summarized row function\n        self._generate_summarized_row_str_fn = self._generate_summarized_row_string\n\n    def set_custom_summarized_row_fn(self, fn: Callable[[pd.DataFrame, dict], str]):\n        \"\"\"\n        Sets a custom function for generating the summarized row string.\n        If this function is not called, the default _generate_summarized_row_string is used.\n        Requirements: signature must start with (self, events_df, meta, ...)\n        If the function meets these criteria but it is still invalid, it will raise an error later on\n        \"\"\"\n        import inspect\n\n        params = list(inspect.signature(fn).parameters)\n        if len(params) &lt; 3 or params[0] != \"self\":  # assumes signature (self, events_df, meta, ...)\n            raise TypeError(\n                f\"Custom summarized row function must accept at least (self, events_df, meta).Got parameters: {params}\"\n            )\n        # Bind it as method\n        self._generate_summarized_row_str_fn = fn.__get__(self, type(self))\n\n    def _generate_prompt(self, all_tasks: list) -&gt; tuple:\n        \"\"\"\n        Constructs the multi-task instruction prompt string from individual task prompts.\n\n        Takes a list of generated tasks (where each task is a tuple containing its specific\n        prompt, target, metadata, and type identifier) and formats them into a single\n        instruction prompt using wrapper templates defined in the config (e.g., numbering\n        tasks, adding introductory/concluding text).\n\n        Parameters\n        ----------\n        all_tasks : list[tuple[str, str, dict, str]]\n            A list where each element represents a single task. The tuple structure is\n            expected to be: (task_prompt_string, task_target_string, task_metadata_dict, task_type_identifier_string).\n\n        Returns\n        -------\n        str\n            The combined multi-task instruction prompt string ready to be prepended\n            with context (patient history, etc.).\n        \"\"\"\n\n        ret_str = self.task_prompt_start\n\n        for idx in range(len(all_tasks)):\n            task = all_tasks[idx]\n\n            ret_str += self.task_prompt_each_task.format(task_nr=idx + 1)\n            ret_str += task[3]\n            ret_str += task[0]\n            ret_str += \"\\n\\n\"\n\n        ret_str += self.task_prompt_end\n\n        return ret_str\n\n    def _generate_target_string(self, all_tasks: list) -&gt; tuple:\n        \"\"\"\n        Constructs the multi-task target answer string and aggregates metadata.\n\n        Takes a list of generated tasks and combines their individual target strings\n        into a single, formatted multi-task answer string using wrapper templates\n        (e.g., numbering answers corresponding to tasks). It also collects the\n        metadata dictionaries from each individual task into a list.\n\n        Parameters\n        ----------\n        all_tasks : list[tuple[str, str, dict, str]]\n            A list where each element represents a single task. The tuple structure is\n            expected to be: (task_prompt_string, task_target_string, task_metadata_dict, task_type_identifier_string).\n\n        Returns\n        -------\n        target_str : str\n            The combined multi-task target answer string.\n        target_meta_list : list[dict]\n            A list containing the metadata dictionaries from each\n            individual task included in the target string.\n        \"\"\"\n\n        ret_str = \"\"\n        ret_meta = []\n\n        for idx in range(len(all_tasks)):\n            task = all_tasks[idx]\n            task_type = task[3]\n            task_text = task[1]\n\n            ret_str += self.task_target_start.format(task_nr=idx + 1)\n            ret_str += task_type\n            ret_str += task_text\n            ret_str += self.task_target_end\n            ret_str += \"\\n\"\n\n            curr_meta = task[2]\n            curr_meta[\"task_type\"] = task_type\n            ret_meta.append(task[2])\n\n        return ret_str, ret_meta\n\n    def get_nr_tokens(self, curr_string):\n        \"\"\"\n        Calculates the number of tokens in a given string using the initialized tokenizer.\n\n        Parameters\n        ----------\n        curr_string : str\n            The string to tokenize.\n\n        Returns\n        -------\n        int\n            The number of tokens generated by the tokenizer for the input string.\n        \"\"\"\n        return len(self.tokenizer(curr_string)[\"input_ids\"])\n\n    def forward_conversion(\n        self,\n        forecasting_splits: list[DataSplitterForecastingGroup],\n        event_splits: list[DataSplitterEventsGroup],\n        override_mode_to_select_forecasting: str = None,\n    ) -&gt; dict:\n        \"\"\"\n        Generates a multi-task instruction prompt and target answer from patient data splits.\n\n        This method orchestrates the creation of a training example. It randomly selects\n        scenarios (splits) for event prediction and forecasting tasks from the provided lists.\n        It then uses the specialized sub-converters to generate prompt/target pairs for these\n        tasks (including potentially forecasting QA tasks). These individual tasks are shuffled\n        and combined into a multi-task instruction prompt and a multi-task target answer using\n        helper methods. Patient history (constant features, time-series events, summarized info)\n        is formatted and prepended to the instruction prompt, respecting the total token budget.\n\n        Parameters\n        ----------\n        forecasting_splits : list[DataSplitterForecastingGroup]\n            List of DataSplitterForecastingGroup objects, each representing a potential forecasting task scenario\n            (containing patient history up to a split date and future target values).\n        event_splits : list[DataSplitterEventsGroup]\n            List of DataSplitterEventsGroup objects, each representing a potential event prediction task scenario\n            (containing patient history up to a split date and event outcome/censoring info).\n        override_mode_to_select_forecasting : str, optional\n            If provided, forces the selection mode for forecasting tasks ('forecasting',\n            'forecasting_qa', or 'both'). If None, the mode is chosen randomly. Defaults to None.\n\n        Returns\n        -------\n        dict\n            A dictionary containing the final formatted data:\n            - 'instruction': The complete input string for the model (context + multi-task prompt).\n            - 'answer': The complete target string for the model (multi-task answer).\n            - 'meta': A dictionary holding metadata including patient ID, structured constant and\n                      history data used, split date, combined metadata from sub-converters, and\n                      a list of detailed metadata for each individual task generated ('target_meta_detailed').\n\n        Raises\n        ------\n        AssertionError\n            If input splits have inconsistent Line of Therapy (LOT) dates or split dates,\n            or if no tasks are generated.\n        \"\"\"\n\n        #: make assertions that data has same split and lot date\n        all_lot_dates_events = [x.lot_date for x in event_splits]\n        all_lot_dates_forecasting = [x.lot_date for x in forecasting_splits]\n        assert len(set(all_lot_dates_events) | set(all_lot_dates_forecasting)) == 1\n        all_split_dates_events = [x.split_date_included_in_input for x in event_splits]\n        all_split_dates_forecasting = [x.split_date_included_in_input for x in forecasting_splits]\n        assert len(set(all_split_dates_events) | set(all_split_dates_forecasting)) == 1\n        # assert that the split date is after or equal to lot date (checking 0, since all same)\n        assert len(event_splits) == 0 or all_split_dates_events[0] &gt;= all_lot_dates_events[0]\n\n        #: randomly select the number of events and which splits to use and generate events\n        min_nr_events = 1 if len(forecasting_splits) == 0 else 0\n        nr_events_to_use = np.random.randint(min_nr_events, len(event_splits) + 1)\n        event_splits_to_use = np.random.choice(event_splits, nr_events_to_use, replace=False)\n\n        event_converted = []\n        for event_split in event_splits_to_use:\n            prompt, target, target_meta = self.converter_events.forward_conversion(event_split)\n            event_converted.append((prompt, target, target_meta, self.config.task_prompt_events))\n\n        #: then randomly select the forecasting splits and generate forecasting\n        forecasting_tasks = []\n        forecasting_qa_tasks = []\n\n        if len(forecasting_splits) &gt; 0:\n            if override_mode_to_select_forecasting is None:\n                #: randomly select whether to do forecasting, forecasting QA or both\n                mode_to_select = np.random.choice([\"forecasting\", \"forecasting_qa\", \"both\"], 1, replace=False)[0]\n            else:\n                mode_to_select = override_mode_to_select_forecasting\n\n            # Randomly select from the forecasting split\n            if mode_to_select == \"forecasting\" or mode_to_select == \"both\":\n                forecasting_split_to_use = np.random.choice(forecasting_splits, 1, replace=False)[0]\n                ret_forecasting = self.converter_forecasting.forward_conversion(forecasting_split_to_use)\n                prompt_forecasting, target_forecasting, target_meta_forecasting = ret_forecasting\n                forecasting_tasks.append(\n                    (\n                        prompt_forecasting,\n                        target_forecasting,\n                        target_meta_forecasting,\n                        self.config.task_prompt_forecasting,\n                    )\n                )\n\n            #: then randomly select the QA splits and generate QA\n            if mode_to_select == \"forecasting_qa\" or mode_to_select == \"both\":\n                forecasting_qa_split_to_use = np.random.choice(forecasting_splits, 1, replace=False)[0]\n                ret_forecasting_qa = self.converter_forecasting_qa.forward_conversion(forecasting_qa_split_to_use)\n                (\n                    prompt_forecasting_qa,\n                    target_forecasting_qa,\n                    target_meta_forecasting_qa,\n                ) = ret_forecasting_qa\n                forecasting_qa_tasks.append(\n                    (\n                        prompt_forecasting_qa,\n                        target_forecasting_qa,\n                        target_meta_forecasting_qa,\n                        self.config.task_prompt_forecasting_qa,\n                    )\n                )\n\n        #: determine random order of all tasks\n        all_tasks = forecasting_tasks + forecasting_qa_tasks + event_converted\n        all_tasks = np.random.permutation(all_tasks)\n        assert len(all_tasks) &gt; 0, \"No tasks generated!\"\n\n        #: generate overall prompt\n        prompt = self._generate_prompt(all_tasks)\n\n        #: generate overall target\n        target_str, target_meta = self._generate_target_string(all_tasks)\n\n        #: aggregate all meta needed and add into target_meta for the summarized row\n        combined_meta = {\n            \"dates_per_variable\": {},\n            \"variable_name_mapping\": {},\n        }\n        for curr_meta in target_meta:\n            if \"dates_per_variable\" in curr_meta:\n                combined_meta[\"dates_per_variable\"].update(curr_meta[\"dates_per_variable\"])\n            if \"variable_name_mapping\" in curr_meta:\n                combined_meta[\"variable_name_mapping\"].update(curr_meta[\"variable_name_mapping\"])\n\n        # Since the history and constant are the same, we can just use the first\n        data_to_use = forecasting_splits[0] if len(forecasting_splits) &gt; 0 else event_splits[0]\n        events_until_split = data_to_use.events_until_split\n        constant = data_to_use.constant_data\n        patientid = data_to_use.constant_data[self.config.patient_id_col]\n        split_date_included_in_input = data_to_use.split_date_included_in_input\n\n        #: generate constant string\n        constant, constant_description = self._preprocess_constant_date(\n            events_until_split, constant, self.constant_description\n        )\n        constant_string = self._get_constant_string(constant, constant_description)\n\n        #: generate summarized row string\n        try:\n            summarized_row_string = self._generate_summarized_row_str_fn(events_until_split, combined_meta)\n        except Exception as e:\n            raise TypeError(f\"Custom summarized_row_fn failed: {e}.\")\n\n        #: get current budget\n        budget_total = self.nr_tokens_budget_total\n        budget_total -= self.get_nr_tokens(prompt)\n        budget_total -= self.get_nr_tokens(constant_string)\n        budget_total -= self.get_nr_tokens(summarized_row_string)\n        budget_total -= self.get_nr_tokens(target_str)\n        budget_total -= self.nr_tokens_budget_padding\n\n        #: select events within token budget\n        patient_history = self._get_all_most_recent_events_within_budget(events_until_split, budget_total)\n\n        #: generate history string\n        patient_history_processed = patient_history.copy()\n        patient_history_processed = self._preprocess_events(patient_history_processed)\n        history_str = self._get_event_string(patient_history_processed, use_accumulative_dates=False)\n\n        #: generate final string\n        input_string = self.config.preamble_text + constant_string + history_str + summarized_row_string + prompt\n\n        #: generate return &amp; meta (incl. structured data)\n        ret = {\n            \"instruction\": input_string,\n            \"answer\": target_str,\n            \"meta\": {\n                self.config.patient_id_col: patientid,\n                \"constant_data\": constant,\n                \"history_data\": patient_history,\n                \"split_date_included_in_input\": split_date_included_in_input,\n                \"combined_meta\": combined_meta,\n                \"target_meta_detailed\": target_meta,\n            },\n        }\n        return ret\n\n    def forward_conversion_inference(\n        self,\n        forecasting_split: DataSplitterForecastingOption = None,\n        forecasting_future_weeks_per_variable: dict = None,\n        event_split: DataSplitterEventsOption = None,\n        custom_tasks: list = None,\n    ) -&gt; dict:\n        \"\"\"\n        Generates a multi-task instruction prompt suitable for inference.\n\n        Constructs the input prompt for the model based on provided task scenarios\n        (forecasting, event prediction, and/or custom text tasks). It does not generate\n        a target 'answer' string. Patient history context (constant, time-series, summarized)\n        is generated and prepended, respecting token limits.\n\n        Parameters\n        ----------\n        forecasting_split : DataSplitterForecastingOption, optional\n            Data for a forecasting task scenario. Defaults to None.\n        forecasting_future_weeks_per_variable : dict, optional\n            Specifies future time points (in weeks) for forecasting specific variables.\n            Used by the forecasting sub-converter during inference. Defaults to None.\n        event_split : DataSplitterEventsOption, optional\n            Data for an event prediction task scenario. Defaults to None.\n        custom_tasks : list[str], optional\n            A list of strings, each representing a custom task/question to include\n            in the prompt. Defaults to None.\n\n        Returns\n        -------\n        dict\n            A dictionary containing:\n            - 'instruction': The complete input string for the model (context + multi-task prompt).\n            - 'answer': None (as this is for inference).\n            - 'meta': Metadata including patient ID, structured data used for context,\n                      split date, and metadata associated with the generated tasks.\n\n        Raises\n        ------\n        AssertionError\n            If both `forecasting_split` and `event_split` are None, or if their\n            split dates do not match when both are provided.\n        \"\"\"\n\n        #: make assertions that data has same split date\n        if forecasting_split is not None and event_split is not None:\n            assert forecasting_split.split_date_included_in_input == event_split.split_date_included_in_input, (\n                \"Split dates do not match!\"\n            )\n        assert forecasting_split is not None or event_split is not None, \"No data provided!\"\n\n        #: convert events, if needed\n        event_prompt_str = event_meta = None\n        if event_split is not None:\n            event_prompt_str, event_meta = self.converter_events.forward_conversion_inference(event_split)\n\n        #: convert forecasting, if needed\n        forecasting_prompt_str = forecasting_meta = None\n        if forecasting_split is not None:\n            f_ret = self.converter_forecasting.forward_conversion_inference(\n                forecasting_split,\n                future_weeks_per_variable=forecasting_future_weeks_per_variable,\n            )\n            forecasting_prompt_str, forecasting_meta = f_ret\n\n        #: generate meta and aggregate\n        all_tasks = []\n        target_meta = []\n        if event_prompt_str is not None:\n            all_tasks.append((event_prompt_str, None, event_meta, self.config.task_prompt_events))\n            target_meta.append(event_meta)\n        if forecasting_prompt_str is not None:\n            all_tasks.append(\n                (\n                    forecasting_prompt_str,\n                    None,\n                    forecasting_meta,\n                    self.config.task_prompt_forecasting,\n                )\n            )\n            target_meta.append(forecasting_meta)\n\n        # Add in custom tasks\n        if custom_tasks is not None:\n            for custom_task in custom_tasks:\n                all_tasks.append((custom_task, None, None, self.config.task_prompt_custom + \"\\n\"))\n                target_meta.append({})\n\n        # Generate prompt\n        prompt = self._generate_prompt(all_tasks)\n\n        #: generate constant string\n        data_to_use = forecasting_split if forecasting_split is not None else event_split\n        events_until_split = data_to_use.events_until_split\n        constant = data_to_use.constant_data\n        patientid = data_to_use.constant_data[self.config.patient_id_col]\n        split_date_included_in_input = data_to_use.split_date_included_in_input\n        constant, constant_description = self._preprocess_constant_date(\n            events_until_split, constant, self.constant_description\n        )\n        constant_string = self._get_constant_string(constant, constant_description)\n\n        #: generate summarized row string\n        combined_meta = {\n            \"dates_per_variable\": {},\n            \"variable_name_mapping\": {},\n        }\n        for curr_meta in target_meta:\n            if \"dates_per_variable\" in curr_meta:\n                combined_meta[\"dates_per_variable\"].update(curr_meta[\"dates_per_variable\"])\n            if \"variable_name_mapping\" in curr_meta:\n                combined_meta[\"variable_name_mapping\"].update(curr_meta[\"variable_name_mapping\"])\n        try:\n            summarized_row_string = self._generate_summarized_row_str_fn(events_until_split, combined_meta)\n        except Exception as e:\n            raise TypeError(f\"Custom summarized_row_fn failed: {e}.\")\n\n        #: get current budget\n        budget_total = self.nr_tokens_budget_total\n        budget_total -= self.get_nr_tokens(prompt)\n        budget_total -= self.get_nr_tokens(constant_string)\n        budget_total -= self.get_nr_tokens(summarized_row_string)\n        budget_total -= self.nr_tokens_budget_padding\n\n        #: select events within token budget\n        patient_history = self._get_all_most_recent_events_within_budget(events_until_split, budget_total)\n\n        #: generate history string\n        patient_history_processed = patient_history.copy()\n        patient_history_processed = self._preprocess_events(patient_history_processed)\n        history_str = self._get_event_string(patient_history, use_accumulative_dates=False)\n\n        #: generate final string\n        input_string = self.config.preamble_text + constant_string + history_str + summarized_row_string + prompt\n\n        #: generate return &amp; meta (incl. structured data)\n        ret = {\n            \"instruction\": input_string,\n            \"answer\": None,\n            \"meta\": {\n                self.config.patient_id_col: patientid,\n                \"constant_data\": constant,\n                \"history_data\": patient_history,\n                \"split_date_included_in_input\": split_date_included_in_input,\n                \"combined_meta\": combined_meta,\n                \"target_meta_detailed\": target_meta,\n            },\n        }\n        return ret\n\n    def generate_target_manual(self, reverse_converted, split_date, events_until_split):\n        \"\"\"\n        Reconstructs a multi-task target string from previously reverse-converted structured data.\n\n        This method takes a list of dictionaries (where each dictionary represents the\n        parsed result of a single task from a model's output) and generates the corresponding\n        multi-task target string. It calls the appropriate sub-converter's manual target\n        generation method for each task type.\n\n        Parameters\n        ----------\n        reverse_converted : list[dict]\n            A list of dictionaries, each resulting from the `reverse_conversion` method,\n            containing keys 'task_type' and 'result' (the structured data).\n        split_date : datetime\n            The reference split date, needed for reconstructing forecasting targets.\n        events_until_split : pd.DataFrame\n            Patient's historical event data up to the split date, needed for context\n            by some sub-converters' target generation (e.g., forecasting).\n\n        Returns\n        -------\n        tuple\n            - str: The reconstructed multi-task target string.\n            - list[dict]: A list containing the metadata generated during the reconstruction\n                          of each individual task's target string.\n            - list[dict]: The input `reverse_converted` list, potentially updated with\n                          'original_text' (the reconstructed target snippet for each task)\n                          and 'target_meta'.\n        \"\"\"\n\n        #: go through all reverse converted and generate only the target string\n        individual_targets = []\n\n        for idx, reverse_converted_dic in enumerate(reverse_converted):\n            if reverse_converted_dic[\"task_type\"] == self.config.task_prompt_events:\n                occurred = reverse_converted_dic[\"result\"][\"occurred\"].iloc[0]\n                censored = reverse_converted_dic[\"result\"][\"censoring\"].iloc[0]\n                target_name = reverse_converted_dic[\"result\"][\"target_name\"].iloc[0]\n\n                ret_prompt, ret_meta = self.converter_events.generate_target_manual(\n                    target_name=target_name,\n                    event_occurred=occurred,\n                    event_censored=censored,\n                )\n                individual_targets.append((None, ret_prompt, {}, self.config.task_prompt_events))\n                reverse_converted[idx][\"original_text\"] = ret_prompt\n\n            if reverse_converted_dic[\"task_type\"] == self.config.task_prompt_forecasting:\n                ret_prompt, ret_meta = self.converter_forecasting.generate_target_manual(\n                    split_date_included_in_input=split_date,\n                    events_until_split=events_until_split,\n                    target_events_after_split=reverse_converted_dic[\"result\"],\n                )\n                individual_targets.append((None, ret_prompt, {}, self.config.task_prompt_forecasting))\n                reverse_converted[idx][\"original_text\"] = ret_prompt\n                reverse_converted[idx][\"target_meta\"] = ret_meta\n\n            if reverse_converted_dic[\"task_type\"] == self.config.task_prompt_custom:\n                individual_targets.append(\n                    (\n                        None,\n                        reverse_converted_dic[\"result\"],\n                        {},\n                        self.config.task_prompt_custom,\n                    )\n                )\n                reverse_converted[idx][\"original_text\"] = reverse_converted_dic[\"result\"]\n\n        #: then aggegrate all target strings with own\n        target_str, target_meta = self._generate_target_string(individual_targets)\n\n        # Return\n        return target_str, target_meta, reverse_converted\n\n    def aggregate_multiple_responses(self, all_responses: list[list[dict]]) -&gt; dict:\n        \"\"\"\n        Aggregates multiple model responses for the same multi-task prompt.\n\n        Takes a list of responses, where each response is itself a list of parsed results\n        (one dictionary per task from the `reverse_conversion` method). It groups the results\n        for each task position across all responses and then calls the appropriate\n        sub-converter's aggregation method to determine the most likely result for that task.\n\n        Parameters\n        ----------\n        all_responses : list[list[dict]]\n            A list where each inner list corresponds to one full model response (parsed by\n            `reverse_conversion`) to the multi-task prompt. Each dictionary in the inner list\n            should contain 'task_type' and 'result'.\n\n        Returns\n        -------\n        list[dict]\n            A list of dictionaries, one for each task position in the original prompt. Each\n            dictionary contains:\n            - 'task_type': The type of the task at this position.\n            - 'result': The aggregated result (e.g., the most common structured data).\n            - 'original_text': None (as this aggregates structured results, not text).\n            - 'aggregated_meta': Metadata about the aggregation process (e.g., distribution\n                               of responses for this task), provided by the sub-converter.\n\n        Raises\n        ------\n        ValueError\n            If `all_responses` is empty or if the inner lists have inconsistent lengths.\n        \"\"\"\n\n        #: first restructure the format to get appropriate responses together\n        ret_order = {idx: [] for idx in range(len(all_responses[0]))}\n\n        for response_list in all_responses:\n            # This is one response, which is a list since we may have multiple tasks per response\n            for idx, response in enumerate(response_list):\n                ret_order[idx].append(response)\n\n        #: for each, aggregate by correctly calling the correct converter\n        ret_aggregated = []\n\n        for idx in range(max(ret_order.keys()) + 1):\n            task_type = ret_order[idx][0][\"task_type\"]\n            results = [x[\"result\"] for x in ret_order[idx]]\n\n            if task_type == self.config.task_prompt_events:\n                ret, meta = self.converter_events.aggregate_multiple_responses(results)\n            if task_type == self.config.task_prompt_forecasting:\n                ret, meta = self.converter_forecasting.aggregate_multiple_responses(results)\n            if task_type == self.config.task_prompt_forecasting_qa:\n                ret, meta = self.converter_forecasting_qa.aggregate_multiple_responses(results)\n            if task_type == self.config.task_prompt_custom:\n                # Aggregate by taking the most common answer\n                all_responses_count = Counter([x[\"original_text\"] for x in results])\n                most_common_response = all_responses_count.most_common(1)[0][0]\n                ret = most_common_response\n                # Get all responses and their counts as DF\n                all_responses_count = pd.DataFrame(all_responses_count.items(), columns=[\"response\", \"count\"])\n                all_responses_count = all_responses_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n                meta = {\n                    \"all_custom_responses\": all_responses_count,\n                }\n\n            # Set as appropriate dic\n            ret_dic = {\n                \"task_type\": task_type,\n                \"result\": ret,\n                \"original_text\": None,\n                \"aggregated_meta\": meta,\n            }\n            ret_aggregated.append(ret_dic)\n\n        #: return\n        return ret_aggregated\n\n    def reverse_conversion(\n        self,\n        target_string: str,\n        data_manager: DataManager,\n        split_date: datetime,\n        patientid: str = None,\n        inference_override: bool = False,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Parses a multi-task model output string back into a list of structured results.\n\n        Splits the input string based on task markers (e.g., \"Task 1:\", \"Task 2:\").\n        For each segment, it identifies the task type using configured prompt identifiers\n        and calls the corresponding sub-converter's `reverse_conversion` method to parse\n        the text into structured data (e.g., a DataFrame).\n\n        Parameters\n        ----------\n        target_string : str\n            The complete multi-task output string generated by the language model.\n        data_manager : DataManager\n            The data manager instance, needed by some sub-converters for context\n            (e.g., unique event mappings).\n        split_date : datetime\n            The reference split date, required by some sub-converters (e.g., forecasting)\n            to interpret time-related outputs correctly.\n        patientid : str, optional\n            If provided, this patient ID is added to the resulting structured data\n            for each task. Defaults to None.\n        inference_override : bool, optional\n            If True, allows processing even if a task type cannot be strictly identified\n            (e.g., by falling back to heuristics). Primarily for inference robustness\n            where model output might be slightly malformed. Defaults to False.\n\n        Returns\n        -------\n        list[dict]\n            A list of dictionaries, one for each task identified and parsed in the\n            `target_string`. Each dictionary contains:\n            - 'task_type': The identified type of the task.\n            - 'original_text': The raw text segment corresponding to this task's answer.\n            - 'result': The structured data parsed from 'original_text' by the appropriate\n                        sub-converter (e.g., a pandas DataFrame).\n\n        Raises\n        ------\n        ValueError\n            If `inference_override` is False and the type of a task segment cannot be\n            determined using the configured prompt identifiers.\n        \"\"\"\n\n        #: split up by task\n        all_tasks = target_string.split(\"Task\")\n        ret_list = []\n\n        for curr_task in all_tasks:\n            # Basic preprocessing\n            curr_task = curr_task.strip()\n            if len(curr_task) == 0:\n                continue\n\n            #: determine which task it is\n            task_type = None\n            standard_extraction = True\n\n            if self.config.task_prompt_forecasting in curr_task:\n                task_type = self.config.task_prompt_forecasting\n            elif self.config.task_prompt_forecasting_qa in curr_task:\n                task_type = self.config.task_prompt_forecasting_qa\n            elif self.config.task_prompt_events in curr_task:\n                task_type = self.config.task_prompt_events\n            elif \"custom\" in curr_task:\n                task_type = self.config.task_prompt_custom\n            else:\n                #: Try determining by whether the task contains a \"censored\"\n                if \"censored\" in curr_task:\n                    task_type = self.config.task_prompt_events\n                    standard_extraction = False\n                elif inference_override is False:\n                    raise ValueError(\"Could not determine task type\")\n\n            #: extract the relevant parts\n            if standard_extraction:\n                curr_task_text = curr_task.split(task_type)[1]\n            else:\n                curr_task_text = \":\".join(curr_task.split(\":\")[1:])\n\n                # Often we have \"[\"\"]\" which need to be removed\n                curr_task_text = curr_task_text.replace(\"[\", \"\")\n                curr_task_text = curr_task_text.replace(\"]\", \"\")\n\n            #: pass to correct reverse converter\n            if task_type == self.config.task_prompt_forecasting:\n                ret = self.converter_forecasting.reverse_conversion(\n                    curr_task_text, data_manager.unique_events, split_date\n                )\n            elif task_type == self.config.task_prompt_forecasting_qa:\n                ret = self.converter_forecasting_qa.reverse_conversion(\n                    curr_task_text, data_manager.unique_events, split_date\n                )\n            elif task_type == self.config.task_prompt_events:\n                ret = self.converter_events.reverse_conversion(curr_task_text)\n\n            elif task_type == self.config.task_prompt_custom:\n                ret = {\"original_text\": curr_task_text}\n\n            if patientid is not None:\n                ret[self.config.patient_id_col] = patientid\n\n            ret_dic = {\n                \"task_type\": task_type,\n                \"original_text\": curr_task_text,\n                \"result\": ret,\n            }\n\n            #: make list of results\n            ret_list.append(ret_dic)\n\n        #: return list of results\n        return ret_list\n\n    def get_difference_in_event_dataframes(\n        self,\n        list_of_original_conversions_meta: list[dict],\n        list_of_reversed_conversions: list[dict],\n    ) -&gt; list[pd.DataFrame]:\n        \"\"\"\n        Compares original structured data with reverse-converted data for each task in a multi-task scenario.\n\n        Iterates through the tasks, matching the original metadata (which contains the\n        ground truth structured data, e.g., in 'target_data_processed') with the result\n        of the reverse conversion for that task. It then calls the appropriate sub-converter's\n        difference calculation method.\n\n        Parameters\n        ----------\n        list_of_original_conversions_meta : list[dict]\n            The list of metadata dictionaries generated during the *forward* conversion,\n            where each dictionary corresponds to a task and should contain the original\n            structured target data (e.g., under 'target_data_processed'). Usually obtained\n            from the 'target_meta_detailed' key in the forward conversion output meta.\n        list_of_reversed_conversions : list[dict]\n            The list of dictionaries resulting from the `reverse_conversion` method, where\n            each dictionary contains the parsed structured data ('result') for a task.\n\n        Returns\n        -------\n        list[pd.DataFrame]\n            A list of pandas DataFrames. Each DataFrame highlights the differences found\n            for the corresponding task between the original data and the reverse-converted data.\n            An empty DataFrame indicates no differences were found for that task.\n\n        Raises\n        ------\n        AssertionError\n            If the number of original metadata items does not match the number of reversed\n            conversion results.\n        ValueError\n             If a task type mismatch is detected or if expected data keys are missing.\n        \"\"\"\n\n        # Check and setup data\n        # list_of_original_conversions_meta = original_conversion[\"meta\"][\"target_meta_detailed\"]\n\n        assert len(list_of_original_conversions_meta) == len(list_of_reversed_conversions)\n        ret_diffs = []\n\n        #: go through each task and compare using the correct converter\n        for idx in range(len(list_of_original_conversions_meta)):\n            original = list_of_original_conversions_meta[idx]\n            reversed = list_of_reversed_conversions[idx]\n\n            if reversed[\"task_type\"] == self.config.task_prompt_forecasting:\n                ret_diff = self.converter_forecasting.get_difference_in_event_dataframes(\n                    reversed[\"result\"], original[\"target_data_processed\"]\n                )\n            elif original[\"task_type\"] == self.config.task_prompt_forecasting_qa:\n                ret_diff = self.converter_forecasting_qa.get_difference_in_event_dataframes(\n                    reversed[\"result\"], original[\"target_data_processed\"]\n                )\n            elif original[\"task_type\"] == self.config.task_prompt_events:\n                ret_diff = self.converter_events.get_difference_in_event_dataframes(\n                    reversed[\"result\"], original[\"target_data_processed\"]\n                )\n\n            ret_diffs.append(ret_diff)\n\n        #: return list of differences\n        return ret_diffs\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction-functions","title":"Functions","text":""},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.__init__","title":"__init__","text":"<pre><code>__init__(\n    nr_tokens_budget_total, config, dm, variable_stats=None\n)\n</code></pre> <p>Initializes the ConverterInstruction class.</p> <p>Sets up the main converter by initializing base class settings, storing essential data descriptions and token budgets, configuring the tokenizer, loading multi-task prompt templates from the config, and initializing instances of the specialized sub-converters required for different task types.</p> <p>Parameters:</p> Name Type Description Default <code>nr_tokens_budget_total</code> <code>int</code> <p>The total token budget available for the generated context + instruction prompt. Used for dynamically selecting history length.</p> required <code>config</code> <code>Config</code> <p>A configuration object supplying tokenizer details, prompt templates (for tasks and context generation), padding values, and other settings.</p> required <code>dm</code> <code>DataManager</code> <p>The data manager instance providing access to data frames and variable types.</p> required <code>variable_stats</code> <code>DataFrame</code> <p>DataFrame containing statistics about variables, used by the forecasting QA converter.</p> <code>None</code> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def __init__(\n    self,\n    nr_tokens_budget_total: int,\n    config: Config,\n    dm: DataManager,\n    variable_stats: pd.DataFrame = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the ConverterInstruction class.\n\n    Sets up the main converter by initializing base class settings, storing essential\n    data descriptions and token budgets, configuring the tokenizer, loading multi-task\n    prompt templates from the config, and initializing instances of the specialized\n    sub-converters required for different task types.\n\n    Parameters\n    ----------\n    nr_tokens_budget_total : int\n        The total token budget available for the generated context + instruction prompt.\n        Used for dynamically selecting history length.\n    config : Config\n        A configuration object supplying tokenizer details, prompt templates (for tasks\n        and context generation), padding values, and other settings.\n    dm : DataManager\n        The data manager instance providing access to data frames and variable types.\n    variable_stats : pd.DataFrame, optional\n        DataFrame containing statistics about variables, used by the forecasting QA converter.\n    \"\"\"\n\n    super().__init__(config)\n\n    # Set vars\n    self.nr_tokens_budget_total = nr_tokens_budget_total\n    self.dm = dm\n    self.constant_description = self.dm.data_frames[\"constant_description\"]\n\n    # Set forecasting prompt texts using overrides or config defaults\n    self.forecasting_prompt_start = config.forecasting_fval_prompt_start\n    self.forecasting_prompt_var_time = config.forecasting_prompt_var_time\n    self.forecasting_prompt_summarized_start = config.forecasting_prompt_summarized_start\n    self.forecasting_prompt_summarized_genetic = config.forecasting_prompt_summarized_genetic\n    self.forecasting_prompt_summarized_lot = config.forecasting_prompt_summarized_lot\n\n    # Initialize tokenizer and related settings\n    self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_to_use)\n    self.nr_tokens_budget_padding = self.config.nr_tokens_budget_padding\n    self.always_keep_first_visit = self.config.always_keep_first_visit\n\n    # Set multi-task prompt texts using overrides or config defaults\n    self.task_prompt_start = config.task_prompt_start\n    self.task_prompt_each_task = config.task_prompt_each_task\n    self.task_prompt_end = config.task_prompt_end\n    self.task_target_start = config.task_target_start\n    self.task_target_end = config.task_target_end\n\n    # Initialize converters - can this be done more elegantly?\n    self.converter_forecasting = ConverterForecasting(\n        self.constant_description, nr_tokens_budget_total, config=config, dm=dm\n    )\n\n    self.converter_events = ConverterEvents(\n        config,\n        self.constant_description,\n        nr_tokens_budget_total,\n    )\n\n    self.converter_forecasting_qa = ConverterForecastingQA(\n        variable_stats=variable_stats,\n        constant_description=self.constant_description,\n        nr_tokens_budget_total=nr_tokens_budget_total,\n        config=config,\n        dm=dm,\n    )\n\n    # Default summarized row function\n    self._generate_summarized_row_str_fn = self._generate_summarized_row_string\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.aggregate_multiple_responses","title":"aggregate_multiple_responses","text":"<pre><code>aggregate_multiple_responses(all_responses)\n</code></pre> <p>Aggregates multiple model responses for the same multi-task prompt.</p> <p>Takes a list of responses, where each response is itself a list of parsed results (one dictionary per task from the <code>reverse_conversion</code> method). It groups the results for each task position across all responses and then calls the appropriate sub-converter's aggregation method to determine the most likely result for that task.</p> <p>Parameters:</p> Name Type Description Default <code>all_responses</code> <code>list[list[dict]]</code> <p>A list where each inner list corresponds to one full model response (parsed by <code>reverse_conversion</code>) to the multi-task prompt. Each dictionary in the inner list should contain 'task_type' and 'result'.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries, one for each task position in the original prompt. Each dictionary contains: - 'task_type': The type of the task at this position. - 'result': The aggregated result (e.g., the most common structured data). - 'original_text': None (as this aggregates structured results, not text). - 'aggregated_meta': Metadata about the aggregation process (e.g., distribution                    of responses for this task), provided by the sub-converter.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>all_responses</code> is empty or if the inner lists have inconsistent lengths.</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def aggregate_multiple_responses(self, all_responses: list[list[dict]]) -&gt; dict:\n    \"\"\"\n    Aggregates multiple model responses for the same multi-task prompt.\n\n    Takes a list of responses, where each response is itself a list of parsed results\n    (one dictionary per task from the `reverse_conversion` method). It groups the results\n    for each task position across all responses and then calls the appropriate\n    sub-converter's aggregation method to determine the most likely result for that task.\n\n    Parameters\n    ----------\n    all_responses : list[list[dict]]\n        A list where each inner list corresponds to one full model response (parsed by\n        `reverse_conversion`) to the multi-task prompt. Each dictionary in the inner list\n        should contain 'task_type' and 'result'.\n\n    Returns\n    -------\n    list[dict]\n        A list of dictionaries, one for each task position in the original prompt. Each\n        dictionary contains:\n        - 'task_type': The type of the task at this position.\n        - 'result': The aggregated result (e.g., the most common structured data).\n        - 'original_text': None (as this aggregates structured results, not text).\n        - 'aggregated_meta': Metadata about the aggregation process (e.g., distribution\n                           of responses for this task), provided by the sub-converter.\n\n    Raises\n    ------\n    ValueError\n        If `all_responses` is empty or if the inner lists have inconsistent lengths.\n    \"\"\"\n\n    #: first restructure the format to get appropriate responses together\n    ret_order = {idx: [] for idx in range(len(all_responses[0]))}\n\n    for response_list in all_responses:\n        # This is one response, which is a list since we may have multiple tasks per response\n        for idx, response in enumerate(response_list):\n            ret_order[idx].append(response)\n\n    #: for each, aggregate by correctly calling the correct converter\n    ret_aggregated = []\n\n    for idx in range(max(ret_order.keys()) + 1):\n        task_type = ret_order[idx][0][\"task_type\"]\n        results = [x[\"result\"] for x in ret_order[idx]]\n\n        if task_type == self.config.task_prompt_events:\n            ret, meta = self.converter_events.aggregate_multiple_responses(results)\n        if task_type == self.config.task_prompt_forecasting:\n            ret, meta = self.converter_forecasting.aggregate_multiple_responses(results)\n        if task_type == self.config.task_prompt_forecasting_qa:\n            ret, meta = self.converter_forecasting_qa.aggregate_multiple_responses(results)\n        if task_type == self.config.task_prompt_custom:\n            # Aggregate by taking the most common answer\n            all_responses_count = Counter([x[\"original_text\"] for x in results])\n            most_common_response = all_responses_count.most_common(1)[0][0]\n            ret = most_common_response\n            # Get all responses and their counts as DF\n            all_responses_count = pd.DataFrame(all_responses_count.items(), columns=[\"response\", \"count\"])\n            all_responses_count = all_responses_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n            meta = {\n                \"all_custom_responses\": all_responses_count,\n            }\n\n        # Set as appropriate dic\n        ret_dic = {\n            \"task_type\": task_type,\n            \"result\": ret,\n            \"original_text\": None,\n            \"aggregated_meta\": meta,\n        }\n        ret_aggregated.append(ret_dic)\n\n    #: return\n    return ret_aggregated\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.forward_conversion","title":"forward_conversion","text":"<pre><code>forward_conversion(\n    forecasting_splits,\n    event_splits,\n    override_mode_to_select_forecasting=None,\n)\n</code></pre> <p>Generates a multi-task instruction prompt and target answer from patient data splits.</p> <p>This method orchestrates the creation of a training example. It randomly selects scenarios (splits) for event prediction and forecasting tasks from the provided lists. It then uses the specialized sub-converters to generate prompt/target pairs for these tasks (including potentially forecasting QA tasks). These individual tasks are shuffled and combined into a multi-task instruction prompt and a multi-task target answer using helper methods. Patient history (constant features, time-series events, summarized info) is formatted and prepended to the instruction prompt, respecting the total token budget.</p> <p>Parameters:</p> Name Type Description Default <code>forecasting_splits</code> <code>list[DataSplitterForecastingGroup]</code> <p>List of DataSplitterForecastingGroup objects, each representing a potential forecasting task scenario (containing patient history up to a split date and future target values).</p> required <code>event_splits</code> <code>list[DataSplitterEventsGroup]</code> <p>List of DataSplitterEventsGroup objects, each representing a potential event prediction task scenario (containing patient history up to a split date and event outcome/censoring info).</p> required <code>override_mode_to_select_forecasting</code> <code>str</code> <p>If provided, forces the selection mode for forecasting tasks ('forecasting', 'forecasting_qa', or 'both'). If None, the mode is chosen randomly. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the final formatted data: - 'instruction': The complete input string for the model (context + multi-task prompt). - 'answer': The complete target string for the model (multi-task answer). - 'meta': A dictionary holding metadata including patient ID, structured constant and           history data used, split date, combined metadata from sub-converters, and           a list of detailed metadata for each individual task generated ('target_meta_detailed').</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If input splits have inconsistent Line of Therapy (LOT) dates or split dates, or if no tasks are generated.</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def forward_conversion(\n    self,\n    forecasting_splits: list[DataSplitterForecastingGroup],\n    event_splits: list[DataSplitterEventsGroup],\n    override_mode_to_select_forecasting: str = None,\n) -&gt; dict:\n    \"\"\"\n    Generates a multi-task instruction prompt and target answer from patient data splits.\n\n    This method orchestrates the creation of a training example. It randomly selects\n    scenarios (splits) for event prediction and forecasting tasks from the provided lists.\n    It then uses the specialized sub-converters to generate prompt/target pairs for these\n    tasks (including potentially forecasting QA tasks). These individual tasks are shuffled\n    and combined into a multi-task instruction prompt and a multi-task target answer using\n    helper methods. Patient history (constant features, time-series events, summarized info)\n    is formatted and prepended to the instruction prompt, respecting the total token budget.\n\n    Parameters\n    ----------\n    forecasting_splits : list[DataSplitterForecastingGroup]\n        List of DataSplitterForecastingGroup objects, each representing a potential forecasting task scenario\n        (containing patient history up to a split date and future target values).\n    event_splits : list[DataSplitterEventsGroup]\n        List of DataSplitterEventsGroup objects, each representing a potential event prediction task scenario\n        (containing patient history up to a split date and event outcome/censoring info).\n    override_mode_to_select_forecasting : str, optional\n        If provided, forces the selection mode for forecasting tasks ('forecasting',\n        'forecasting_qa', or 'both'). If None, the mode is chosen randomly. Defaults to None.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the final formatted data:\n        - 'instruction': The complete input string for the model (context + multi-task prompt).\n        - 'answer': The complete target string for the model (multi-task answer).\n        - 'meta': A dictionary holding metadata including patient ID, structured constant and\n                  history data used, split date, combined metadata from sub-converters, and\n                  a list of detailed metadata for each individual task generated ('target_meta_detailed').\n\n    Raises\n    ------\n    AssertionError\n        If input splits have inconsistent Line of Therapy (LOT) dates or split dates,\n        or if no tasks are generated.\n    \"\"\"\n\n    #: make assertions that data has same split and lot date\n    all_lot_dates_events = [x.lot_date for x in event_splits]\n    all_lot_dates_forecasting = [x.lot_date for x in forecasting_splits]\n    assert len(set(all_lot_dates_events) | set(all_lot_dates_forecasting)) == 1\n    all_split_dates_events = [x.split_date_included_in_input for x in event_splits]\n    all_split_dates_forecasting = [x.split_date_included_in_input for x in forecasting_splits]\n    assert len(set(all_split_dates_events) | set(all_split_dates_forecasting)) == 1\n    # assert that the split date is after or equal to lot date (checking 0, since all same)\n    assert len(event_splits) == 0 or all_split_dates_events[0] &gt;= all_lot_dates_events[0]\n\n    #: randomly select the number of events and which splits to use and generate events\n    min_nr_events = 1 if len(forecasting_splits) == 0 else 0\n    nr_events_to_use = np.random.randint(min_nr_events, len(event_splits) + 1)\n    event_splits_to_use = np.random.choice(event_splits, nr_events_to_use, replace=False)\n\n    event_converted = []\n    for event_split in event_splits_to_use:\n        prompt, target, target_meta = self.converter_events.forward_conversion(event_split)\n        event_converted.append((prompt, target, target_meta, self.config.task_prompt_events))\n\n    #: then randomly select the forecasting splits and generate forecasting\n    forecasting_tasks = []\n    forecasting_qa_tasks = []\n\n    if len(forecasting_splits) &gt; 0:\n        if override_mode_to_select_forecasting is None:\n            #: randomly select whether to do forecasting, forecasting QA or both\n            mode_to_select = np.random.choice([\"forecasting\", \"forecasting_qa\", \"both\"], 1, replace=False)[0]\n        else:\n            mode_to_select = override_mode_to_select_forecasting\n\n        # Randomly select from the forecasting split\n        if mode_to_select == \"forecasting\" or mode_to_select == \"both\":\n            forecasting_split_to_use = np.random.choice(forecasting_splits, 1, replace=False)[0]\n            ret_forecasting = self.converter_forecasting.forward_conversion(forecasting_split_to_use)\n            prompt_forecasting, target_forecasting, target_meta_forecasting = ret_forecasting\n            forecasting_tasks.append(\n                (\n                    prompt_forecasting,\n                    target_forecasting,\n                    target_meta_forecasting,\n                    self.config.task_prompt_forecasting,\n                )\n            )\n\n        #: then randomly select the QA splits and generate QA\n        if mode_to_select == \"forecasting_qa\" or mode_to_select == \"both\":\n            forecasting_qa_split_to_use = np.random.choice(forecasting_splits, 1, replace=False)[0]\n            ret_forecasting_qa = self.converter_forecasting_qa.forward_conversion(forecasting_qa_split_to_use)\n            (\n                prompt_forecasting_qa,\n                target_forecasting_qa,\n                target_meta_forecasting_qa,\n            ) = ret_forecasting_qa\n            forecasting_qa_tasks.append(\n                (\n                    prompt_forecasting_qa,\n                    target_forecasting_qa,\n                    target_meta_forecasting_qa,\n                    self.config.task_prompt_forecasting_qa,\n                )\n            )\n\n    #: determine random order of all tasks\n    all_tasks = forecasting_tasks + forecasting_qa_tasks + event_converted\n    all_tasks = np.random.permutation(all_tasks)\n    assert len(all_tasks) &gt; 0, \"No tasks generated!\"\n\n    #: generate overall prompt\n    prompt = self._generate_prompt(all_tasks)\n\n    #: generate overall target\n    target_str, target_meta = self._generate_target_string(all_tasks)\n\n    #: aggregate all meta needed and add into target_meta for the summarized row\n    combined_meta = {\n        \"dates_per_variable\": {},\n        \"variable_name_mapping\": {},\n    }\n    for curr_meta in target_meta:\n        if \"dates_per_variable\" in curr_meta:\n            combined_meta[\"dates_per_variable\"].update(curr_meta[\"dates_per_variable\"])\n        if \"variable_name_mapping\" in curr_meta:\n            combined_meta[\"variable_name_mapping\"].update(curr_meta[\"variable_name_mapping\"])\n\n    # Since the history and constant are the same, we can just use the first\n    data_to_use = forecasting_splits[0] if len(forecasting_splits) &gt; 0 else event_splits[0]\n    events_until_split = data_to_use.events_until_split\n    constant = data_to_use.constant_data\n    patientid = data_to_use.constant_data[self.config.patient_id_col]\n    split_date_included_in_input = data_to_use.split_date_included_in_input\n\n    #: generate constant string\n    constant, constant_description = self._preprocess_constant_date(\n        events_until_split, constant, self.constant_description\n    )\n    constant_string = self._get_constant_string(constant, constant_description)\n\n    #: generate summarized row string\n    try:\n        summarized_row_string = self._generate_summarized_row_str_fn(events_until_split, combined_meta)\n    except Exception as e:\n        raise TypeError(f\"Custom summarized_row_fn failed: {e}.\")\n\n    #: get current budget\n    budget_total = self.nr_tokens_budget_total\n    budget_total -= self.get_nr_tokens(prompt)\n    budget_total -= self.get_nr_tokens(constant_string)\n    budget_total -= self.get_nr_tokens(summarized_row_string)\n    budget_total -= self.get_nr_tokens(target_str)\n    budget_total -= self.nr_tokens_budget_padding\n\n    #: select events within token budget\n    patient_history = self._get_all_most_recent_events_within_budget(events_until_split, budget_total)\n\n    #: generate history string\n    patient_history_processed = patient_history.copy()\n    patient_history_processed = self._preprocess_events(patient_history_processed)\n    history_str = self._get_event_string(patient_history_processed, use_accumulative_dates=False)\n\n    #: generate final string\n    input_string = self.config.preamble_text + constant_string + history_str + summarized_row_string + prompt\n\n    #: generate return &amp; meta (incl. structured data)\n    ret = {\n        \"instruction\": input_string,\n        \"answer\": target_str,\n        \"meta\": {\n            self.config.patient_id_col: patientid,\n            \"constant_data\": constant,\n            \"history_data\": patient_history,\n            \"split_date_included_in_input\": split_date_included_in_input,\n            \"combined_meta\": combined_meta,\n            \"target_meta_detailed\": target_meta,\n        },\n    }\n    return ret\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.forward_conversion_inference","title":"forward_conversion_inference","text":"<pre><code>forward_conversion_inference(\n    forecasting_split=None,\n    forecasting_future_weeks_per_variable=None,\n    event_split=None,\n    custom_tasks=None,\n)\n</code></pre> <p>Generates a multi-task instruction prompt suitable for inference.</p> <p>Constructs the input prompt for the model based on provided task scenarios (forecasting, event prediction, and/or custom text tasks). It does not generate a target 'answer' string. Patient history context (constant, time-series, summarized) is generated and prepended, respecting token limits.</p> <p>Parameters:</p> Name Type Description Default <code>forecasting_split</code> <code>DataSplitterForecastingOption</code> <p>Data for a forecasting task scenario. Defaults to None.</p> <code>None</code> <code>forecasting_future_weeks_per_variable</code> <code>dict</code> <p>Specifies future time points (in weeks) for forecasting specific variables. Used by the forecasting sub-converter during inference. Defaults to None.</p> <code>None</code> <code>event_split</code> <code>DataSplitterEventsOption</code> <p>Data for an event prediction task scenario. Defaults to None.</p> <code>None</code> <code>custom_tasks</code> <code>list[str]</code> <p>A list of strings, each representing a custom task/question to include in the prompt. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing: - 'instruction': The complete input string for the model (context + multi-task prompt). - 'answer': None (as this is for inference). - 'meta': Metadata including patient ID, structured data used for context,           split date, and metadata associated with the generated tasks.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If both <code>forecasting_split</code> and <code>event_split</code> are None, or if their split dates do not match when both are provided.</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def forward_conversion_inference(\n    self,\n    forecasting_split: DataSplitterForecastingOption = None,\n    forecasting_future_weeks_per_variable: dict = None,\n    event_split: DataSplitterEventsOption = None,\n    custom_tasks: list = None,\n) -&gt; dict:\n    \"\"\"\n    Generates a multi-task instruction prompt suitable for inference.\n\n    Constructs the input prompt for the model based on provided task scenarios\n    (forecasting, event prediction, and/or custom text tasks). It does not generate\n    a target 'answer' string. Patient history context (constant, time-series, summarized)\n    is generated and prepended, respecting token limits.\n\n    Parameters\n    ----------\n    forecasting_split : DataSplitterForecastingOption, optional\n        Data for a forecasting task scenario. Defaults to None.\n    forecasting_future_weeks_per_variable : dict, optional\n        Specifies future time points (in weeks) for forecasting specific variables.\n        Used by the forecasting sub-converter during inference. Defaults to None.\n    event_split : DataSplitterEventsOption, optional\n        Data for an event prediction task scenario. Defaults to None.\n    custom_tasks : list[str], optional\n        A list of strings, each representing a custom task/question to include\n        in the prompt. Defaults to None.\n\n    Returns\n    -------\n    dict\n        A dictionary containing:\n        - 'instruction': The complete input string for the model (context + multi-task prompt).\n        - 'answer': None (as this is for inference).\n        - 'meta': Metadata including patient ID, structured data used for context,\n                  split date, and metadata associated with the generated tasks.\n\n    Raises\n    ------\n    AssertionError\n        If both `forecasting_split` and `event_split` are None, or if their\n        split dates do not match when both are provided.\n    \"\"\"\n\n    #: make assertions that data has same split date\n    if forecasting_split is not None and event_split is not None:\n        assert forecasting_split.split_date_included_in_input == event_split.split_date_included_in_input, (\n            \"Split dates do not match!\"\n        )\n    assert forecasting_split is not None or event_split is not None, \"No data provided!\"\n\n    #: convert events, if needed\n    event_prompt_str = event_meta = None\n    if event_split is not None:\n        event_prompt_str, event_meta = self.converter_events.forward_conversion_inference(event_split)\n\n    #: convert forecasting, if needed\n    forecasting_prompt_str = forecasting_meta = None\n    if forecasting_split is not None:\n        f_ret = self.converter_forecasting.forward_conversion_inference(\n            forecasting_split,\n            future_weeks_per_variable=forecasting_future_weeks_per_variable,\n        )\n        forecasting_prompt_str, forecasting_meta = f_ret\n\n    #: generate meta and aggregate\n    all_tasks = []\n    target_meta = []\n    if event_prompt_str is not None:\n        all_tasks.append((event_prompt_str, None, event_meta, self.config.task_prompt_events))\n        target_meta.append(event_meta)\n    if forecasting_prompt_str is not None:\n        all_tasks.append(\n            (\n                forecasting_prompt_str,\n                None,\n                forecasting_meta,\n                self.config.task_prompt_forecasting,\n            )\n        )\n        target_meta.append(forecasting_meta)\n\n    # Add in custom tasks\n    if custom_tasks is not None:\n        for custom_task in custom_tasks:\n            all_tasks.append((custom_task, None, None, self.config.task_prompt_custom + \"\\n\"))\n            target_meta.append({})\n\n    # Generate prompt\n    prompt = self._generate_prompt(all_tasks)\n\n    #: generate constant string\n    data_to_use = forecasting_split if forecasting_split is not None else event_split\n    events_until_split = data_to_use.events_until_split\n    constant = data_to_use.constant_data\n    patientid = data_to_use.constant_data[self.config.patient_id_col]\n    split_date_included_in_input = data_to_use.split_date_included_in_input\n    constant, constant_description = self._preprocess_constant_date(\n        events_until_split, constant, self.constant_description\n    )\n    constant_string = self._get_constant_string(constant, constant_description)\n\n    #: generate summarized row string\n    combined_meta = {\n        \"dates_per_variable\": {},\n        \"variable_name_mapping\": {},\n    }\n    for curr_meta in target_meta:\n        if \"dates_per_variable\" in curr_meta:\n            combined_meta[\"dates_per_variable\"].update(curr_meta[\"dates_per_variable\"])\n        if \"variable_name_mapping\" in curr_meta:\n            combined_meta[\"variable_name_mapping\"].update(curr_meta[\"variable_name_mapping\"])\n    try:\n        summarized_row_string = self._generate_summarized_row_str_fn(events_until_split, combined_meta)\n    except Exception as e:\n        raise TypeError(f\"Custom summarized_row_fn failed: {e}.\")\n\n    #: get current budget\n    budget_total = self.nr_tokens_budget_total\n    budget_total -= self.get_nr_tokens(prompt)\n    budget_total -= self.get_nr_tokens(constant_string)\n    budget_total -= self.get_nr_tokens(summarized_row_string)\n    budget_total -= self.nr_tokens_budget_padding\n\n    #: select events within token budget\n    patient_history = self._get_all_most_recent_events_within_budget(events_until_split, budget_total)\n\n    #: generate history string\n    patient_history_processed = patient_history.copy()\n    patient_history_processed = self._preprocess_events(patient_history_processed)\n    history_str = self._get_event_string(patient_history, use_accumulative_dates=False)\n\n    #: generate final string\n    input_string = self.config.preamble_text + constant_string + history_str + summarized_row_string + prompt\n\n    #: generate return &amp; meta (incl. structured data)\n    ret = {\n        \"instruction\": input_string,\n        \"answer\": None,\n        \"meta\": {\n            self.config.patient_id_col: patientid,\n            \"constant_data\": constant,\n            \"history_data\": patient_history,\n            \"split_date_included_in_input\": split_date_included_in_input,\n            \"combined_meta\": combined_meta,\n            \"target_meta_detailed\": target_meta,\n        },\n    }\n    return ret\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.generate_target_manual","title":"generate_target_manual","text":"<pre><code>generate_target_manual(\n    reverse_converted, split_date, events_until_split\n)\n</code></pre> <p>Reconstructs a multi-task target string from previously reverse-converted structured data.</p> <p>This method takes a list of dictionaries (where each dictionary represents the parsed result of a single task from a model's output) and generates the corresponding multi-task target string. It calls the appropriate sub-converter's manual target generation method for each task type.</p> <p>Parameters:</p> Name Type Description Default <code>reverse_converted</code> <code>list[dict]</code> <p>A list of dictionaries, each resulting from the <code>reverse_conversion</code> method, containing keys 'task_type' and 'result' (the structured data).</p> required <code>split_date</code> <code>datetime</code> <p>The reference split date, needed for reconstructing forecasting targets.</p> required <code>events_until_split</code> <code>DataFrame</code> <p>Patient's historical event data up to the split date, needed for context by some sub-converters' target generation (e.g., forecasting).</p> required <p>Returns:</p> Type Description <code>tuple</code> <ul> <li>str: The reconstructed multi-task target string.</li> <li>list[dict]: A list containing the metadata generated during the reconstruction               of each individual task's target string.</li> <li>list[dict]: The input <code>reverse_converted</code> list, potentially updated with               'original_text' (the reconstructed target snippet for each task)               and 'target_meta'.</li> </ul> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def generate_target_manual(self, reverse_converted, split_date, events_until_split):\n    \"\"\"\n    Reconstructs a multi-task target string from previously reverse-converted structured data.\n\n    This method takes a list of dictionaries (where each dictionary represents the\n    parsed result of a single task from a model's output) and generates the corresponding\n    multi-task target string. It calls the appropriate sub-converter's manual target\n    generation method for each task type.\n\n    Parameters\n    ----------\n    reverse_converted : list[dict]\n        A list of dictionaries, each resulting from the `reverse_conversion` method,\n        containing keys 'task_type' and 'result' (the structured data).\n    split_date : datetime\n        The reference split date, needed for reconstructing forecasting targets.\n    events_until_split : pd.DataFrame\n        Patient's historical event data up to the split date, needed for context\n        by some sub-converters' target generation (e.g., forecasting).\n\n    Returns\n    -------\n    tuple\n        - str: The reconstructed multi-task target string.\n        - list[dict]: A list containing the metadata generated during the reconstruction\n                      of each individual task's target string.\n        - list[dict]: The input `reverse_converted` list, potentially updated with\n                      'original_text' (the reconstructed target snippet for each task)\n                      and 'target_meta'.\n    \"\"\"\n\n    #: go through all reverse converted and generate only the target string\n    individual_targets = []\n\n    for idx, reverse_converted_dic in enumerate(reverse_converted):\n        if reverse_converted_dic[\"task_type\"] == self.config.task_prompt_events:\n            occurred = reverse_converted_dic[\"result\"][\"occurred\"].iloc[0]\n            censored = reverse_converted_dic[\"result\"][\"censoring\"].iloc[0]\n            target_name = reverse_converted_dic[\"result\"][\"target_name\"].iloc[0]\n\n            ret_prompt, ret_meta = self.converter_events.generate_target_manual(\n                target_name=target_name,\n                event_occurred=occurred,\n                event_censored=censored,\n            )\n            individual_targets.append((None, ret_prompt, {}, self.config.task_prompt_events))\n            reverse_converted[idx][\"original_text\"] = ret_prompt\n\n        if reverse_converted_dic[\"task_type\"] == self.config.task_prompt_forecasting:\n            ret_prompt, ret_meta = self.converter_forecasting.generate_target_manual(\n                split_date_included_in_input=split_date,\n                events_until_split=events_until_split,\n                target_events_after_split=reverse_converted_dic[\"result\"],\n            )\n            individual_targets.append((None, ret_prompt, {}, self.config.task_prompt_forecasting))\n            reverse_converted[idx][\"original_text\"] = ret_prompt\n            reverse_converted[idx][\"target_meta\"] = ret_meta\n\n        if reverse_converted_dic[\"task_type\"] == self.config.task_prompt_custom:\n            individual_targets.append(\n                (\n                    None,\n                    reverse_converted_dic[\"result\"],\n                    {},\n                    self.config.task_prompt_custom,\n                )\n            )\n            reverse_converted[idx][\"original_text\"] = reverse_converted_dic[\"result\"]\n\n    #: then aggegrate all target strings with own\n    target_str, target_meta = self._generate_target_string(individual_targets)\n\n    # Return\n    return target_str, target_meta, reverse_converted\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.get_difference_in_event_dataframes","title":"get_difference_in_event_dataframes","text":"<pre><code>get_difference_in_event_dataframes(\n    list_of_original_conversions_meta,\n    list_of_reversed_conversions,\n)\n</code></pre> <p>Compares original structured data with reverse-converted data for each task in a multi-task scenario.</p> <p>Iterates through the tasks, matching the original metadata (which contains the ground truth structured data, e.g., in 'target_data_processed') with the result of the reverse conversion for that task. It then calls the appropriate sub-converter's difference calculation method.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_original_conversions_meta</code> <code>list[dict]</code> <p>The list of metadata dictionaries generated during the forward conversion, where each dictionary corresponds to a task and should contain the original structured target data (e.g., under 'target_data_processed'). Usually obtained from the 'target_meta_detailed' key in the forward conversion output meta.</p> required <code>list_of_reversed_conversions</code> <code>list[dict]</code> <p>The list of dictionaries resulting from the <code>reverse_conversion</code> method, where each dictionary contains the parsed structured data ('result') for a task.</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of pandas DataFrames. Each DataFrame highlights the differences found for the corresponding task between the original data and the reverse-converted data. An empty DataFrame indicates no differences were found for that task.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the number of original metadata items does not match the number of reversed conversion results.</p> <code>ValueError</code> <p>If a task type mismatch is detected or if expected data keys are missing.</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def get_difference_in_event_dataframes(\n    self,\n    list_of_original_conversions_meta: list[dict],\n    list_of_reversed_conversions: list[dict],\n) -&gt; list[pd.DataFrame]:\n    \"\"\"\n    Compares original structured data with reverse-converted data for each task in a multi-task scenario.\n\n    Iterates through the tasks, matching the original metadata (which contains the\n    ground truth structured data, e.g., in 'target_data_processed') with the result\n    of the reverse conversion for that task. It then calls the appropriate sub-converter's\n    difference calculation method.\n\n    Parameters\n    ----------\n    list_of_original_conversions_meta : list[dict]\n        The list of metadata dictionaries generated during the *forward* conversion,\n        where each dictionary corresponds to a task and should contain the original\n        structured target data (e.g., under 'target_data_processed'). Usually obtained\n        from the 'target_meta_detailed' key in the forward conversion output meta.\n    list_of_reversed_conversions : list[dict]\n        The list of dictionaries resulting from the `reverse_conversion` method, where\n        each dictionary contains the parsed structured data ('result') for a task.\n\n    Returns\n    -------\n    list[pd.DataFrame]\n        A list of pandas DataFrames. Each DataFrame highlights the differences found\n        for the corresponding task between the original data and the reverse-converted data.\n        An empty DataFrame indicates no differences were found for that task.\n\n    Raises\n    ------\n    AssertionError\n        If the number of original metadata items does not match the number of reversed\n        conversion results.\n    ValueError\n         If a task type mismatch is detected or if expected data keys are missing.\n    \"\"\"\n\n    # Check and setup data\n    # list_of_original_conversions_meta = original_conversion[\"meta\"][\"target_meta_detailed\"]\n\n    assert len(list_of_original_conversions_meta) == len(list_of_reversed_conversions)\n    ret_diffs = []\n\n    #: go through each task and compare using the correct converter\n    for idx in range(len(list_of_original_conversions_meta)):\n        original = list_of_original_conversions_meta[idx]\n        reversed = list_of_reversed_conversions[idx]\n\n        if reversed[\"task_type\"] == self.config.task_prompt_forecasting:\n            ret_diff = self.converter_forecasting.get_difference_in_event_dataframes(\n                reversed[\"result\"], original[\"target_data_processed\"]\n            )\n        elif original[\"task_type\"] == self.config.task_prompt_forecasting_qa:\n            ret_diff = self.converter_forecasting_qa.get_difference_in_event_dataframes(\n                reversed[\"result\"], original[\"target_data_processed\"]\n            )\n        elif original[\"task_type\"] == self.config.task_prompt_events:\n            ret_diff = self.converter_events.get_difference_in_event_dataframes(\n                reversed[\"result\"], original[\"target_data_processed\"]\n            )\n\n        ret_diffs.append(ret_diff)\n\n    #: return list of differences\n    return ret_diffs\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.get_nr_tokens","title":"get_nr_tokens","text":"<pre><code>get_nr_tokens(curr_string)\n</code></pre> <p>Calculates the number of tokens in a given string using the initialized tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>curr_string</code> <code>str</code> <p>The string to tokenize.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of tokens generated by the tokenizer for the input string.</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def get_nr_tokens(self, curr_string):\n    \"\"\"\n    Calculates the number of tokens in a given string using the initialized tokenizer.\n\n    Parameters\n    ----------\n    curr_string : str\n        The string to tokenize.\n\n    Returns\n    -------\n    int\n        The number of tokens generated by the tokenizer for the input string.\n    \"\"\"\n    return len(self.tokenizer(curr_string)[\"input_ids\"])\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.reverse_conversion","title":"reverse_conversion","text":"<pre><code>reverse_conversion(\n    target_string,\n    data_manager,\n    split_date,\n    patientid=None,\n    inference_override=False,\n)\n</code></pre> <p>Parses a multi-task model output string back into a list of structured results.</p> <p>Splits the input string based on task markers (e.g., \"Task 1:\", \"Task 2:\"). For each segment, it identifies the task type using configured prompt identifiers and calls the corresponding sub-converter's <code>reverse_conversion</code> method to parse the text into structured data (e.g., a DataFrame).</p> <p>Parameters:</p> Name Type Description Default <code>target_string</code> <code>str</code> <p>The complete multi-task output string generated by the language model.</p> required <code>data_manager</code> <code>DataManager</code> <p>The data manager instance, needed by some sub-converters for context (e.g., unique event mappings).</p> required <code>split_date</code> <code>datetime</code> <p>The reference split date, required by some sub-converters (e.g., forecasting) to interpret time-related outputs correctly.</p> required <code>patientid</code> <code>str</code> <p>If provided, this patient ID is added to the resulting structured data for each task. Defaults to None.</p> <code>None</code> <code>inference_override</code> <code>bool</code> <p>If True, allows processing even if a task type cannot be strictly identified (e.g., by falling back to heuristics). Primarily for inference robustness where model output might be slightly malformed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries, one for each task identified and parsed in the <code>target_string</code>. Each dictionary contains: - 'task_type': The identified type of the task. - 'original_text': The raw text segment corresponding to this task's answer. - 'result': The structured data parsed from 'original_text' by the appropriate             sub-converter (e.g., a pandas DataFrame).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>inference_override</code> is False and the type of a task segment cannot be determined using the configured prompt identifiers.</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def reverse_conversion(\n    self,\n    target_string: str,\n    data_manager: DataManager,\n    split_date: datetime,\n    patientid: str = None,\n    inference_override: bool = False,\n) -&gt; list[dict]:\n    \"\"\"\n    Parses a multi-task model output string back into a list of structured results.\n\n    Splits the input string based on task markers (e.g., \"Task 1:\", \"Task 2:\").\n    For each segment, it identifies the task type using configured prompt identifiers\n    and calls the corresponding sub-converter's `reverse_conversion` method to parse\n    the text into structured data (e.g., a DataFrame).\n\n    Parameters\n    ----------\n    target_string : str\n        The complete multi-task output string generated by the language model.\n    data_manager : DataManager\n        The data manager instance, needed by some sub-converters for context\n        (e.g., unique event mappings).\n    split_date : datetime\n        The reference split date, required by some sub-converters (e.g., forecasting)\n        to interpret time-related outputs correctly.\n    patientid : str, optional\n        If provided, this patient ID is added to the resulting structured data\n        for each task. Defaults to None.\n    inference_override : bool, optional\n        If True, allows processing even if a task type cannot be strictly identified\n        (e.g., by falling back to heuristics). Primarily for inference robustness\n        where model output might be slightly malformed. Defaults to False.\n\n    Returns\n    -------\n    list[dict]\n        A list of dictionaries, one for each task identified and parsed in the\n        `target_string`. Each dictionary contains:\n        - 'task_type': The identified type of the task.\n        - 'original_text': The raw text segment corresponding to this task's answer.\n        - 'result': The structured data parsed from 'original_text' by the appropriate\n                    sub-converter (e.g., a pandas DataFrame).\n\n    Raises\n    ------\n    ValueError\n        If `inference_override` is False and the type of a task segment cannot be\n        determined using the configured prompt identifiers.\n    \"\"\"\n\n    #: split up by task\n    all_tasks = target_string.split(\"Task\")\n    ret_list = []\n\n    for curr_task in all_tasks:\n        # Basic preprocessing\n        curr_task = curr_task.strip()\n        if len(curr_task) == 0:\n            continue\n\n        #: determine which task it is\n        task_type = None\n        standard_extraction = True\n\n        if self.config.task_prompt_forecasting in curr_task:\n            task_type = self.config.task_prompt_forecasting\n        elif self.config.task_prompt_forecasting_qa in curr_task:\n            task_type = self.config.task_prompt_forecasting_qa\n        elif self.config.task_prompt_events in curr_task:\n            task_type = self.config.task_prompt_events\n        elif \"custom\" in curr_task:\n            task_type = self.config.task_prompt_custom\n        else:\n            #: Try determining by whether the task contains a \"censored\"\n            if \"censored\" in curr_task:\n                task_type = self.config.task_prompt_events\n                standard_extraction = False\n            elif inference_override is False:\n                raise ValueError(\"Could not determine task type\")\n\n        #: extract the relevant parts\n        if standard_extraction:\n            curr_task_text = curr_task.split(task_type)[1]\n        else:\n            curr_task_text = \":\".join(curr_task.split(\":\")[1:])\n\n            # Often we have \"[\"\"]\" which need to be removed\n            curr_task_text = curr_task_text.replace(\"[\", \"\")\n            curr_task_text = curr_task_text.replace(\"]\", \"\")\n\n        #: pass to correct reverse converter\n        if task_type == self.config.task_prompt_forecasting:\n            ret = self.converter_forecasting.reverse_conversion(\n                curr_task_text, data_manager.unique_events, split_date\n            )\n        elif task_type == self.config.task_prompt_forecasting_qa:\n            ret = self.converter_forecasting_qa.reverse_conversion(\n                curr_task_text, data_manager.unique_events, split_date\n            )\n        elif task_type == self.config.task_prompt_events:\n            ret = self.converter_events.reverse_conversion(curr_task_text)\n\n        elif task_type == self.config.task_prompt_custom:\n            ret = {\"original_text\": curr_task_text}\n\n        if patientid is not None:\n            ret[self.config.patient_id_col] = patientid\n\n        ret_dic = {\n            \"task_type\": task_type,\n            \"original_text\": curr_task_text,\n            \"result\": ret,\n        }\n\n        #: make list of results\n        ret_list.append(ret_dic)\n\n    #: return list of results\n    return ret_list\n</code></pre>"},{"location":"reference/instruction/converter_instruction/#twinweaver.instruction.converter_instruction.ConverterInstruction.set_custom_summarized_row_fn","title":"set_custom_summarized_row_fn","text":"<pre><code>set_custom_summarized_row_fn(fn)\n</code></pre> <p>Sets a custom function for generating the summarized row string. If this function is not called, the default _generate_summarized_row_string is used. Requirements: signature must start with (self, events_df, meta, ...) If the function meets these criteria but it is still invalid, it will raise an error later on</p> Source code in <code>twinweaver/instruction/converter_instruction.py</code> <pre><code>def set_custom_summarized_row_fn(self, fn: Callable[[pd.DataFrame, dict], str]):\n    \"\"\"\n    Sets a custom function for generating the summarized row string.\n    If this function is not called, the default _generate_summarized_row_string is used.\n    Requirements: signature must start with (self, events_df, meta, ...)\n    If the function meets these criteria but it is still invalid, it will raise an error later on\n    \"\"\"\n    import inspect\n\n    params = list(inspect.signature(fn).parameters)\n    if len(params) &lt; 3 or params[0] != \"self\":  # assumes signature (self, events_df, meta, ...)\n        raise TypeError(\n            f\"Custom summarized row function must accept at least (self, events_df, meta).Got parameters: {params}\"\n        )\n    # Bind it as method\n    self._generate_summarized_row_str_fn = fn.__get__(self, type(self))\n</code></pre>"},{"location":"reference/instruction/data_splitter/","title":"Data Splitter (Main)","text":""},{"location":"reference/instruction/data_splitter/#twinweaver.instruction.data_splitter","title":"twinweaver.instruction.data_splitter","text":""},{"location":"reference/instruction/data_splitter/#twinweaver.instruction.data_splitter-classes","title":"Classes","text":""},{"location":"reference/instruction/data_splitter/#twinweaver.instruction.data_splitter.DataSplitter","title":"DataSplitter","text":"<p>Combines both data splitters into one interface for easier usage. For more advanced use cases, the individual data splitters can still be used directly.</p> Source code in <code>twinweaver/instruction/data_splitter.py</code> <pre><code>class DataSplitter:\n    \"\"\"\n    Combines both data splitters into one interface for easier usage.\n    For more advanced use cases, the individual data splitters can still be used directly.\n    \"\"\"\n\n    def __init__(self, data_splitter_events: DataSplitterEvents, data_splitter_forecasting: DataSplitterForecasting):\n        self.data_splitter_events = data_splitter_events\n        self.data_splitter_forecasting = data_splitter_forecasting\n\n    def get_splits_from_patient_with_target(\n        self,\n        patient_data: dict,\n        max_num_splits_per_split_event: int = 1,\n        forecasting_nr_samples_per_split: int = 1,\n        events_max_nr_samples_per_split: int = 1,\n        forecasting_filter_outliers: bool = False,\n        forecasting_override_categories_to_predict: list[str] = None,\n        forecasting_override_variables_to_predict: list[str] = None,\n        forecasting_override_split_dates: list[datetime] = None,\n        events_override_category: str = None,\n        events_override_observation_time_delta: pd.Timedelta = None,\n    ) -&gt; tuple[list[DataSplitterForecastingGroup], list[DataSplitterEventsGroup]]:\n        \"\"\"\n        Generates both forecasting and event prediction splits for a patient, ensuring proper alignment.\n\n        This value uses the forecasting splitter to determine candidate split dates (based on LoT\n        or overrides), which are then passed to the event prediction splitter to ensure both tasks\n        use the same anchor points in time. This is critical for multitasking or consistent\n        evaluation.\n\n        Parameters\n        ----------\n        patient_data : dict\n            Dictionary containing the patient's data ('events' and 'constant').\n        max_num_splits_per_split_event : int\n            Maximum number of random split dates to select per Line of Therapy. Defaults to 1.\n        forecasting_nr_samples_per_split : int\n            Number of forecasting task variants (variable subsets) to generate per split date. Defaults to 1.\n        events_max_nr_samples_per_split : int\n            Maximum number of event prediction tasks to generate per split date. Defaults to 1.\n        forecasting_filter_outliers : bool\n            Whether to apply outlier filtering (e.g., 3-sigma) to target values in forecasting tasks.\n            Defaults to False.\n        forecasting_override_categories_to_predict : list[str], optional\n            Force forecasting of all variables in these categories.\n        forecasting_override_variables_to_predict : list[str], optional\n            Force forecasting of these specific variables.\n        forecasting_override_split_dates : list[datetime], optional\n            Force usage of these specific split dates.\n        events_override_category : str, optional\n            Force event prediction for this specific event category.\n        events_override_observation_time_delta : pd.Timedelta, optional\n            Force a specific prediction window duration for event tasks.\n\n        Returns\n        -------\n        tuple\n            A tuple containing three elements:\n            1. forecasting_splits: list[DataSplitterForecastingGroup]\n               List of generated forecasting split groups.\n            2. events_splits: list[DataSplitterEventsGroup]\n               List of generated event prediction split groups, corresponding to the forecasting splits.\n            3. reference_dates: pd.DataFrame\n               DataFrame containing the split dates and LoT dates used.\n        \"\"\"\n        # Process forecasting splits\n        forecasting_splits, reference_dates = self.data_splitter_forecasting.get_splits_from_patient(\n            patient_data,\n            nr_samples_per_split=forecasting_nr_samples_per_split,\n            include_metadata=True,\n            max_num_splits_per_split_event=max_num_splits_per_split_event,\n            filter_outliers=forecasting_filter_outliers,\n            override_categories_to_predict=forecasting_override_categories_to_predict,\n            override_variables_to_predict=forecasting_override_variables_to_predict,\n            override_split_dates=forecasting_override_split_dates,\n        )\n\n        # Process event prediction splits\n        events_splits = self.data_splitter_events.get_splits_from_patient(\n            patient_data,\n            reference_split_dates=reference_dates,\n            max_nr_samples_per_split=events_max_nr_samples_per_split,\n            override_category=events_override_category,\n            override_observation_time_delta=events_override_observation_time_delta,\n        )\n\n        #: return both, since we want to be able to still have the flexibility to use both splitters directly\n        return forecasting_splits, events_splits, reference_dates\n\n    def get_splits_from_patient_inference(\n        self,\n        patient_data: dict,\n        inference_type: str = \"both\",\n        forecasting_override_variables_to_predict: list[str] = None,\n        events_override_category: str = None,\n        events_override_observation_time_delta: pd.Timedelta = None,\n    ):\n        \"\"\"\n        Generates a single split for inference based on the latest available data.\n\n        This method assumes the inference should occur at the last recorded date in the\n        patient's event history. It generates a single split (forecasting, events, or both)\n        anchored at this date. This is typically used for generating predictions on new data.\n        Target values will not be available or filtered.\n\n        Parameters\n        ----------\n        patient_data : dict\n            Dictionary containing the patient's data. 'events' dataframe must be present.\n        inference_type : str\n            The type of inference task to generate: 'forecasting', 'events', or 'both'.\n            Defaults to \"both\".\n        forecasting_override_variables_to_predict : list[str], optional\n            List of variables to generate forecasts for. If None, variables might be sampled\n            (though sampling behavior depends on implementation when no target is present).\n        events_override_category : str, optional\n            The event category to predict (e.g., 'death', 'progression').\n        events_override_observation_time_delta : pd.Timedelta, optional\n            The time window for the event prediction.\n\n        Returns\n        -------\n        tuple\n            A tuple containing:\n            1. forecast_split: DataSplitterForecastingOption or None\n               The generated forecasting option, or None if inference_type is 'events'.\n            2. events_split: DataSplitterEventsOption or None\n               The generated event prediction option, or None if inference_type is 'forecasting'.\n        \"\"\"\n        # assume last date in events is the split date that we're interested in\n        patient_data[\"events\"] = patient_data[\"events\"].sort_values(by=self.data_splitter_events.config.date_col)\n        split_date = patient_data[\"events\"][self.data_splitter_events.config.date_col].iloc[-1]\n\n        #: generate forecasting split\n        if inference_type == \"both\" or inference_type == \"forecasting\":\n            forecast_splits = self.data_splitter_forecasting.get_splits_from_patient(\n                patient_data,\n                nr_samples_per_split=1,\n                filter_outliers=False,  # Since no filtering needed, since no target exists\n                override_split_dates=[split_date],\n                override_variables_to_predict=forecasting_override_variables_to_predict,\n            )\n            # The first one is the only one\n            forecast_split = forecast_splits[0][0]\n        else:\n            forecast_split = None\n\n        #: generate event split\n        if inference_type == \"both\" or inference_type == \"events\":\n            events_splits = self.data_splitter_events.get_splits_from_patient(\n                patient_data,\n                max_nr_samples_per_split=1,\n                override_split_dates=[split_date],\n                override_category=events_override_category,\n                override_observation_time_delta=events_override_observation_time_delta,\n            )\n            # The first one is the only one\n            events_split = events_splits[0][0]\n        else:\n            events_split = None\n\n        return forecast_split, events_split\n</code></pre>"},{"location":"reference/instruction/data_splitter/#twinweaver.instruction.data_splitter.DataSplitter-functions","title":"Functions","text":""},{"location":"reference/instruction/data_splitter/#twinweaver.instruction.data_splitter.DataSplitter.get_splits_from_patient_inference","title":"get_splits_from_patient_inference","text":"<pre><code>get_splits_from_patient_inference(\n    patient_data,\n    inference_type=\"both\",\n    forecasting_override_variables_to_predict=None,\n    events_override_category=None,\n    events_override_observation_time_delta=None,\n)\n</code></pre> <p>Generates a single split for inference based on the latest available data.</p> <p>This method assumes the inference should occur at the last recorded date in the patient's event history. It generates a single split (forecasting, events, or both) anchored at this date. This is typically used for generating predictions on new data. Target values will not be available or filtered.</p> <p>Parameters:</p> Name Type Description Default <code>patient_data</code> <code>dict</code> <p>Dictionary containing the patient's data. 'events' dataframe must be present.</p> required <code>inference_type</code> <code>str</code> <p>The type of inference task to generate: 'forecasting', 'events', or 'both'. Defaults to \"both\".</p> <code>'both'</code> <code>forecasting_override_variables_to_predict</code> <code>list[str]</code> <p>List of variables to generate forecasts for. If None, variables might be sampled (though sampling behavior depends on implementation when no target is present).</p> <code>None</code> <code>events_override_category</code> <code>str</code> <p>The event category to predict (e.g., 'death', 'progression').</p> <code>None</code> <code>events_override_observation_time_delta</code> <code>Timedelta</code> <p>The time window for the event prediction.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: 1. forecast_split: DataSplitterForecastingOption or None    The generated forecasting option, or None if inference_type is 'events'. 2. events_split: DataSplitterEventsOption or None    The generated event prediction option, or None if inference_type is 'forecasting'.</p> Source code in <code>twinweaver/instruction/data_splitter.py</code> <pre><code>def get_splits_from_patient_inference(\n    self,\n    patient_data: dict,\n    inference_type: str = \"both\",\n    forecasting_override_variables_to_predict: list[str] = None,\n    events_override_category: str = None,\n    events_override_observation_time_delta: pd.Timedelta = None,\n):\n    \"\"\"\n    Generates a single split for inference based on the latest available data.\n\n    This method assumes the inference should occur at the last recorded date in the\n    patient's event history. It generates a single split (forecasting, events, or both)\n    anchored at this date. This is typically used for generating predictions on new data.\n    Target values will not be available or filtered.\n\n    Parameters\n    ----------\n    patient_data : dict\n        Dictionary containing the patient's data. 'events' dataframe must be present.\n    inference_type : str\n        The type of inference task to generate: 'forecasting', 'events', or 'both'.\n        Defaults to \"both\".\n    forecasting_override_variables_to_predict : list[str], optional\n        List of variables to generate forecasts for. If None, variables might be sampled\n        (though sampling behavior depends on implementation when no target is present).\n    events_override_category : str, optional\n        The event category to predict (e.g., 'death', 'progression').\n    events_override_observation_time_delta : pd.Timedelta, optional\n        The time window for the event prediction.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        1. forecast_split: DataSplitterForecastingOption or None\n           The generated forecasting option, or None if inference_type is 'events'.\n        2. events_split: DataSplitterEventsOption or None\n           The generated event prediction option, or None if inference_type is 'forecasting'.\n    \"\"\"\n    # assume last date in events is the split date that we're interested in\n    patient_data[\"events\"] = patient_data[\"events\"].sort_values(by=self.data_splitter_events.config.date_col)\n    split_date = patient_data[\"events\"][self.data_splitter_events.config.date_col].iloc[-1]\n\n    #: generate forecasting split\n    if inference_type == \"both\" or inference_type == \"forecasting\":\n        forecast_splits = self.data_splitter_forecasting.get_splits_from_patient(\n            patient_data,\n            nr_samples_per_split=1,\n            filter_outliers=False,  # Since no filtering needed, since no target exists\n            override_split_dates=[split_date],\n            override_variables_to_predict=forecasting_override_variables_to_predict,\n        )\n        # The first one is the only one\n        forecast_split = forecast_splits[0][0]\n    else:\n        forecast_split = None\n\n    #: generate event split\n    if inference_type == \"both\" or inference_type == \"events\":\n        events_splits = self.data_splitter_events.get_splits_from_patient(\n            patient_data,\n            max_nr_samples_per_split=1,\n            override_split_dates=[split_date],\n            override_category=events_override_category,\n            override_observation_time_delta=events_override_observation_time_delta,\n        )\n        # The first one is the only one\n        events_split = events_splits[0][0]\n    else:\n        events_split = None\n\n    return forecast_split, events_split\n</code></pre>"},{"location":"reference/instruction/data_splitter/#twinweaver.instruction.data_splitter.DataSplitter.get_splits_from_patient_with_target","title":"get_splits_from_patient_with_target","text":"<pre><code>get_splits_from_patient_with_target(\n    patient_data,\n    max_num_splits_per_split_event=1,\n    forecasting_nr_samples_per_split=1,\n    events_max_nr_samples_per_split=1,\n    forecasting_filter_outliers=False,\n    forecasting_override_categories_to_predict=None,\n    forecasting_override_variables_to_predict=None,\n    forecasting_override_split_dates=None,\n    events_override_category=None,\n    events_override_observation_time_delta=None,\n)\n</code></pre> <p>Generates both forecasting and event prediction splits for a patient, ensuring proper alignment.</p> <p>This value uses the forecasting splitter to determine candidate split dates (based on LoT or overrides), which are then passed to the event prediction splitter to ensure both tasks use the same anchor points in time. This is critical for multitasking or consistent evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>patient_data</code> <code>dict</code> <p>Dictionary containing the patient's data ('events' and 'constant').</p> required <code>max_num_splits_per_split_event</code> <code>int</code> <p>Maximum number of random split dates to select per Line of Therapy. Defaults to 1.</p> <code>1</code> <code>forecasting_nr_samples_per_split</code> <code>int</code> <p>Number of forecasting task variants (variable subsets) to generate per split date. Defaults to 1.</p> <code>1</code> <code>events_max_nr_samples_per_split</code> <code>int</code> <p>Maximum number of event prediction tasks to generate per split date. Defaults to 1.</p> <code>1</code> <code>forecasting_filter_outliers</code> <code>bool</code> <p>Whether to apply outlier filtering (e.g., 3-sigma) to target values in forecasting tasks. Defaults to False.</p> <code>False</code> <code>forecasting_override_categories_to_predict</code> <code>list[str]</code> <p>Force forecasting of all variables in these categories.</p> <code>None</code> <code>forecasting_override_variables_to_predict</code> <code>list[str]</code> <p>Force forecasting of these specific variables.</p> <code>None</code> <code>forecasting_override_split_dates</code> <code>list[datetime]</code> <p>Force usage of these specific split dates.</p> <code>None</code> <code>events_override_category</code> <code>str</code> <p>Force event prediction for this specific event category.</p> <code>None</code> <code>events_override_observation_time_delta</code> <code>Timedelta</code> <p>Force a specific prediction window duration for event tasks.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing three elements: 1. forecasting_splits: list[DataSplitterForecastingGroup]    List of generated forecasting split groups. 2. events_splits: list[DataSplitterEventsGroup]    List of generated event prediction split groups, corresponding to the forecasting splits. 3. reference_dates: pd.DataFrame    DataFrame containing the split dates and LoT dates used.</p> Source code in <code>twinweaver/instruction/data_splitter.py</code> <pre><code>def get_splits_from_patient_with_target(\n    self,\n    patient_data: dict,\n    max_num_splits_per_split_event: int = 1,\n    forecasting_nr_samples_per_split: int = 1,\n    events_max_nr_samples_per_split: int = 1,\n    forecasting_filter_outliers: bool = False,\n    forecasting_override_categories_to_predict: list[str] = None,\n    forecasting_override_variables_to_predict: list[str] = None,\n    forecasting_override_split_dates: list[datetime] = None,\n    events_override_category: str = None,\n    events_override_observation_time_delta: pd.Timedelta = None,\n) -&gt; tuple[list[DataSplitterForecastingGroup], list[DataSplitterEventsGroup]]:\n    \"\"\"\n    Generates both forecasting and event prediction splits for a patient, ensuring proper alignment.\n\n    This value uses the forecasting splitter to determine candidate split dates (based on LoT\n    or overrides), which are then passed to the event prediction splitter to ensure both tasks\n    use the same anchor points in time. This is critical for multitasking or consistent\n    evaluation.\n\n    Parameters\n    ----------\n    patient_data : dict\n        Dictionary containing the patient's data ('events' and 'constant').\n    max_num_splits_per_split_event : int\n        Maximum number of random split dates to select per Line of Therapy. Defaults to 1.\n    forecasting_nr_samples_per_split : int\n        Number of forecasting task variants (variable subsets) to generate per split date. Defaults to 1.\n    events_max_nr_samples_per_split : int\n        Maximum number of event prediction tasks to generate per split date. Defaults to 1.\n    forecasting_filter_outliers : bool\n        Whether to apply outlier filtering (e.g., 3-sigma) to target values in forecasting tasks.\n        Defaults to False.\n    forecasting_override_categories_to_predict : list[str], optional\n        Force forecasting of all variables in these categories.\n    forecasting_override_variables_to_predict : list[str], optional\n        Force forecasting of these specific variables.\n    forecasting_override_split_dates : list[datetime], optional\n        Force usage of these specific split dates.\n    events_override_category : str, optional\n        Force event prediction for this specific event category.\n    events_override_observation_time_delta : pd.Timedelta, optional\n        Force a specific prediction window duration for event tasks.\n\n    Returns\n    -------\n    tuple\n        A tuple containing three elements:\n        1. forecasting_splits: list[DataSplitterForecastingGroup]\n           List of generated forecasting split groups.\n        2. events_splits: list[DataSplitterEventsGroup]\n           List of generated event prediction split groups, corresponding to the forecasting splits.\n        3. reference_dates: pd.DataFrame\n           DataFrame containing the split dates and LoT dates used.\n    \"\"\"\n    # Process forecasting splits\n    forecasting_splits, reference_dates = self.data_splitter_forecasting.get_splits_from_patient(\n        patient_data,\n        nr_samples_per_split=forecasting_nr_samples_per_split,\n        include_metadata=True,\n        max_num_splits_per_split_event=max_num_splits_per_split_event,\n        filter_outliers=forecasting_filter_outliers,\n        override_categories_to_predict=forecasting_override_categories_to_predict,\n        override_variables_to_predict=forecasting_override_variables_to_predict,\n        override_split_dates=forecasting_override_split_dates,\n    )\n\n    # Process event prediction splits\n    events_splits = self.data_splitter_events.get_splits_from_patient(\n        patient_data,\n        reference_split_dates=reference_dates,\n        max_nr_samples_per_split=events_max_nr_samples_per_split,\n        override_category=events_override_category,\n        override_observation_time_delta=events_override_observation_time_delta,\n    )\n\n    #: return both, since we want to be able to still have the flexibility to use both splitters directly\n    return forecasting_splits, events_splits, reference_dates\n</code></pre>"},{"location":"reference/instruction/data_splitters/","title":"Data Splitters (Base &amp; Implementations)","text":""},{"location":"reference/instruction/data_splitters/#base-splitter","title":"Base Splitter","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_base","title":"twinweaver.instruction.data_splitter_base","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_base-classes","title":"Classes","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_base.BaseDataSplitter","title":"BaseDataSplitter","text":"<p>Base splitter class, used for both time to event splitting and forecasting splitting. It implements some basic functionality that is shared between the two types of splitters.</p> Source code in <code>twinweaver/instruction/data_splitter_base.py</code> <pre><code>class BaseDataSplitter:\n    \"\"\"\n    Base splitter class, used for both time to event splitting and forecasting splitting.\n    It implements some basic functionality that is shared between the two types of splitters.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_manager: DataManager,\n        config: Config,\n        max_split_length_after_split_event: pd.Timedelta = pd.Timedelta(days=90),\n        max_lookback_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n        max_forecast_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n    ):\n        \"\"\"\n        Constructor for the BaseDataSplitter class.\n\n        Parameters\n        ----------\n        data_manager: DataManager\n            the data manager object that holds the data.\n        config: Config\n            Configuration object holding constants.\n        max_split_length_after_split_event: pd.Timedelta\n            the maximum number of days after a LoT event that we want to consider as\n            a starting point.\n        max_lookback_time_for_value: pd.Timedelta\n            the maximum number of days before a certain split date where we need to see\n            the value of the target variable.\n        max_forecast_time_for_value : pd.Timedelta\n            the maximum number of days after a certain split date where we need to see\n            the value of the target variable when filtering.\n        \"\"\"\n\n        assert config.split_event_category is not None, \"config.split_event_category must be set (e.g. ['lab']).\"\n\n        self.dm = data_manager\n        self.config = config\n        self.max_split_length_after_split_event = max_split_length_after_split_event\n        self.max_lookback_time_for_value = max_lookback_time_for_value\n        self.max_forecast_time_for_value = max_forecast_time_for_value\n\n    def _get_all_dates_within_range_of_split_event(\n        self,\n        patient_data_dic: dict,\n        time_before_lot_start: pd.Timedelta,\n        max_split_length_after_split_event: pd.Timedelta,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Get all possible valid split dates for a given patient data dictionary, without\n        any filtering regarding variables, used in some helper functions\n\n        Parameters\n        ----------\n        patient_data_dic: dict\n            the patient data dictionary that holds the data for a given patient.\n\n        Returns\n        -------\n        pd.DataFrame\n            a pandas dataframe that holds all possible split dates for the given patient data dictionary.\n            It has columns self.config.date_col and self.config.split_date_col.\n            Each row is a date which is valid for a split.\n        \"\"\"\n\n        #: setup data\n        events = patient_data_dic[\"events\"].copy()\n\n        if self.config.event_category_death in events[self.config.event_category_col].unique():\n            # Exclude death events for splitting, to avoid edge cases\n            events = events[events[self.config.event_category_col] != self.config.event_category_death]\n        else:\n            # Exclude last date for splitting\n            events = events[events[self.config.date_col] != events[self.config.date_col].max()]\n\n        #: get all starting split dates, sorted, excluding death\n        all_split_dates = events[events[self.config.event_category_col] == self.config.split_event_category][\n            self.config.date_col\n        ]\n        all_split_dates = list(set(all_split_dates.tolist()))\n        all_split_dates.sort()\n\n        #: go over all events\n        all_dates = events[self.config.date_col].copy()\n        all_dates = list(set(all_dates.tolist()))\n        all_dates.sort()\n\n        #: restrict search space to only events that are within max_split_length_after_split_event days after LoT\n        all_possible_dates = []\n        for curr_split_date in all_split_dates:\n            for curr_date in all_dates:\n                if (\n                    curr_date &lt;= curr_split_date + max_split_length_after_split_event\n                    and curr_date &gt;= curr_split_date - time_before_lot_start\n                ):\n                    all_possible_dates.append((curr_date, curr_split_date))\n\n        # Serve as df\n        df = pd.DataFrame(all_possible_dates, columns=[self.config.date_col, self.config.split_date_col])\n        if df.shape[0] == 0:\n            return df\n\n        #: keep only unique dates, using the one with closest split_date\n        df[\"diff\"] = (df[self.config.date_col] - df[self.config.split_date_col]).dt.days\n        df[\"diff_abs\"] = df[\"diff\"].abs()\n        df = df.loc[df.groupby(self.config.date_col)[\"diff_abs\"].idxmin()]\n        df = df.drop(columns=[\"diff\", \"diff_abs\"])\n\n        return df\n\n    def select_random_splits(\n        self, all_possible_split_dates: pd.DataFrame, max_num_splits_per_split_event: int = 1\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Select random splits within a given lot, based on the input split dates.\n        Thus each LoT has max_num_splits_per_split_event random split.\n\n\n        Parameters\n        ----------\n        all_possible_split_dates: pd.DataFrame\n            a pandas dataframe that holds all possible split dates for the given patient data dictionary.\n            It has columns self.config.date_col, self.config.split_date_col.\n            Each row is a date which is valid for a split.\n\n        max_num_splits_per_split_event: int\n            the maximum number of samples per lot that we want to sample.\n\n        Returns\n        -------\n        pd.DataFrame\n            a pandas dataframe that holds a randomly selected split date for each unique lot date.\n            It has columns self.config.date_col, self.config.split_date_col.\n            Each row is a date which is valid for a split.\n        \"\"\"\n\n        #: select one randomly per unique LOT_self.config.date_col\n        randomly_selected_per_lot = (\n            all_possible_split_dates.groupby(self.config.split_date_col)\n            .sample(n=max_num_splits_per_split_event, replace=True, random_state=1)\n            .reset_index(drop=True)\n        )\n\n        # Sort\n        randomly_selected_per_lot = randomly_selected_per_lot.sort_values(\n            by=[self.config.split_date_col, self.config.date_col]\n        )\n\n        #: return\n        return randomly_selected_per_lot\n\n    def drop_duplicates_except_na_for_date_col(self, df):\n        \"\"\"\n        Drops duplicates from the DataFrame except for rows with NA in the date column.\n        Note: Original function description mentioned split_date_col, but implementation uses date_col.\n              Following the implementation.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            The input DataFrame.\n\n        Returns\n        -------\n        pd.DataFrame\n            The DataFrame with duplicates dropped, except for rows with NA in the date column.\n        \"\"\"\n\n        # Edge case handling\n        if df.shape[0] == 0:\n            return df\n\n        # Split the DataFrame into rows with NA in the specified column and rows without NA\n        na_rows = df[df[self.config.date_col].isna()]\n        non_na_rows = df[~df[self.config.date_col].isna()]\n\n        # Drop duplicates from the rows without NA\n        non_na_rows_deduped = non_na_rows.drop_duplicates()\n\n        # Concatenate the NA rows and the deduplicated non-NA rows\n        result_df = pd.concat([na_rows, non_na_rows_deduped])\n\n        # Sort by index\n        result_df = result_df.sort_index()\n\n        return result_df\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_base.BaseDataSplitter-functions","title":"Functions","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_base.BaseDataSplitter.__init__","title":"__init__","text":"<pre><code>__init__(\n    data_manager,\n    config,\n    max_split_length_after_split_event=pd.Timedelta(\n        days=90\n    ),\n    max_lookback_time_for_value=pd.Timedelta(days=90),\n    max_forecast_time_for_value=pd.Timedelta(days=90),\n)\n</code></pre> <p>Constructor for the BaseDataSplitter class.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManager</code> <p>the data manager object that holds the data.</p> required <code>config</code> <code>Config</code> <p>Configuration object holding constants.</p> required <code>max_split_length_after_split_event</code> <code>Timedelta</code> <p>the maximum number of days after a LoT event that we want to consider as a starting point.</p> <code>Timedelta(days=90)</code> <code>max_lookback_time_for_value</code> <code>Timedelta</code> <p>the maximum number of days before a certain split date where we need to see the value of the target variable.</p> <code>Timedelta(days=90)</code> <code>max_forecast_time_for_value</code> <code>Timedelta</code> <p>the maximum number of days after a certain split date where we need to see the value of the target variable when filtering.</p> <code>Timedelta(days=90)</code> Source code in <code>twinweaver/instruction/data_splitter_base.py</code> <pre><code>def __init__(\n    self,\n    data_manager: DataManager,\n    config: Config,\n    max_split_length_after_split_event: pd.Timedelta = pd.Timedelta(days=90),\n    max_lookback_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n    max_forecast_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n):\n    \"\"\"\n    Constructor for the BaseDataSplitter class.\n\n    Parameters\n    ----------\n    data_manager: DataManager\n        the data manager object that holds the data.\n    config: Config\n        Configuration object holding constants.\n    max_split_length_after_split_event: pd.Timedelta\n        the maximum number of days after a LoT event that we want to consider as\n        a starting point.\n    max_lookback_time_for_value: pd.Timedelta\n        the maximum number of days before a certain split date where we need to see\n        the value of the target variable.\n    max_forecast_time_for_value : pd.Timedelta\n        the maximum number of days after a certain split date where we need to see\n        the value of the target variable when filtering.\n    \"\"\"\n\n    assert config.split_event_category is not None, \"config.split_event_category must be set (e.g. ['lab']).\"\n\n    self.dm = data_manager\n    self.config = config\n    self.max_split_length_after_split_event = max_split_length_after_split_event\n    self.max_lookback_time_for_value = max_lookback_time_for_value\n    self.max_forecast_time_for_value = max_forecast_time_for_value\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_base.BaseDataSplitter.drop_duplicates_except_na_for_date_col","title":"drop_duplicates_except_na_for_date_col","text":"<pre><code>drop_duplicates_except_na_for_date_col(df)\n</code></pre> <p>Drops duplicates from the DataFrame except for rows with NA in the date column. Note: Original function description mentioned split_date_col, but implementation uses date_col.       Following the implementation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame with duplicates dropped, except for rows with NA in the date column.</p> Source code in <code>twinweaver/instruction/data_splitter_base.py</code> <pre><code>def drop_duplicates_except_na_for_date_col(self, df):\n    \"\"\"\n    Drops duplicates from the DataFrame except for rows with NA in the date column.\n    Note: Original function description mentioned split_date_col, but implementation uses date_col.\n          Following the implementation.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input DataFrame.\n\n    Returns\n    -------\n    pd.DataFrame\n        The DataFrame with duplicates dropped, except for rows with NA in the date column.\n    \"\"\"\n\n    # Edge case handling\n    if df.shape[0] == 0:\n        return df\n\n    # Split the DataFrame into rows with NA in the specified column and rows without NA\n    na_rows = df[df[self.config.date_col].isna()]\n    non_na_rows = df[~df[self.config.date_col].isna()]\n\n    # Drop duplicates from the rows without NA\n    non_na_rows_deduped = non_na_rows.drop_duplicates()\n\n    # Concatenate the NA rows and the deduplicated non-NA rows\n    result_df = pd.concat([na_rows, non_na_rows_deduped])\n\n    # Sort by index\n    result_df = result_df.sort_index()\n\n    return result_df\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_base.BaseDataSplitter.select_random_splits","title":"select_random_splits","text":"<pre><code>select_random_splits(\n    all_possible_split_dates,\n    max_num_splits_per_split_event=1,\n)\n</code></pre> <p>Select random splits within a given lot, based on the input split dates. Thus each LoT has max_num_splits_per_split_event random split.</p> <p>Parameters:</p> Name Type Description Default <code>all_possible_split_dates</code> <code>DataFrame</code> <p>a pandas dataframe that holds all possible split dates for the given patient data dictionary. It has columns self.config.date_col, self.config.split_date_col. Each row is a date which is valid for a split.</p> required <code>max_num_splits_per_split_event</code> <code>int</code> <p>the maximum number of samples per lot that we want to sample.</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>a pandas dataframe that holds a randomly selected split date for each unique lot date. It has columns self.config.date_col, self.config.split_date_col. Each row is a date which is valid for a split.</p> Source code in <code>twinweaver/instruction/data_splitter_base.py</code> <pre><code>def select_random_splits(\n    self, all_possible_split_dates: pd.DataFrame, max_num_splits_per_split_event: int = 1\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Select random splits within a given lot, based on the input split dates.\n    Thus each LoT has max_num_splits_per_split_event random split.\n\n\n    Parameters\n    ----------\n    all_possible_split_dates: pd.DataFrame\n        a pandas dataframe that holds all possible split dates for the given patient data dictionary.\n        It has columns self.config.date_col, self.config.split_date_col.\n        Each row is a date which is valid for a split.\n\n    max_num_splits_per_split_event: int\n        the maximum number of samples per lot that we want to sample.\n\n    Returns\n    -------\n    pd.DataFrame\n        a pandas dataframe that holds a randomly selected split date for each unique lot date.\n        It has columns self.config.date_col, self.config.split_date_col.\n        Each row is a date which is valid for a split.\n    \"\"\"\n\n    #: select one randomly per unique LOT_self.config.date_col\n    randomly_selected_per_lot = (\n        all_possible_split_dates.groupby(self.config.split_date_col)\n        .sample(n=max_num_splits_per_split_event, replace=True, random_state=1)\n        .reset_index(drop=True)\n    )\n\n    # Sort\n    randomly_selected_per_lot = randomly_selected_per_lot.sort_values(\n        by=[self.config.split_date_col, self.config.date_col]\n    )\n\n    #: return\n    return randomly_selected_per_lot\n</code></pre>"},{"location":"reference/instruction/data_splitters/#events-splitter","title":"Events Splitter","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events","title":"twinweaver.instruction.data_splitter_events","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events-classes","title":"Classes","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events.DataSplitterEvents","title":"DataSplitterEvents","text":"<p>               Bases: <code>BaseDataSplitter</code></p> Source code in <code>twinweaver/instruction/data_splitter_events.py</code> <pre><code>class DataSplitterEvents(BaseDataSplitter):\n    def __init__(\n        self,\n        data_manager: DataManager,\n        config: Config,\n        max_length_to_sample: pd.Timedelta = pd.Timedelta(weeks=104),\n        min_length_to_sample: pd.Timedelta = pd.Timedelta(weeks=1),\n        unit_length_to_sample: str = \"weeks\",\n        max_split_length_after_split_event: pd.Timedelta = pd.Timedelta(days=90),\n        max_lookback_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n        max_forecast_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n    ):\n        \"\"\"\n        Initialize the DataSplitterEvents class.\n\n        Parameters\n        ----------\n        data_manager : DataManager\n            The data manager to handle data operations.\n        config : Config\n            Configuration object holding constants.\n        max_length_to_sample : pd.Timedelta\n            The maximum number of weeks into the future to sample for event prediction.\n        min_length_to_sample : pd.Timedelta\n            The minimum number of weeks into the future to sample for event prediction.\n        unit_length_to_sample : str\n            The unit of time for the length to sample (e.g. \"weeks\").\n        max_split_length_after_split_event : pd.Timedelta, optional\n            The maximum number of days after the split event (e.g. line of therapy) to consider for split points.\n        max_lookback_time_for_value : pd.Timedelta, optional\n            The maximum number of days to look back for a value (inherited but not directly used here).\n        max_forecast_time_for_value : pd.Timedelta, optional\n            The maximum number of days to forecast a value (inherited but not directly used here).\n        \"\"\"\n        super().__init__(\n            data_manager,\n            config,\n            max_split_length_after_split_event,\n            max_lookback_time_for_value,\n            max_forecast_time_for_value,\n        )\n        self.max_length_to_sample = max_length_to_sample\n        self.min_length_to_sample = min_length_to_sample\n        self.unit_length_to_sample = unit_length_to_sample\n\n        assert self.config.data_splitter_events_variables_category_mapping is not None, (\n            \"data_splitter_events_variables_category_mapping must be set in Config for DataSplitterEvents.\"\n            \"For example: { 'death': 'death', 'progression': 'next progression'}\"\n        )\n\n        self.manual_variables_category_mapping = self.config.data_splitter_events_variables_category_mapping\n\n    def setup_variables(self):\n        \"\"\"\n        Setup the variables to be used for sampling.\n        \"\"\"\n\n        #: get all categories available\n        all_categories = self.dm.data_frames[\"events\"][self.config.event_category_col].unique().tolist()\n\n        #: first look at the manual variables\n        self.manual_variables_category_mapping = {\n            x: self.manual_variables_category_mapping[x]\n            for x in self.manual_variables_category_mapping.keys()\n            if x in all_categories\n        }\n\n        # Sanity check to ensure we have at least one variable to sample\n        if len(self.manual_variables_category_mapping) == 0:\n            raise ValueError(\n                \"No valid event categories found in the data for event prediction splitting. \"\n                \"Check the data or adjust data_splitter_events_variables_category_mapping in Config.\"\n            )\n\n    def _sample_manual_variables(self, events_after_split: pd.DataFrame, override_category: str) -&gt; tuple:\n        \"\"\"\n        Sample manual variables from the events occurring after the split date.\n\n        Parameters\n        ----------\n        events_after_split : pd.DataFrame\n            The dataframe containing events that occur after the split date.\n        override_category : str\n            If provided, the sampling is done for this specific category.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the category of the sampled variable,\n            and the descriptive name of the sampled variable.\n        \"\"\"\n\n        if override_category is None:\n            #: we need to uniformly sample the exact variable based on category\n            category = np.random.choice(list(self.manual_variables_category_mapping.keys()))\n        else:\n            category = override_category\n\n        # Also return the descriptive name based on category\n        next_var_descriptive = self.manual_variables_category_mapping[category]\n\n        # We allow backup categories in case the exact category is not present\n        # E.g. in case of progression, try alternatively death, since it is also a progression event\n        if category not in events_after_split[self.config.event_category_col].unique():\n            if category in self.config.data_splitter_events_backup_category_mapping.keys():\n                backup_category = self.config.data_splitter_events_backup_category_mapping[category]\n                if backup_category in events_after_split[self.config.event_category_col].unique():\n                    category = backup_category\n\n        #: return exact variable\n        return category, next_var_descriptive\n\n    def get_splits_from_patient(\n        self,\n        patient_data: dict,\n        max_nr_samples_per_split: int,\n        max_num_splits_per_split_event: int = 1,\n        reference_split_dates: pd.DataFrame = None,\n        override_split_dates: list = None,\n        override_category: str = None,\n        override_observation_time_delta: pd.Timedelta = None,\n    ) -&gt; list[DataSplitterEventsGroup]:\n        \"\"\"\n        Generates event prediction tasks (splits) for a given patient.\n\n        For each unique split event (e.g. Line of Therapy, LoT) start date in the patient's history,\n        this function potentially selects one or more random split points within a defined\n        window after the split event (e.g. LoT start - `max_split_length_after_split_event`). The number of\n        split points selected per LoT is controlled by `max_num_splits_per_split_event`.\n\n        If `reference_split_dates` (typically generated by a parallel forecasting\n        splitter for consistency) is provided, those exact split dates are used instead\n        of random sampling based on the split event. If `override_split_dates` is provided\n        (e.g., for inference), those specific dates are used. Only one of\n        `reference_split_dates` or `override_split_dates` can be used.\n\n        For each chosen split date (`curr_date`), this method generates multiple event\n        prediction tasks (up to `max_nr_samples_per_split`). Each task involves predicting\n        a specific event category (e.g., 'death', 'next line of therapy') within a\n        randomly determined future time window (`end_week_delta`, up to\n        `max_length_to_sample`). The function handles censoring based on\n        subsequent events (like next LoT start or death) or end of available data.\n\n        Parameters\n        ----------\n        patient_data : dict\n            A dictionary containing the patient's data. Expected keys:\n            'events': pd.DataFrame with patient event history, including columns defined\n                      in `self.config` (e.g., date, event category, LoT date).\n            'constant': pd.DataFrame with static patient information.\n        max_nr_samples_per_split : int\n            The maximum number of distinct event prediction tasks (different event\n            categories or prediction windows) to generate for *each* selected split date.\n            The actual number might be less due to random sampling and avoiding duplicates.\n        max_num_splits_per_split_event: int, optional\n            When split dates are *not* overridden, this determines the maximum number\n            of random split dates to select per unique LoT start date during the\n            initial candidate selection. Defaults to 1.\n        reference_split_dates : pd.DataFrame, optional\n            A DataFrame containing specific split dates to use, typically generated by\n            another data splitter (e.g., DataSplitterForecasting) to ensure alignment\n            between different task types. Must contain the columns specified in\n            `self.config.date_col` and `self.config.split_date_col`. If provided,\n            `override_split_dates` must be None. Defaults to None.\n        override_split_dates : list, optional\n            A list of specific datetime objects to use as split dates, typically for\n            inference scenarios. If provided, `reference_split_dates` must be None.\n            Defaults to None.\n        override_category : str, optional\n            If provided, forces the sampling process to only consider this specific\n            event category for prediction, instead of randomly sampling from available\n            categories. Defaults to None.\n        override_observation_time_delta : pd.Timedelta, optional\n            If provided, forces the prediction window to be exactly this duration,\n            instead of randomly sampling a window duration. Defaults to None.\n\n        Returns\n        -------\n        list[DataSplitterEventsGroup]\n            A list where each element corresponds to one of the selected split dates.\n            Each element is a DataSplitterEventsGroup containing multiple DataSplitterEventsOption objects.\n            Each option represents a single event prediction task (split) and contains attributes as\n            defined in DataSplitterEventsOption class.\n\n        Raises\n        ------\n        ValueError\n            If both `reference_split_dates` and `override_split_dates` are provided.\n            If required columns are missing in `patient_data['events']`.\n        AssertionError\n            If internal checks fail, e.g., when using `reference_split_dates` and\n            consistency checks with potential dates fail.\n        TypeError\n            If input arguments have incorrect types.\n        \"\"\"\n\n        # --- Assertions ---\n\n        # Input Type Assertions\n        assert isinstance(patient_data, dict), \"patient_data must be a dictionary.\"\n        assert isinstance(max_nr_samples_per_split, int) and max_nr_samples_per_split &gt; 0, \"max_nr_samples_per_split \"\n        \"must be a positive integer.\"\n        assert isinstance(max_num_splits_per_split_event, int) and max_num_splits_per_split_event &gt; 0, (\n            \"max_num_samples_per_lot must be a positive integer.\"\n        )\n        assert reference_split_dates is None or isinstance(reference_split_dates, pd.DataFrame), (\n            \"reference_split_dates must be None or a pandas DataFrame.\"\n        )\n        assert override_split_dates is None or isinstance(override_split_dates, list), (\n            \"override_split_dates must be None or a list.\"\n        )\n        assert override_category is None or isinstance(override_category, str), (\n            \"override_category must be None or a string.\"\n        )\n        assert override_observation_time_delta is None or isinstance(override_observation_time_delta, pd.Timedelta), (\n            \"override_observation_time_delta must be None or a pandas Timedelta.\"\n        )\n\n        # Input Data Structure and Content Assertions\n        assert \"events\" in patient_data, \"patient_data dictionary must contain the key 'events'.\"\n        assert \"constant\" in patient_data, \"patient_data dictionary must contain the key 'constant'.\"\n        assert isinstance(patient_data[\"events\"], pd.DataFrame), \"patient_data['events'] must be a pandas DataFrame.\"\n        assert isinstance(patient_data[\"constant\"], pd.DataFrame), (\n            \"patient_data['constant'] must be a pandas DataFrame.\"\n        )\n\n        # Check for required columns in the events dataframe\n        required_event_cols = [\n            self.config.date_col,\n            self.config.event_category_col,\n            self.config.event_name_col,\n        ]\n        missing_cols = [col for col in required_event_cols if col not in patient_data[\"events\"].columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns in patient_data['events']: {missing_cols}\")\n\n        # Mutual Exclusivity Assertion for Split Date Sources\n        assert reference_split_dates is None or override_split_dates is None, (\n            \"Cannot provide both reference_split_dates and override_split_dates.\"\n        )\n\n        #: get all possible splits\n        events = patient_data[\"events\"]\n        events = events.sort_values(self.config.date_col)\n\n        # Do some quick sanity checks\n        if self.config.warning_for_splitters_patient_without_splits:\n            lot_events = events[events[self.config.event_category_col] == self.config.event_category_lot]\n            if lot_events.shape[0] == 0:\n                logging.warning(\n                    \"Patient \"\n                    + str(patient_data[\"constant\"][self.config.patient_id_col].iloc[0])\n                    + \" has no split events.\"\n                    \"To disable this warning, set warning_for_splitters_patient_without_splits to False in config.\"\n                )\n\n        if reference_split_dates is None and override_split_dates is None:\n            #: get unique dates, if needed\n            pot_all_possible_split_dates = self._get_all_dates_within_range_of_split_event(\n                patient_data, pd.Timedelta(0), self.max_split_length_after_split_event\n            )\n            pot_all_possible_split_dates = self.select_random_splits(\n                pot_all_possible_split_dates,\n                max_num_splits_per_split_event=max_num_splits_per_split_event,\n            )\n\n            all_possible_split_dates = pot_all_possible_split_dates\n\n        elif reference_split_dates is not None:\n            # Set to the preselected split dates, and do some assertions\n            all_possible_split_dates = reference_split_dates.copy()\n            all_possible_split_dates = all_possible_split_dates.reset_index(drop=True)\n            assert all_possible_split_dates[self.config.date_col].isna().sum() == 0, \"Still missing dates\"\n\n        elif override_split_dates is not None:\n            # If we're overriding the split dates, then we need to create a new dataframe\n            all_possible_split_dates = pd.DataFrame(\n                {\n                    self.config.date_col: override_split_dates,\n                    self.config.split_date_col: [pd.NA] * len(override_split_dates),\n                }\n            )\n\n        else:\n            raise ValueError(\"Invalid split dates provided\")\n\n        ret_splits = []\n\n        for curr_sample_index in range(len(all_possible_split_dates)):\n            #: get current data\n            curr_date, lot_date = all_possible_split_dates.iloc[curr_sample_index, :].tolist()\n\n            #: get the input &amp; output data\n            events_before_split = events[events[self.config.date_col] &lt;= curr_date]\n            events_after_split = events[events[self.config.date_col] &gt; curr_date]\n\n            prev_sampled_category = []\n            ret_split_lot = DataSplitterEventsGroup()\n\n            #: loop through 1 to max_nr_samples_per_split\n            for _ in range(max_nr_samples_per_split):\n                #: sample variables\n                sampled_cateogry, sampled_var_name = self._sample_manual_variables(\n                    events_after_split, override_category\n                )\n\n                #: check if we sampled the same category as before\n                if sampled_cateogry in prev_sampled_category:\n                    continue\n                prev_sampled_category.append(sampled_cateogry)\n\n                # Determine how many weeks to predict into the future\n                if override_observation_time_delta is None:\n                    #: randomly sample end date -&gt; so that we also get random values in between for consistency\n                    # This is so that the model can learn different time values for the same variable\n                    #: To not bias the model, we select a random nr time as max end date``\n\n                    if self.unit_length_to_sample == \"days\":\n                        max_units = self.max_length_to_sample.days\n                        min_units = self.min_length_to_sample.days\n                        random_units = np.random.randint(min_units, max_units + 1)\n                        end_time_delta = pd.Timedelta(days=random_units)\n                    elif self.unit_length_to_sample == \"weeks\":\n                        max_units = self.max_length_to_sample.days // 7\n                        min_units = self.min_length_to_sample.days // 7\n                        random_units = np.random.randint(min_units, max_units + 1)\n                        end_time_delta = pd.Timedelta(weeks=random_units)\n                    else:\n                        raise NotImplementedError(\n                            f\"Unit length to sample {self.unit_length_to_sample} not implemented.\"\n                        )\n                else:\n                    end_time_delta = override_observation_time_delta\n\n                # Process the actual end date\n                end_date = curr_date + end_time_delta\n                end_date = max(end_date, events_after_split[self.config.date_col].min())\n                end_date_within_data = end_date &lt;= events[self.config.date_col].max()\n                events_limited_after_split = events_after_split[events_after_split[self.config.date_col] &lt;= end_date]\n\n                # Get the events\n                diagnosis_after_split = events_limited_after_split[\n                    events_limited_after_split[self.config.event_category_col] == sampled_cateogry\n                ]\n                lot_after_split = events_limited_after_split[\n                    events_limited_after_split[self.config.event_category_col] == self.config.event_category_lot\n                ]\n                death_after_split = events_limited_after_split[\n                    events_limited_after_split[self.config.event_name_col] == self.config.event_category_death\n                ]\n\n                #: apply censoring using next_lot_date\n                next_lot_date = lot_after_split[self.config.date_col].min() if len(lot_after_split) &gt; 0 else None\n                next_death_date = death_after_split[self.config.date_col].min() if len(death_after_split) &gt; 0 else None\n\n                #: determine whether occurred, censored &amp; if so, which date\n                occurred = None\n                censored = None\n                date_occurred = end_date\n\n                if diagnosis_after_split.shape[0] &gt; 0:\n                    # Event occurred within end date\n                    occurred = True\n\n                    # If an lot occurred first though, then we're censored\n                    if next_lot_date is not None and diagnosis_after_split[self.config.date_col].min() &gt; next_lot_date:\n                        censored = \"new_therapy_start\"\n                        occurred = False\n\n                else:\n                    # Event did not occur\n                    occurred = False\n\n                    if next_lot_date is not None:\n                        # If we were censored by the next lot date\n                        censored = \"new_therapy_start\"\n\n                    elif next_death_date is not None:\n                        # If death occurred then not censored, since this is the only time we\n                        # actually know event didn't occur\n                        # In case we're sampling for death var, and it occurred, then it wouldn't trigger this logic\n                        censored = None\n\n                    elif end_date_within_data:\n                        # Event did not occur within the given time frame\n                        censored = None\n\n                    else:\n                        # If we were censored by the end of the data, but not death\n                        censored = \"end_of_data\"\n\n                # Check for data cutoff\n                if self.config.date_cutoff is not None:\n                    if censored is None and occurred is False and end_date &gt; self.config.date_cutoff:\n                        # Check if outside of date cutoff\n                        # if occurred is False and not censored, then we know event didn't occur in the mean time\n                        occurred = False\n                        censored = \"data_cutoff\"\n\n                #: add to return list\n                ret_split_lot.append(\n                    DataSplitterEventsOption(\n                        events_until_split=events_before_split,\n                        constant_data=patient_data[\"constant\"].copy(),\n                        event_occurred=occurred,\n                        event_censored=censored,\n                        observation_end_date=date_occurred,\n                        split_date_included_in_input=curr_date,\n                        sampled_category=str(sampled_cateogry),\n                        sampled_category_name=sampled_var_name,\n                        lot_date=lot_date,\n                    )\n                )\n\n            # Add for current LoT possible splits\n            ret_splits.append(ret_split_lot)\n\n        #: return list\n        return ret_splits\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events.DataSplitterEvents-functions","title":"Functions","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events.DataSplitterEvents.__init__","title":"__init__","text":"<pre><code>__init__(\n    data_manager,\n    config,\n    max_length_to_sample=pd.Timedelta(weeks=104),\n    min_length_to_sample=pd.Timedelta(weeks=1),\n    unit_length_to_sample=\"weeks\",\n    max_split_length_after_split_event=pd.Timedelta(\n        days=90\n    ),\n    max_lookback_time_for_value=pd.Timedelta(days=90),\n    max_forecast_time_for_value=pd.Timedelta(days=90),\n)\n</code></pre> <p>Initialize the DataSplitterEvents class.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManager</code> <p>The data manager to handle data operations.</p> required <code>config</code> <code>Config</code> <p>Configuration object holding constants.</p> required <code>max_length_to_sample</code> <code>Timedelta</code> <p>The maximum number of weeks into the future to sample for event prediction.</p> <code>Timedelta(weeks=104)</code> <code>min_length_to_sample</code> <code>Timedelta</code> <p>The minimum number of weeks into the future to sample for event prediction.</p> <code>Timedelta(weeks=1)</code> <code>unit_length_to_sample</code> <code>str</code> <p>The unit of time for the length to sample (e.g. \"weeks\").</p> <code>'weeks'</code> <code>max_split_length_after_split_event</code> <code>Timedelta</code> <p>The maximum number of days after the split event (e.g. line of therapy) to consider for split points.</p> <code>Timedelta(days=90)</code> <code>max_lookback_time_for_value</code> <code>Timedelta</code> <p>The maximum number of days to look back for a value (inherited but not directly used here).</p> <code>Timedelta(days=90)</code> <code>max_forecast_time_for_value</code> <code>Timedelta</code> <p>The maximum number of days to forecast a value (inherited but not directly used here).</p> <code>Timedelta(days=90)</code> Source code in <code>twinweaver/instruction/data_splitter_events.py</code> <pre><code>def __init__(\n    self,\n    data_manager: DataManager,\n    config: Config,\n    max_length_to_sample: pd.Timedelta = pd.Timedelta(weeks=104),\n    min_length_to_sample: pd.Timedelta = pd.Timedelta(weeks=1),\n    unit_length_to_sample: str = \"weeks\",\n    max_split_length_after_split_event: pd.Timedelta = pd.Timedelta(days=90),\n    max_lookback_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n    max_forecast_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n):\n    \"\"\"\n    Initialize the DataSplitterEvents class.\n\n    Parameters\n    ----------\n    data_manager : DataManager\n        The data manager to handle data operations.\n    config : Config\n        Configuration object holding constants.\n    max_length_to_sample : pd.Timedelta\n        The maximum number of weeks into the future to sample for event prediction.\n    min_length_to_sample : pd.Timedelta\n        The minimum number of weeks into the future to sample for event prediction.\n    unit_length_to_sample : str\n        The unit of time for the length to sample (e.g. \"weeks\").\n    max_split_length_after_split_event : pd.Timedelta, optional\n        The maximum number of days after the split event (e.g. line of therapy) to consider for split points.\n    max_lookback_time_for_value : pd.Timedelta, optional\n        The maximum number of days to look back for a value (inherited but not directly used here).\n    max_forecast_time_for_value : pd.Timedelta, optional\n        The maximum number of days to forecast a value (inherited but not directly used here).\n    \"\"\"\n    super().__init__(\n        data_manager,\n        config,\n        max_split_length_after_split_event,\n        max_lookback_time_for_value,\n        max_forecast_time_for_value,\n    )\n    self.max_length_to_sample = max_length_to_sample\n    self.min_length_to_sample = min_length_to_sample\n    self.unit_length_to_sample = unit_length_to_sample\n\n    assert self.config.data_splitter_events_variables_category_mapping is not None, (\n        \"data_splitter_events_variables_category_mapping must be set in Config for DataSplitterEvents.\"\n        \"For example: { 'death': 'death', 'progression': 'next progression'}\"\n    )\n\n    self.manual_variables_category_mapping = self.config.data_splitter_events_variables_category_mapping\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events.DataSplitterEvents.get_splits_from_patient","title":"get_splits_from_patient","text":"<pre><code>get_splits_from_patient(\n    patient_data,\n    max_nr_samples_per_split,\n    max_num_splits_per_split_event=1,\n    reference_split_dates=None,\n    override_split_dates=None,\n    override_category=None,\n    override_observation_time_delta=None,\n)\n</code></pre> <p>Generates event prediction tasks (splits) for a given patient.</p> <p>For each unique split event (e.g. Line of Therapy, LoT) start date in the patient's history, this function potentially selects one or more random split points within a defined window after the split event (e.g. LoT start - <code>max_split_length_after_split_event</code>). The number of split points selected per LoT is controlled by <code>max_num_splits_per_split_event</code>.</p> <p>If <code>reference_split_dates</code> (typically generated by a parallel forecasting splitter for consistency) is provided, those exact split dates are used instead of random sampling based on the split event. If <code>override_split_dates</code> is provided (e.g., for inference), those specific dates are used. Only one of <code>reference_split_dates</code> or <code>override_split_dates</code> can be used.</p> <p>For each chosen split date (<code>curr_date</code>), this method generates multiple event prediction tasks (up to <code>max_nr_samples_per_split</code>). Each task involves predicting a specific event category (e.g., 'death', 'next line of therapy') within a randomly determined future time window (<code>end_week_delta</code>, up to <code>max_length_to_sample</code>). The function handles censoring based on subsequent events (like next LoT start or death) or end of available data.</p> <p>Parameters:</p> Name Type Description Default <code>patient_data</code> <code>dict</code> <p>A dictionary containing the patient's data. Expected keys: 'events': pd.DataFrame with patient event history, including columns defined           in <code>self.config</code> (e.g., date, event category, LoT date). 'constant': pd.DataFrame with static patient information.</p> required <code>max_nr_samples_per_split</code> <code>int</code> <p>The maximum number of distinct event prediction tasks (different event categories or prediction windows) to generate for each selected split date. The actual number might be less due to random sampling and avoiding duplicates.</p> required <code>max_num_splits_per_split_event</code> <code>int</code> <p>When split dates are not overridden, this determines the maximum number of random split dates to select per unique LoT start date during the initial candidate selection. Defaults to 1.</p> <code>1</code> <code>reference_split_dates</code> <code>DataFrame</code> <p>A DataFrame containing specific split dates to use, typically generated by another data splitter (e.g., DataSplitterForecasting) to ensure alignment between different task types. Must contain the columns specified in <code>self.config.date_col</code> and <code>self.config.split_date_col</code>. If provided, <code>override_split_dates</code> must be None. Defaults to None.</p> <code>None</code> <code>override_split_dates</code> <code>list</code> <p>A list of specific datetime objects to use as split dates, typically for inference scenarios. If provided, <code>reference_split_dates</code> must be None. Defaults to None.</p> <code>None</code> <code>override_category</code> <code>str</code> <p>If provided, forces the sampling process to only consider this specific event category for prediction, instead of randomly sampling from available categories. Defaults to None.</p> <code>None</code> <code>override_observation_time_delta</code> <code>Timedelta</code> <p>If provided, forces the prediction window to be exactly this duration, instead of randomly sampling a window duration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[DataSplitterEventsGroup]</code> <p>A list where each element corresponds to one of the selected split dates. Each element is a DataSplitterEventsGroup containing multiple DataSplitterEventsOption objects. Each option represents a single event prediction task (split) and contains attributes as defined in DataSplitterEventsOption class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>reference_split_dates</code> and <code>override_split_dates</code> are provided. If required columns are missing in <code>patient_data['events']</code>.</p> <code>AssertionError</code> <p>If internal checks fail, e.g., when using <code>reference_split_dates</code> and consistency checks with potential dates fail.</p> <code>TypeError</code> <p>If input arguments have incorrect types.</p> Source code in <code>twinweaver/instruction/data_splitter_events.py</code> <pre><code>def get_splits_from_patient(\n    self,\n    patient_data: dict,\n    max_nr_samples_per_split: int,\n    max_num_splits_per_split_event: int = 1,\n    reference_split_dates: pd.DataFrame = None,\n    override_split_dates: list = None,\n    override_category: str = None,\n    override_observation_time_delta: pd.Timedelta = None,\n) -&gt; list[DataSplitterEventsGroup]:\n    \"\"\"\n    Generates event prediction tasks (splits) for a given patient.\n\n    For each unique split event (e.g. Line of Therapy, LoT) start date in the patient's history,\n    this function potentially selects one or more random split points within a defined\n    window after the split event (e.g. LoT start - `max_split_length_after_split_event`). The number of\n    split points selected per LoT is controlled by `max_num_splits_per_split_event`.\n\n    If `reference_split_dates` (typically generated by a parallel forecasting\n    splitter for consistency) is provided, those exact split dates are used instead\n    of random sampling based on the split event. If `override_split_dates` is provided\n    (e.g., for inference), those specific dates are used. Only one of\n    `reference_split_dates` or `override_split_dates` can be used.\n\n    For each chosen split date (`curr_date`), this method generates multiple event\n    prediction tasks (up to `max_nr_samples_per_split`). Each task involves predicting\n    a specific event category (e.g., 'death', 'next line of therapy') within a\n    randomly determined future time window (`end_week_delta`, up to\n    `max_length_to_sample`). The function handles censoring based on\n    subsequent events (like next LoT start or death) or end of available data.\n\n    Parameters\n    ----------\n    patient_data : dict\n        A dictionary containing the patient's data. Expected keys:\n        'events': pd.DataFrame with patient event history, including columns defined\n                  in `self.config` (e.g., date, event category, LoT date).\n        'constant': pd.DataFrame with static patient information.\n    max_nr_samples_per_split : int\n        The maximum number of distinct event prediction tasks (different event\n        categories or prediction windows) to generate for *each* selected split date.\n        The actual number might be less due to random sampling and avoiding duplicates.\n    max_num_splits_per_split_event: int, optional\n        When split dates are *not* overridden, this determines the maximum number\n        of random split dates to select per unique LoT start date during the\n        initial candidate selection. Defaults to 1.\n    reference_split_dates : pd.DataFrame, optional\n        A DataFrame containing specific split dates to use, typically generated by\n        another data splitter (e.g., DataSplitterForecasting) to ensure alignment\n        between different task types. Must contain the columns specified in\n        `self.config.date_col` and `self.config.split_date_col`. If provided,\n        `override_split_dates` must be None. Defaults to None.\n    override_split_dates : list, optional\n        A list of specific datetime objects to use as split dates, typically for\n        inference scenarios. If provided, `reference_split_dates` must be None.\n        Defaults to None.\n    override_category : str, optional\n        If provided, forces the sampling process to only consider this specific\n        event category for prediction, instead of randomly sampling from available\n        categories. Defaults to None.\n    override_observation_time_delta : pd.Timedelta, optional\n        If provided, forces the prediction window to be exactly this duration,\n        instead of randomly sampling a window duration. Defaults to None.\n\n    Returns\n    -------\n    list[DataSplitterEventsGroup]\n        A list where each element corresponds to one of the selected split dates.\n        Each element is a DataSplitterEventsGroup containing multiple DataSplitterEventsOption objects.\n        Each option represents a single event prediction task (split) and contains attributes as\n        defined in DataSplitterEventsOption class.\n\n    Raises\n    ------\n    ValueError\n        If both `reference_split_dates` and `override_split_dates` are provided.\n        If required columns are missing in `patient_data['events']`.\n    AssertionError\n        If internal checks fail, e.g., when using `reference_split_dates` and\n        consistency checks with potential dates fail.\n    TypeError\n        If input arguments have incorrect types.\n    \"\"\"\n\n    # --- Assertions ---\n\n    # Input Type Assertions\n    assert isinstance(patient_data, dict), \"patient_data must be a dictionary.\"\n    assert isinstance(max_nr_samples_per_split, int) and max_nr_samples_per_split &gt; 0, \"max_nr_samples_per_split \"\n    \"must be a positive integer.\"\n    assert isinstance(max_num_splits_per_split_event, int) and max_num_splits_per_split_event &gt; 0, (\n        \"max_num_samples_per_lot must be a positive integer.\"\n    )\n    assert reference_split_dates is None or isinstance(reference_split_dates, pd.DataFrame), (\n        \"reference_split_dates must be None or a pandas DataFrame.\"\n    )\n    assert override_split_dates is None or isinstance(override_split_dates, list), (\n        \"override_split_dates must be None or a list.\"\n    )\n    assert override_category is None or isinstance(override_category, str), (\n        \"override_category must be None or a string.\"\n    )\n    assert override_observation_time_delta is None or isinstance(override_observation_time_delta, pd.Timedelta), (\n        \"override_observation_time_delta must be None or a pandas Timedelta.\"\n    )\n\n    # Input Data Structure and Content Assertions\n    assert \"events\" in patient_data, \"patient_data dictionary must contain the key 'events'.\"\n    assert \"constant\" in patient_data, \"patient_data dictionary must contain the key 'constant'.\"\n    assert isinstance(patient_data[\"events\"], pd.DataFrame), \"patient_data['events'] must be a pandas DataFrame.\"\n    assert isinstance(patient_data[\"constant\"], pd.DataFrame), (\n        \"patient_data['constant'] must be a pandas DataFrame.\"\n    )\n\n    # Check for required columns in the events dataframe\n    required_event_cols = [\n        self.config.date_col,\n        self.config.event_category_col,\n        self.config.event_name_col,\n    ]\n    missing_cols = [col for col in required_event_cols if col not in patient_data[\"events\"].columns]\n    if missing_cols:\n        raise ValueError(f\"Missing required columns in patient_data['events']: {missing_cols}\")\n\n    # Mutual Exclusivity Assertion for Split Date Sources\n    assert reference_split_dates is None or override_split_dates is None, (\n        \"Cannot provide both reference_split_dates and override_split_dates.\"\n    )\n\n    #: get all possible splits\n    events = patient_data[\"events\"]\n    events = events.sort_values(self.config.date_col)\n\n    # Do some quick sanity checks\n    if self.config.warning_for_splitters_patient_without_splits:\n        lot_events = events[events[self.config.event_category_col] == self.config.event_category_lot]\n        if lot_events.shape[0] == 0:\n            logging.warning(\n                \"Patient \"\n                + str(patient_data[\"constant\"][self.config.patient_id_col].iloc[0])\n                + \" has no split events.\"\n                \"To disable this warning, set warning_for_splitters_patient_without_splits to False in config.\"\n            )\n\n    if reference_split_dates is None and override_split_dates is None:\n        #: get unique dates, if needed\n        pot_all_possible_split_dates = self._get_all_dates_within_range_of_split_event(\n            patient_data, pd.Timedelta(0), self.max_split_length_after_split_event\n        )\n        pot_all_possible_split_dates = self.select_random_splits(\n            pot_all_possible_split_dates,\n            max_num_splits_per_split_event=max_num_splits_per_split_event,\n        )\n\n        all_possible_split_dates = pot_all_possible_split_dates\n\n    elif reference_split_dates is not None:\n        # Set to the preselected split dates, and do some assertions\n        all_possible_split_dates = reference_split_dates.copy()\n        all_possible_split_dates = all_possible_split_dates.reset_index(drop=True)\n        assert all_possible_split_dates[self.config.date_col].isna().sum() == 0, \"Still missing dates\"\n\n    elif override_split_dates is not None:\n        # If we're overriding the split dates, then we need to create a new dataframe\n        all_possible_split_dates = pd.DataFrame(\n            {\n                self.config.date_col: override_split_dates,\n                self.config.split_date_col: [pd.NA] * len(override_split_dates),\n            }\n        )\n\n    else:\n        raise ValueError(\"Invalid split dates provided\")\n\n    ret_splits = []\n\n    for curr_sample_index in range(len(all_possible_split_dates)):\n        #: get current data\n        curr_date, lot_date = all_possible_split_dates.iloc[curr_sample_index, :].tolist()\n\n        #: get the input &amp; output data\n        events_before_split = events[events[self.config.date_col] &lt;= curr_date]\n        events_after_split = events[events[self.config.date_col] &gt; curr_date]\n\n        prev_sampled_category = []\n        ret_split_lot = DataSplitterEventsGroup()\n\n        #: loop through 1 to max_nr_samples_per_split\n        for _ in range(max_nr_samples_per_split):\n            #: sample variables\n            sampled_cateogry, sampled_var_name = self._sample_manual_variables(\n                events_after_split, override_category\n            )\n\n            #: check if we sampled the same category as before\n            if sampled_cateogry in prev_sampled_category:\n                continue\n            prev_sampled_category.append(sampled_cateogry)\n\n            # Determine how many weeks to predict into the future\n            if override_observation_time_delta is None:\n                #: randomly sample end date -&gt; so that we also get random values in between for consistency\n                # This is so that the model can learn different time values for the same variable\n                #: To not bias the model, we select a random nr time as max end date``\n\n                if self.unit_length_to_sample == \"days\":\n                    max_units = self.max_length_to_sample.days\n                    min_units = self.min_length_to_sample.days\n                    random_units = np.random.randint(min_units, max_units + 1)\n                    end_time_delta = pd.Timedelta(days=random_units)\n                elif self.unit_length_to_sample == \"weeks\":\n                    max_units = self.max_length_to_sample.days // 7\n                    min_units = self.min_length_to_sample.days // 7\n                    random_units = np.random.randint(min_units, max_units + 1)\n                    end_time_delta = pd.Timedelta(weeks=random_units)\n                else:\n                    raise NotImplementedError(\n                        f\"Unit length to sample {self.unit_length_to_sample} not implemented.\"\n                    )\n            else:\n                end_time_delta = override_observation_time_delta\n\n            # Process the actual end date\n            end_date = curr_date + end_time_delta\n            end_date = max(end_date, events_after_split[self.config.date_col].min())\n            end_date_within_data = end_date &lt;= events[self.config.date_col].max()\n            events_limited_after_split = events_after_split[events_after_split[self.config.date_col] &lt;= end_date]\n\n            # Get the events\n            diagnosis_after_split = events_limited_after_split[\n                events_limited_after_split[self.config.event_category_col] == sampled_cateogry\n            ]\n            lot_after_split = events_limited_after_split[\n                events_limited_after_split[self.config.event_category_col] == self.config.event_category_lot\n            ]\n            death_after_split = events_limited_after_split[\n                events_limited_after_split[self.config.event_name_col] == self.config.event_category_death\n            ]\n\n            #: apply censoring using next_lot_date\n            next_lot_date = lot_after_split[self.config.date_col].min() if len(lot_after_split) &gt; 0 else None\n            next_death_date = death_after_split[self.config.date_col].min() if len(death_after_split) &gt; 0 else None\n\n            #: determine whether occurred, censored &amp; if so, which date\n            occurred = None\n            censored = None\n            date_occurred = end_date\n\n            if diagnosis_after_split.shape[0] &gt; 0:\n                # Event occurred within end date\n                occurred = True\n\n                # If an lot occurred first though, then we're censored\n                if next_lot_date is not None and diagnosis_after_split[self.config.date_col].min() &gt; next_lot_date:\n                    censored = \"new_therapy_start\"\n                    occurred = False\n\n            else:\n                # Event did not occur\n                occurred = False\n\n                if next_lot_date is not None:\n                    # If we were censored by the next lot date\n                    censored = \"new_therapy_start\"\n\n                elif next_death_date is not None:\n                    # If death occurred then not censored, since this is the only time we\n                    # actually know event didn't occur\n                    # In case we're sampling for death var, and it occurred, then it wouldn't trigger this logic\n                    censored = None\n\n                elif end_date_within_data:\n                    # Event did not occur within the given time frame\n                    censored = None\n\n                else:\n                    # If we were censored by the end of the data, but not death\n                    censored = \"end_of_data\"\n\n            # Check for data cutoff\n            if self.config.date_cutoff is not None:\n                if censored is None and occurred is False and end_date &gt; self.config.date_cutoff:\n                    # Check if outside of date cutoff\n                    # if occurred is False and not censored, then we know event didn't occur in the mean time\n                    occurred = False\n                    censored = \"data_cutoff\"\n\n            #: add to return list\n            ret_split_lot.append(\n                DataSplitterEventsOption(\n                    events_until_split=events_before_split,\n                    constant_data=patient_data[\"constant\"].copy(),\n                    event_occurred=occurred,\n                    event_censored=censored,\n                    observation_end_date=date_occurred,\n                    split_date_included_in_input=curr_date,\n                    sampled_category=str(sampled_cateogry),\n                    sampled_category_name=sampled_var_name,\n                    lot_date=lot_date,\n                )\n            )\n\n        # Add for current LoT possible splits\n        ret_splits.append(ret_split_lot)\n\n    #: return list\n    return ret_splits\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events.DataSplitterEvents.setup_variables","title":"setup_variables","text":"<pre><code>setup_variables()\n</code></pre> <p>Setup the variables to be used for sampling.</p> Source code in <code>twinweaver/instruction/data_splitter_events.py</code> <pre><code>def setup_variables(self):\n    \"\"\"\n    Setup the variables to be used for sampling.\n    \"\"\"\n\n    #: get all categories available\n    all_categories = self.dm.data_frames[\"events\"][self.config.event_category_col].unique().tolist()\n\n    #: first look at the manual variables\n    self.manual_variables_category_mapping = {\n        x: self.manual_variables_category_mapping[x]\n        for x in self.manual_variables_category_mapping.keys()\n        if x in all_categories\n    }\n\n    # Sanity check to ensure we have at least one variable to sample\n    if len(self.manual_variables_category_mapping) == 0:\n        raise ValueError(\n            \"No valid event categories found in the data for event prediction splitting. \"\n            \"Check the data or adjust data_splitter_events_variables_category_mapping in Config.\"\n        )\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events.DataSplitterEventsGroup","title":"DataSplitterEventsGroup","text":"<p>A class to hold a group of event prediction options for a single split date. Usually one of the elements in this list is then used, e.g. by random selection in converter_manual_instruction.</p> Source code in <code>twinweaver/instruction/data_splitter_events.py</code> <pre><code>class DataSplitterEventsGroup:\n    \"\"\"\n    A class to hold a group of event prediction options for a single split date.\n    Usually one of the elements in this list is then used, e.g. by random\n    selection in converter_manual_instruction.\n    \"\"\"\n\n    def __init__(\n        self,\n        events_options: list[DataSplitterEventsOption] = None,\n    ):\n        if events_options is None:\n            events_options = []\n        self.events_options = events_options\n\n    def append(self, option: DataSplitterEventsOption):\n        self.events_options.append(option)\n\n    def __len__(self):\n        return len(self.events_options)\n\n    def __getitem__(self, index):\n        return self.events_options[index]\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_events.DataSplitterEventsOption","title":"DataSplitterEventsOption","text":"<p>A class to hold the options for a single event prediction split.</p> <p>Attributes:</p> Name Type Description <code>events_until_split</code> <code>DataFrame</code> <p>DataFrame of events up to the split date.</p> <code>constant_data</code> <code>DataFrame</code> <p>DataFrame of constant patient data.</p> <code>event_occurred</code> <code>bool</code> <p>Whether the event occurred within the prediction window.</p> <code>event_censored</code> <code>bool</code> <p>Whether the event was censored.</p> <code>observation_end_date</code> <code>Timestamp</code> <p>The date the event occurred or the end date of the prediction window.</p> <code>split_date_included_in_input</code> <code>Timestamp</code> <p>The split date used for input.</p> <code>sampled_category</code> <code>str</code> <p>The event category being predicted.</p> <code>sampled_category_name</code> <code>str</code> <p>Descriptive name for the sampled category.</p> <code>end_date</code> <code>Timestamp</code> <p>The end of the prediction window.</p> <code>lot_date</code> <code>Timestamp</code> <p>The Line of Therapy (LoT) start date associated with this split point.</p> Source code in <code>twinweaver/instruction/data_splitter_events.py</code> <pre><code>class DataSplitterEventsOption:\n    \"\"\"\n    A class to hold the options for a single event prediction split.\n\n    Attributes\n    ----------\n    events_until_split : pd.DataFrame\n        DataFrame of events up to the split date.\n    constant_data : pd.DataFrame\n        DataFrame of constant patient data.\n    event_occurred : bool\n        Whether the event occurred within the prediction window.\n    event_censored : bool\n        Whether the event was censored.\n    observation_end_date : pd.Timestamp\n        The date the event occurred or the end date of the prediction window.\n    split_date_included_in_input : pd.Timestamp\n        The split date used for input.\n    sampled_category : str\n        The event category being predicted.\n    sampled_category_name : str\n        Descriptive name for the sampled category.\n    end_date : pd.Timestamp\n        The end of the prediction window.\n    lot_date : pd.Timestamp\n        The Line of Therapy (LoT) start date associated with this split point.\n    \"\"\"\n\n    def __init__(\n        self,\n        events_until_split: pd.DataFrame,\n        constant_data: pd.DataFrame,\n        event_occurred: bool,\n        event_censored: bool,\n        observation_end_date: pd.Timestamp,\n        split_date_included_in_input: pd.Timestamp,\n        sampled_category: str,\n        sampled_category_name: str,\n        lot_date: pd.Timestamp,\n    ):\n        self.events_until_split = events_until_split\n        self.constant_data = constant_data\n        self.event_occurred = event_occurred\n        self.event_censored = event_censored\n        self.observation_end_date = observation_end_date\n        self.split_date_included_in_input = split_date_included_in_input\n        self.sampled_category = sampled_category\n        self.sampled_category_name = sampled_category_name\n        self.lot_date = lot_date\n</code></pre>"},{"location":"reference/instruction/data_splitters/#forecasting-splitter","title":"Forecasting Splitter","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting","title":"twinweaver.instruction.data_splitter_forecasting","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting-classes","title":"Classes","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting.DataSplitterForecasting","title":"DataSplitterForecasting","text":"<p>               Bases: <code>BaseDataSplitter</code></p> <p>Generates forecasting tasks by splitting patient time-series data.</p> <p>This class identifies suitable points in a patient's timeline (split dates) and selects specific variables (e.g., lab results) to forecast, and separates them in numeric and categorical. - Numeric branch: it calculates statistics on variable predictability using a simple baseline (copy-forward) to inform variable sampling. It handles filtering outliers and ensures splits meet criteria like minimum data points before and after the split date. - Categorical branch: no statistics are calculated, and variables are sampled uniformly (without using scores).</p> <p>Attributes:</p> Name Type Description <code>variable_stats</code> <code>DataFrame | None</code> <p>Statistics calculated for each potential variable (e.g., R\u00b2, NRMSE based on copy-forward baseline). Computed by <code>setup_statistics</code>.</p> <code>min_num_samples_for_statistics</code> <code>int</code> <p>Minimum data points required per variable across the training set to compute statistics.</p> <code>sampling_score_to_use</code> <code>str</code> <p>The column name in <code>variable_stats</code> used as a score for weighted sampling of variables to forecast.</p> <code>min_nr_variable_seen_previously</code> <code>int</code> <p>Minimum occurrences of a variable required within the lookback period before a split date.</p> <code>min_nr_variable_seen_after</code> <code>int</code> <p>Minimum occurrences of a variable required within the forecast period after a split date.</p> <code>list_of_valid_categories</code> <code>list</code> <p>Event categories (e.g., ['lab']) to consider for forecasting tasks.</p> <code>save_path_for_variable_stats</code> <code>str | None</code> <p>Optional path to save the computed <code>variable_stats</code> DataFrame.</p> <code>min_nr_variables_to_sample</code> <code>int</code> <p>Minimum number of distinct variables to include in a single forecasting task sample.</p> <code>max_nr_variables_to_sample</code> <code>int</code> <p>Maximum number of distinct variables to include in a single forecasting task sample.</p> <code>filtering_strategy</code> <code>str</code> <p>Strategy name ('3-sigma') used to filter or clip outlier values in the target data.</p> <code>_filtering_methods</code> <code>dict</code> <p>Maps filtering strategy names to methods.</p> Source code in <code>twinweaver/instruction/data_splitter_forecasting.py</code> <pre><code>class DataSplitterForecasting(BaseDataSplitter):\n    \"\"\"\n    Generates forecasting tasks by splitting patient time-series data.\n\n    This class identifies suitable points in a patient's timeline (split dates)\n    and selects specific variables (e.g., lab results) to forecast, and separates them in numeric and categorical.\n    - Numeric branch: it calculates statistics on variable predictability using a simple baseline (copy-forward)\n    to inform variable sampling. It handles filtering outliers and ensures\n    splits meet criteria like minimum data points before and after the split date.\n    - Categorical branch: no statistics are calculated, and variables are sampled uniformly (without using scores).\n\n    Attributes\n    ----------\n    variable_stats : pd.DataFrame | None\n        Statistics calculated for each potential variable (e.g., R\u00b2, NRMSE based on copy-forward baseline).\n        Computed by `setup_statistics`.\n    min_num_samples_for_statistics : int\n        Minimum data points required per variable across the training set to compute statistics.\n    sampling_score_to_use : str\n        The column name in `variable_stats` used as a score for weighted sampling of variables to forecast.\n    min_nr_variable_seen_previously : int\n        Minimum occurrences of a variable required within the lookback period before a split date.\n    min_nr_variable_seen_after : int\n        Minimum occurrences of a variable required within the forecast period after a split date.\n    list_of_valid_categories : list\n        Event categories (e.g., ['lab']) to consider for forecasting tasks.\n    save_path_for_variable_stats : str | None\n        Optional path to save the computed `variable_stats` DataFrame.\n    min_nr_variables_to_sample : int\n        Minimum number of distinct variables to include in a single forecasting task sample.\n    max_nr_variables_to_sample : int\n        Maximum number of distinct variables to include in a single forecasting task sample.\n    filtering_strategy : str\n        Strategy name ('3-sigma') used to filter or clip outlier values in the target data.\n    _filtering_methods : dict\n        Maps filtering strategy names to methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Config,\n        data_manager: DataManager,\n        max_split_length_after_split_event: pd.Timedelta = pd.Timedelta(days=90),\n        max_lookback_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n        max_forecast_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n        min_num_samples_for_statistics: int = 10,\n        sampling_score_to_use: str = \"score_log_nrmse_n_samples\",\n        min_nr_variable_seen_previously: int = 1,\n        min_nr_variable_seen_after: int = 1,\n        list_of_valid_categories: list = None,\n        save_path_for_variable_stats: str = None,\n        min_nr_variables_to_sample: int = 1,\n        max_nr_variables_to_sample: int = 3,\n        filtering_strategy: str = \"3-sigma\",\n        sampling_strategy: str = \"proportional\",\n    ):\n        \"\"\"\n        Initializes the DataSplitterForecasting instance.\n\n        Parameters\n        ----------\n        config : Config\n            Configuration object containing shared settings like column names.\n        data_manager : DataManager\n            Provides access to patient data for a single indication.\n        max_split_length_after_split_event : pd.Timedelta\n            Max days after LoT start to consider for split dates. Defaults to 90.\n        max_lookback_time_for_value : pd.Timedelta\n            Max days before a split date to look for past variable occurrences.\n            Defaults to 90.\n        max_forecast_time_for_value : pd.Timedelta\n            Max days after a split date to look for future variable occurrences (target\n            data). Defaults to 90.\n        min_num_samples_for_statistics : int\n            Minimum total occurrences of a variable across the training set\n            needed to calculate statistics. Defaults to 50.\n        sampling_score_to_use : str\n            Column name in the computed statistics table used for weighted sampling of variables.\n            Defaults to 'score_log_nrmse_n_samples'.\n        min_nr_variable_seen_previously : int\n            Min occurrences of a variable required in the lookback window for a split\n            to be valid for that variable. Defaults to 1.\n        min_nr_variable_seen_after : int\n            Min occurrences of a variable required in the forecast window for a split to be\n            valid for that variable. Defaults to 1.\n        list_of_valid_categories : list\n            List of event categories to consider for forecasting (e.g., ['LABS']). Defaults\n            to `config.event_category_forecast`.\n        save_path_for_variable_stats : str, optional\n            Optional file path to save the calculated variable statistics CSV. Defaults to\n            None.\n        min_nr_variables_to_sample : int\n            The minimum number of distinct variables to attempt to sample for each\n            forecasting task. Defaults to 3.\n        max_nr_variables_to_sample : int\n            The maximum number of distinct variables to attempt to sample for each\n            forecasting task. Defaults to 3.\n        filtering_strategy : str\n            The strategy for handling outliers in target variable values ('3-sigma').\n            Defaults to '3-sigma'.\n        sampling_strategy : str\n            The strategy for sampling variables ('proportional' or 'uniform').\n            Defaults to 'proportional'.\n        \"\"\"\n        super().__init__(\n            data_manager,\n            config,\n            max_split_length_after_split_event,\n            max_lookback_time_for_value,\n            max_forecast_time_for_value,\n        )\n\n        assert self.config.event_category_forecast is not None or list_of_valid_categories is not None, (\n            \"event_category_forecast must be set in Config for DataSplitterForecasting.\"\n            \"For example: ['lab']\"\n            \" Alternatively, provide list_of_valid_categories directly.\"\n        )\n\n        self.variable_stats = None\n        self.variable_type = {}  # event_name -&gt; \"numeric\" / \"categorical\"\n        self.min_num_samples_for_statistics = min_num_samples_for_statistics\n        self.sampling_score_to_use = sampling_score_to_use\n\n        self.min_nr_variable_seen_previously = min_nr_variable_seen_previously\n        self.min_nr_variable_seen_after = min_nr_variable_seen_after\n        self.list_of_valid_categories = (\n            list_of_valid_categories if list_of_valid_categories is not None else self.config.event_category_forecast\n        )\n        self.save_path_for_variable_stats = save_path_for_variable_stats\n        self.min_nr_variables_to_sample = min_nr_variables_to_sample\n        self.max_nr_variables_to_sample = max_nr_variables_to_sample\n        self.filtering_strategy = filtering_strategy\n        self.sampling_strategy = sampling_strategy\n\n        self._filtering_methods = {\"3-sigma\": self._filter_3_sigma}\n\n    def setup_statistics(self, train_patientids: list = None):\n        \"\"\"\n        Calculates baseline performance statistics for variables.\n\n        Iterates through all patients in the training set and potential split\n        dates within specified ranges around Lines of Therapy (LoTs). For each\n        numeric variable (typically labs), it calculates metrics like R\u00b2, NRMSE,\n        MAPE, mean, standard deviation, etc., based on a simple \"copy forward\"\n        prediction baseline (predicting the next value as the previous one).\n        These statistics quantify the inherent predictability/variability of each\n        variable and are stored in the `self.variable_stats` DataFrame. This\n        dataframe is used later for filtering variables and weighted sampling\n        during split generation.\n        \"\"\"\n\n        events = self.dm.data_frames[self.config.event_table_name]\n\n        #: Assert that no event values are NaN\n        assert events[self.config.event_value_col].notna().all(), (\n            \"There are NaN values in event_value column of events table\"\n        )\n\n        #: setup forecasting variables\n        mask_cat = events[self.config.event_category_col].isin(self.config.event_category_forecast)\n        all_vars = events[mask_cat][self.config.event_name_col].unique()\n\n        #: setup all possible split dates by looping through all patients\n        all_possible_split_dates = []\n        relevant_events = events[\n            [\n                self.config.patient_id_col,\n                self.config.date_col,\n                self.config.event_category_col,\n                self.config.event_name_col,\n            ]\n        ].copy()\n        relevant_events = relevant_events.sort_values([self.config.patient_id_col, self.config.date_col])\n        grouped_events = relevant_events.groupby(self.config.patient_id_col)\n\n        for idx, (patientid, event_data) in enumerate(grouped_events):\n            if idx % 1000 == 0:\n                logging.info(f\"Processing patient ({idx + 1}/{len(self.dm.all_patientids)})\")\n            temp_patient_data = {\"events\": event_data}\n\n            temp_splits = self._get_all_dates_within_range_of_split_event(\n                temp_patient_data,\n                time_before_lot_start=self.max_lookback_time_for_value,\n                max_split_length_after_split_event=self.max_forecast_time_for_value,\n            )\n            temp_splits[self.config.patient_id_col] = patientid\n            temp_splits = temp_splits[[self.config.date_col, self.config.patient_id_col]]\n            all_possible_split_dates.append(temp_splits)\n            del temp_patient_data\n\n        all_possible_split_dates = pd.concat(all_possible_split_dates, axis=0, ignore_index=True)\n        all_possible_split_dates = all_possible_split_dates.drop_duplicates()\n\n        #: filter to only train patients\n        if train_patientids is not None:\n            all_train_patientids = train_patientids\n        else:\n            all_train_patientids = self.dm.get_all_patientids_in_split(self.config.train_split_name)\n        rows_to_select = all_possible_split_dates[self.config.patient_id_col].isin(all_train_patientids)\n        all_possible_split_dates = all_possible_split_dates[rows_to_select]\n\n        # Setup status\n        self.variable_stats = {}\n\n        #: loop through every forecasting variable\n        for idx, fore_var in enumerate(all_vars):\n            if idx % 20 == 0:\n                logging.info(f\"Processing forecasting variable {fore_var} ({idx + 1}/{len(all_vars)})\")\n\n            # Get corresponding events, sorted by date and patientid\n            curr_events = (\n                events[events[self.config.event_name_col] == fore_var]\n                .copy()\n                .sort_values([self.config.patient_id_col, self.config.date_col])\n            )\n            descriptive_name = curr_events[self.config.event_descriptive_name_col].iloc[0]\n            curr_events = curr_events[\n                [\n                    self.config.patient_id_col,\n                    self.config.date_col,\n                    self.config.event_value_col,\n                ]\n            ].drop_duplicates()\n\n            # : extract only those dates which are given in self._get_all_possible_split_dates\n            # by doing inner join with all_possible_split_dates\n            curr_events = curr_events.merge(\n                all_possible_split_dates,\n                on=[self.config.patient_id_col, self.config.date_col],\n                how=\"inner\",\n            )\n\n            if self.dm.variable_types.get(fore_var) == \"numeric\":\n                # Numeric path: try to parse as numeric\n                curr_events[self.config.event_value_col] = pd.to_numeric(\n                    curr_events[self.config.event_value_col], errors=\"coerce\"\n                )\n                # Shift values by one for copy forward\n                curr_events[\"predicted_value\"] = curr_events.groupby(self.config.patient_id_col)[\n                    self.config.event_value_col\n                ].shift(1)\n\n                # Drop rows where either true value or predicted_value is NaN (first value for each patient)\n                valid_events = curr_events.dropna(subset=[self.config.event_value_col, \"predicted_value\"])\n\n                # Need at least 2 samples for R^2, else, we should ignore the variable anyway\n                if valid_events.shape[0] &gt;= self.min_num_samples_for_statistics:\n                    # Calculate R\u00b2 across all\n                    r2 = r2_score(\n                        valid_events[self.config.event_value_col],\n                        valid_events[\"predicted_value\"],\n                    )\n\n                    # Calculate NRMSE\n                    mse = np.mean((valid_events[self.config.event_value_col] - valid_events[\"predicted_value\"]) ** 2)\n                    rmse = np.sqrt(mse)\n                    nrmse = rmse / (valid_events[self.config.event_value_col].std())\n\n                    # Calculate mape\n                    mape = (\n                        np.mean(\n                            np.abs(\n                                (valid_events[self.config.event_value_col] - valid_events[\"predicted_value\"])\n                                / valid_events[self.config.event_value_col]\n                            )\n                        )\n                        * 100\n                    )\n\n                    # Calculate score\n                    score_nrmse_n_samples = nrmse * valid_events.shape[0]\n                    score_log_nrmse_n_samples = np.log2(score_nrmse_n_samples)\n\n                    # Calculate buckets\n                    _, bin_5_edges = pd.qcut(\n                        valid_events[self.config.event_value_col],\n                        q=5,\n                        retbins=True,\n                        labels=False,\n                        duplicates=\"drop\",\n                    )\n\n                    # Calculate mean and std after removing over 3 standard deviations\n                    mean = valid_events[self.config.event_value_col].mean()\n                    std = valid_events[self.config.event_value_col].std()\n                    valid_events = valid_events.copy()\n\n                    valid_events[self.config.event_value_col + \"_cleaned\"] = valid_events[\n                        self.config.event_value_col\n                    ].apply(lambda x: x if (x &gt; mean - 3 * std) and (x &lt; mean + 3 * std) else np.nan)\n\n                    mean_without_outliers = np.nanmean(valid_events[self.config.event_value_col + \"_cleaned\"].values)\n                    std_without_outliers = np.nanstd(valid_events[self.config.event_value_col + \"_cleaned\"].values)\n\n                    # Record\n                    self.variable_stats[fore_var] = {\n                        \"event_descriptive_name\": descriptive_name,\n                        \"is_numeric\": True,\n                        \"r2\": r2,\n                        \"nrmse\": nrmse,\n                        \"mape\": mape,\n                        \"score_nrmse_n_samples\": score_nrmse_n_samples,\n                        \"score_log_nrmse_n_samples\": score_log_nrmse_n_samples,\n                        \"std\": std,\n                        \"mean\": mean,\n                        \"range\": valid_events[self.config.event_value_col].max()\n                        - valid_events[self.config.event_value_col].min(),\n                        \"num_samples\": valid_events.shape[0],\n                        \"5_equal_sized_bins\": bin_5_edges.tolist(),\n                        \"mean_without_outliers\": mean_without_outliers,\n                        \"std_without_outliers\": std_without_outliers,\n                    }\n            else:\n                # Categorical path: keep as strings\n                curr_events[self.config.event_value_col] = curr_events[self.config.event_value_col].astype(str)\n\n                # Remove rows where event_value is missing\n                valid_events = curr_events.dropna(subset=[self.config.event_value_col])\n\n                # Build placeholder stats, no real metrics calculated\n                self.variable_stats[fore_var] = {\n                    \"event_descriptive_name\": descriptive_name,\n                    \"is_numeric\": False,\n                    # Keep any numeric-score columns unused for cats\n                    \"r2\": np.nan,\n                    \"nrmse\": np.nan,\n                    \"mape\": np.nan,\n                    \"score_nrmse_n_samples\": np.nan,\n                    \"score_log_nrmse_n_samples\": np.nan,\n                    \"std\": np.nan,\n                    \"mean\": np.nan,\n                    \"range\": np.nan,\n                    \"num_samples\": valid_events.shape[0],\n                    \"5_equal_sized_bins\": [],\n                    \"mean_without_outliers\": np.nan,\n                    \"std_without_outliers\": np.nan,\n                }\n\n        #: turn into a pandas dataframe\n        self.variable_stats = pd.DataFrame(self.variable_stats).T\n        self.variable_stats = self.variable_stats.reset_index(drop=False, names=self.config.event_name_col)\n\n        # Print some statistics\n        logging.info(f\"Number of variables included in selection: {self.variable_stats.shape[0]}\")\n        logging.info(f\"Mean of score used for sampling: {self.variable_stats[self.sampling_score_to_use].mean()}\")\n        logging.info(f\"Std of score used for sampling: {self.variable_stats[self.sampling_score_to_use].std()}\")\n        logging.info(f\"Min of score used for sampling: {self.variable_stats[self.sampling_score_to_use].min()}\")\n        logging.info(f\"Max of score used for sampling: {self.variable_stats[self.sampling_score_to_use].max()}\")\n\n        assert self.variable_stats.shape[0] &gt; 0, (\n            \"Error - for some reason no variables have been included in the statistics table. Check your data &amp; setup.\"\n        )\n\n        # Save if requested\n        if self.save_path_for_variable_stats is not None:\n            self.variable_stats.to_csv(self.save_path_for_variable_stats)\n\n    def _sample_proportionally(self, possible_variables: list, num_samples: int) -&gt; np.ndarray:\n        \"\"\"\n        Samples variables based on a pre-calculated score.\n\n        Given a list of variable names deemed valid for a specific split point,\n        this method samples a subset of them without replacement.\n        - Numeric variables: The sampling is weighted proportionally to the score specified by\n        `self.sampling_score_to_use` (calculated in `setup_statistics`), making variables with higher scores\n        (e.g., lower NRMSE * num_samples) more likely to be chosen.\n        - Categorical variables: sampled uniformly (taking the mean score of numeric variables).\n\n        Args:\n            possible_variables: A list of variable names (event_name) eligible\n                for sampling at a particular split date.\n            num_samples: The desired number of variables to sample. The actual\n                number returned will be min(num_samples, len(possible_variables)).\n\n        Returns:\n            A numpy array of sampled variable names, or None if no variables could be\n            sampled (e.g., if `possible_variables` is empty or scores result in NaN probabilities).\n        \"\"\"\n\n        if self.variable_stats is None or self.sampling_strategy == \"uniform\":\n            if self.sampling_strategy == \"proportional\":\n                logging.warning(\"Variable statistics not set up, will fallback to uniform sampling.\")\n                logging.warning(\n                    \"To turn off this warning, either set up statistics or change sampling_strategy to 'uniform'.\"\n                )\n            selection = np.random.choice(\n                possible_variables,\n                size=min(num_samples, len(possible_variables)),\n                replace=False,\n            )\n            return selection\n\n        #: get all variables\n        curr_vars = self.variable_stats[self.config.event_name_col].isin(possible_variables)\n        all_variables = self.variable_stats[curr_vars]\n\n        if all_variables.shape[0] == 0:\n            return None\n\n        # Identify numeric vs categorical\n        if \"is_numeric\" in all_variables.columns:\n            numeric_mask = all_variables[\"is_numeric\"].astype(bool)\n        else:\n            logging.warning(\"no is_numeric column in variable_stats, assuming categorical\")\n            numeric_mask = pd.Series(False, index=all_variables.index)\n\n        numeric_variables = all_variables[numeric_mask]\n        categorical_variables = all_variables[~numeric_mask]\n\n        all_scores = np.zeros(len(all_variables), dtype=np.float64)\n\n        # numeric: get all scores\n        if not numeric_variables.empty:\n            num_scores = numeric_variables[self.sampling_score_to_use].values\n            num_scores = num_scores.astype(np.float64)\n            all_scores[numeric_mask.values] = num_scores\n\n        # categorical: uniform weights\n        if not categorical_variables.empty:\n            # Calculate mean score from numeric variables\n            cat_score = numeric_variables[self.sampling_score_to_use].mean()\n            # Fallback to 1.0 if no numeric variables\n            if pd.isna(cat_score):\n                cat_score = 1.0\n                logging.warning(\"No valid numeric scores found for categorical variables, using fallback of 1.0\")\n            all_scores[~numeric_mask.values] = cat_score\n            cat_vars = categorical_variables[self.config.event_name_col].tolist()\n            logging.info(\"Sampling categorical variables uniformly: \" + \", \".join(map(str, cat_vars)))\n\n        #: get all probabilities, using softmax for numerical stability\n        all_scores_exp = np.exp(-all_scores)\n        all_probs = all_scores_exp / all_scores_exp.sum()\n\n        # if any nans in probs, then return None\n        if np.isnan(all_probs).any():\n            return None\n\n        #: sample\n        var_choice = all_variables[self.config.event_name_col].tolist()\n        real_num_samples = min(num_samples, len(var_choice))\n        sampled_var = np.random.choice(var_choice, size=real_num_samples, replace=False, p=all_probs)\n\n        return sampled_var\n\n    def _filter_3_sigma(self, events: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filters or clips event values based on the 3-sigma rule.\n\n        For each unique event name in the input DataFrame, this method uses the\n        pre-calculated mean and standard deviation (from `self.variable_stats`)\n        for that event type. It then clips values lying outside the\n        [mean - 3*std, mean + 3*std] range to the boundary of this interval.\n        Rows where the value was originally NaN or becomes NaN after potential\n        numeric conversion issues are dropped.\n\n        Args:\n            events: DataFrame containing event data, including columns for\n                    event name and value. Expected to contain target data.\n\n        Returns:\n            A DataFrame with the event values clipped based on the 3-sigma rule,\n            and potentially fewer rows if NaNs were present or introduced.\n        \"\"\"\n\n        assert self.variable_stats is not None, (\n            \"Variable statistics must be set up before applying 3-sigma filtering (via setup_statistics()).\"\n            \"Alternatively, you can disable filtering in the call.\"\n        )\n\n        #: group by event name\n        events = events.copy()\n        grouped_events = events.groupby(self.config.event_name_col)\n\n        #: loop through every group\n        for event_name, event_data in grouped_events:\n            #: get the mean and std\n            stats = self.variable_stats[self.variable_stats[self.config.event_name_col] == event_name]\n            mean_val = stats[\"mean\"].values[0]\n            std_val = stats[\"std\"].values[0]\n\n            #: filter\n            event_data[self.config.event_value_col] = event_data[self.config.event_value_col].apply(\n                lambda x: x\n                if (x &gt; mean_val - 3 * std_val) and (x &lt; mean_val + 3 * std_val)\n                else np.clip(x, mean_val - 3 * std_val, mean_val + 3 * std_val)\n            )\n\n            #: update, convert to float the event value column\n            events[self.config.event_value_col] = events[self.config.event_value_col].astype(float)\n            events.loc[event_data.index, self.config.event_value_col] = event_data[self.config.event_value_col]\n\n        # Drop nan values in value column\n        events = events.dropna(subset=[self.config.event_value_col])\n\n        return events\n\n    def _get_all_possible_splits(\n        self,\n        patient_data_dic: dict,\n        min_nr_variable_seen_previously: int = 1,\n        min_nr_variable_seen_after: int = 1,\n        list_of_valid_categories: list = None,\n        subselect_random_within_lot: int = False,\n        max_num_splits_per_split_event: int = 1,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Identifies all potential (date, variable) pairs for forecasting tasks.\n\n        This method scans a patient's event data to find all combinations of\n        dates and variables that meet the criteria for forming a valid forecasting\n        split. A split is valid for a specific variable at a specific date if:\n        1. The date falls within the allowed range relative to a Line of Therapy (LoT) start.\n        2. The variable belongs to the `list_of_valid_categories`.\n        3. The variable has at least `min_nr_variable_seen_previously` occurrences\n           within the `max_lookback_for_value` period before the date.\n        4. The variable has at least `min_nr_variable_seen_after` occurrences\n           within the `max_forecast_for_value` period after the date.\n\n        If `subselect_random_within_lot` is True, it first identifies all potential\n        split dates per LoT using `_get_all_dates_within_range_of_split_event` and then\n        randomly selects up to `max_num_splits_per_split_event` dates from those associated\n        with each LoT before checking variable validity.\n\n        Parameters\n        ----------\n        patient_data_dic : dict\n            Dictionary containing the patient's dataframes (e.g., 'events', 'constant').\n        min_nr_variable_seen_previously : int\n            Minimum required past occurrences.\n        min_nr_variable_seen_after : int\n            Minimum required future occurrences.\n        list_of_valid_categories : list\n            Categories of variables to consider.\n        subselect_random_within_lot : bool\n            If True, randomly sample dates per LoT before checking variable validity.\n        max_num_splits_per_split_event : int\n            Max dates to sample per LoT if `subselect_random_within_lot` is True.\n\n        Returns\n        -------\n        tuple\n            A tuple containing two DataFrames:\n            1. `return_splits`: DataFrame listing all valid split combinations.\n               Columns include 'date' (split date), 'event_name' (variable),\n               'event_category', and 'lot_date' (associated LoT start date).\n               If no variables are valid for a sampled date, a row with None\n               values for date/event_name/category is added for that lot_date.\n            2. `all_possible_dates`: DataFrame containing the potential split dates\n               considered, along with their associated 'lot_date'. This reflects\n               the output of `_get_all_dates_within_range_of_split_event`, potentially\n               filtered by `select_random_splits`. Columns: 'date', 'lot_date'.\n        \"\"\"\n\n        #: setup data\n        events = patient_data_dic[\"events\"]\n\n        #: go over all possible events\n        all_events = events[[self.config.event_name_col, self.config.event_category_col]].copy()\n        all_events = all_events.drop_duplicates().values.tolist()\n        all_events.sort()\n\n        #: get all starting LoTs dates\n        all_possible_dates = self._get_all_dates_within_range_of_split_event(\n            patient_data_dic,\n            time_before_lot_start=pd.Timedelta(0),\n            max_split_length_after_split_event=self.max_split_length_after_split_event,\n        )\n\n        # If needed, select only those within an lot\n        if subselect_random_within_lot:\n            all_possible_dates = self.select_random_splits(\n                all_possible_dates, max_num_splits_per_split_event=max_num_splits_per_split_event\n            )\n\n        # Go over all dates and check all variables with which are eligible for a split\n        if list_of_valid_categories is not None:\n            events_category = events[events[self.config.event_category_col].isin(list_of_valid_categories)]\n            all_events = [(var, cat) for var, cat in all_events if cat in list_of_valid_categories]\n\n        # Pre-compute date ranges for lookback and forecast\n        lookback_range = self.max_lookback_time_for_value\n        forecast_range = self.max_forecast_time_for_value\n\n        # Initialize the return_splits list\n        return_splits = []\n\n        # Iterate over all possible dates\n        for row in all_possible_dates.itertuples(index=False):\n            curr_date, lot_date = row\n            num_added = 0\n\n            # Filter events within the lookback and forecast ranges\n            lookback_events = events_category[\n                (events_category[self.config.date_col] &lt;= curr_date)\n                &amp; (events_category[self.config.date_col] &gt;= curr_date - lookback_range)\n            ]\n            forecast_events = events_category[\n                (events_category[self.config.date_col] &gt; curr_date)\n                &amp; (events_category[self.config.date_col] &lt;= curr_date + forecast_range)\n            ]\n\n            # Iterate over all events\n            for curr_var, curr_event_category in all_events:\n                # Filter events by current variable\n                prev_events = lookback_events[lookback_events[self.config.event_name_col] == curr_var]\n                future_events = forecast_events[forecast_events[self.config.event_name_col] == curr_var]\n\n                # Count events\n                prev_events_count = prev_events.shape[0]\n                future_events_count = future_events.shape[0]\n\n                # Check conditions and add to return_splits if valid\n                if (\n                    prev_events_count &gt;= min_nr_variable_seen_previously\n                    and future_events_count &gt;= min_nr_variable_seen_after\n                ):\n                    return_splits.append(\n                        {\n                            \"date\": curr_date,\n                            \"event_name\": curr_var,\n                            \"event_category\": curr_event_category,\n                            \"lot_date\": lot_date,\n                        }\n                    )\n                    num_added += 1\n\n            #: if nothing has been added for current lot_date, then add a none event\n            if num_added == 0:\n                return_splits.append(\n                    {\n                        \"date\": None,\n                        \"event_name\": None,\n                        \"event_category\": None,\n                        \"lot_date\": lot_date,\n                    }\n                )\n\n        #: transform to pandas dataframe\n        return_splits = pd.DataFrame(return_splits)\n\n        #: drop duplicates\n        return_splits = self.drop_duplicates_except_na_for_date_col(return_splits)\n\n        #: return splits list\n        return return_splits, all_possible_dates\n\n    def _generate_variable_splits_for_date(\n        self,\n        curr_date,\n        nr_samples,\n        override_variables_to_predict,\n        events,\n        all_possible_split_dates,\n        apply_filtering,\n        override_split_dates,\n        patient_data,\n        lot_date,\n    ):\n        \"\"\"\n        Generates specific forecasting task samples for a given split date.\n\n        For a single potential split date (`curr_date`), this method creates\n        `nr_samples` forecasting tasks. Each task involves:\n        1. Determining the set of variables valid for forecasting at `curr_date`\n           (based on `all_possible_split_dates`).\n        2. Sampling a subset of these variables (between `min_nr_variables_to_sample`\n           and `max_nr_variables_to_sample`), weighted by their score, unless\n           `override_variables_to_predict` is provided.\n        3. Creating the actual data split: 'events_until_split' (history) and\n           'target_events_after_split' (future values of sampled variables within\n           the forecast window, ensuring no overlap with the next LoT).\n        4. Optionally applying filtering (e.g., 3-sigma) to the target values.\n        5. Bundling the split data along with metadata ('constant_data',\n           'split_date_included_in_input', 'sampled_variables', 'lot_date').\n\n        It also updates `all_possible_split_dates` by removing the variable/date\n        combinations used in the generated samples to avoid reuse.\n\n        Parameters\n        ----------\n        curr_date : datetime or None\n            The specific date to generate splits for. Can be None.\n        nr_samples : int\n            The number of variable sets to sample for this date.\n        override_variables_to_predict : list, optional\n            If provided, forces these variables to be used instead of sampling (checks if they are valid first).\n        events : pd.DataFrame\n            The full event history for the patient.\n        all_possible_split_dates : pd.DataFrame\n            DataFrame mapping valid dates to valid variables.\n        apply_filtering : bool\n            Whether to filter target event values.\n        override_split_dates : list, optional\n            List of externally provided split dates (used to check if filtering should be skipped even if target is\n            empty).\n        patient_data : dict\n            Dictionary containing patient's 'events' and 'constant' data.\n        lot_date : datetime\n            The Line of Therapy start date associated with `curr_date`.\n\n        Returns\n        -------\n        tuple\n            A tuple containing:\n            - `date_splits`: A list of dictionaries, each dictionary is a complete\n              forecasting sample {'events_until_split', 'target_events_after_split', ...}.\n              Empty if no valid samples could be generated for this date.\n            - `valid_sample_date`: Boolean, True if `curr_date` was not None or NaN.\n            - `date_splits_meta`: A single-row DataFrame containing the `curr_date`\n              and `lot_date` used for this attempt.\n            - `all_possible_split_dates`: The input DataFrame, updated to remove\n              the date/variable combinations that were successfully used.\n        \"\"\"\n\n        # Get current date -&gt; can be multiple dates per lot\n        possible_variables = all_possible_split_dates[all_possible_split_dates[self.config.date_col] == curr_date]\n        possible_variables = possible_variables[self.config.event_name_col].tolist()\n        date_splits = DataSplitterForecastingGroup()\n        valid_sample_date = False\n\n        if curr_date is None or pd.isna(curr_date):\n            # Generate empty meta and return\n            date_splits_meta = [{self.config.date_col: curr_date, self.config.split_date_col: lot_date}]\n            date_splits_meta = pd.DataFrame(date_splits_meta)\n\n            return (\n                date_splits,\n                valid_sample_date,\n                date_splits_meta,\n                all_possible_split_dates,\n            )\n\n        # Note that generally valid date\n        valid_sample_date = True\n\n        # Try generating samples\n        for _ in range(nr_samples):\n            #: uniformly sample nr of variables to sample in\n            # range(min_nr_variables_to_sample, max_nr_variables_to_sample)\n            max_nr_variables_to_sample = min(len(possible_variables), self.max_nr_variables_to_sample)\n            min_nr_variables_to_sample = min(len(possible_variables), self.min_nr_variables_to_sample)\n            if max_nr_variables_to_sample &gt; min_nr_variables_to_sample:\n                nr_variables_to_sample = np.random.randint(min_nr_variables_to_sample, max_nr_variables_to_sample)\n            else:\n                nr_variables_to_sample = min_nr_variables_to_sample\n\n            # If we have less variables than the minimum, we skip this sample\n            if nr_variables_to_sample == 0:\n                continue\n\n            #: sample which variables via _sample_proportionally or manual override\n            if override_variables_to_predict is None:\n                sampled_variables = self._sample_proportionally(possible_variables, nr_variables_to_sample)\n            else:\n                # Skip if not all override variables are in possible_variables\n                # instead of skipping the sample, we just skip the invalid variables\n                if not all([var in possible_variables for var in override_variables_to_predict]):\n                    original_override = override_variables_to_predict.copy()\n                    override_variables_to_predict = [\n                        var for var in override_variables_to_predict if var in possible_variables\n                    ]\n                    problematic_vars = set(original_override) - set(override_variables_to_predict)\n                    logging.warning(\n                        f\"Not all override_variables_to_predict are valid at date {curr_date}. \"\n                        f\"Skipping invalid variables: {problematic_vars}\"\n                    )\n                    # continue\n                sampled_variables = override_variables_to_predict\n\n            #: if no variables sampled, skip\n            if sampled_variables is None:\n                continue\n\n            #: remove only sampled variables at current date from all_possible_split_dates\n            rows_to_remove = (all_possible_split_dates[self.config.date_col] == curr_date) &amp; all_possible_split_dates[\n                self.config.event_name_col\n            ].isin(sampled_variables)\n            all_possible_split_dates = all_possible_split_dates[~rows_to_remove]\n\n            #: get the splits for the given patient data\n            events_before_split = events[events[self.config.date_col] &lt;= curr_date]\n            events_after_split = events[events[self.config.date_col] &gt; curr_date]\n            events_after_split = events_after_split[\n                events_after_split[self.config.date_col] &lt;= curr_date + self.max_forecast_time_for_value\n            ]\n            events_after_split = events_after_split[\n                events_after_split[self.config.event_name_col].isin(sampled_variables)\n            ]\n\n            #: filter so that we do not overlap with next LoT, since that will invalidate the results\n            lots = events[events[self.config.event_category_col] == self.config.event_category_lot]\n            lots = lots[lots[self.config.date_col] &gt; curr_date]\n            lots = lots.sort_values(self.config.date_col)\n            if lots.shape[0] &gt; 0 and not self.config.skip_future_lot_filtering:\n                date_of_next_lot = lots[self.config.date_col].iloc[0]\n                events_after_split = events_after_split[events_after_split[self.config.date_col] &lt; date_of_next_lot]\n\n            #: if apply_filtering, apply 3-sigma filtering (only to target) and drop any bad rows\n            if apply_filtering:\n                events_after_split[self.config.event_value_col] = pd.to_numeric(\n                    events_after_split[self.config.event_value_col], errors=\"coerce\"\n                )\n                events_after_split = self._filtering_methods[self.filtering_strategy](events_after_split)\n\n            #: check if still valid samples (i.e. values are not nan in output),\n            # but only if no override (e.g. in inference)\n            if events_after_split.shape[0] == 0 and override_split_dates is None:\n                continue\n\n            #: save to a list\n            new_option = DataSplitterForecastingOption(\n                events_until_split=events_before_split,\n                target_events_after_split=events_after_split,\n                constant_data=patient_data[\"constant\"].copy(),\n                split_date_included_in_input=curr_date,\n                sampled_variables=sampled_variables,\n                lot_date=lot_date,\n            )\n            date_splits.append(new_option)\n\n        # Turn into 1 row dataframe\n        date_splits_meta = [{self.config.date_col: curr_date, self.config.split_date_col: lot_date}]\n        date_splits_meta = pd.DataFrame(date_splits_meta)\n\n        return (\n            date_splits,\n            valid_sample_date,\n            date_splits_meta,\n            all_possible_split_dates,\n        )\n\n    def get_splits_from_patient(\n        self,\n        patient_data: dict,\n        nr_samples_per_split: int,\n        max_num_splits_per_split_event: int = 1,\n        include_metadata: bool = False,\n        filter_outliers: bool = True,\n        override_categories_to_predict: list[str] = None,\n        override_variables_to_predict: list[str] = None,\n        override_split_dates: list[datetime] = None,\n    ) -&gt; list[DataSplitterForecastingGroup]:\n        \"\"\"\n        Generates multiple forecasting splits for a patient.\n\n        This is the main method for creating forecasting tasks for a single patient.\n        It first identifies potential split dates, typically by randomly selecting\n        up to `max_num_splits_per_split_event` valid dates associated with each Line of\n        Therapy (LoT) using `_get_all_possible_splits`. Alternatively, specific\n        dates can be provided via `override_split_dates`.\n\n        For each selected date, it calls `_generate_variable_splits_for_date` to\n        generate `nr_samples_per_split` distinct forecasting tasks by sampling different\n        sets of variables to predict (unless `override_variables_to_predict` is set).\n        Filtering of target values can be applied.\n\n        Parameters\n        ----------\n        patient_data : dict\n            Dictionary containing the patient's 'events' and 'constant' data.\n        nr_samples_per_split : int\n            The number of variable sets to sample per selected split date.\n        max_num_splits_per_split_event : int, optional\n            The maximum number of dates to randomly sample per LoT when `override_split_dates` is None. Defaults to 1.\n        include_metadata : bool, optional\n            If True, returns both the generated splits and a DataFrame of the split dates used. Defaults to False.\n            Useful for alignment with other splitters, such as DataSplitterEvents.\n        filter_outliers : bool, optional\n            If True, applies filtering (e.g., 3-sigma) to the target event values.\n        override_categories_to_predict : list[str], optional\n            If provided, forces prediction of all variables present in these categories for all generated splits,\n            bypassing proportional sampling.\n        override_variables_to_predict : list[str], optional\n            If provided, forces prediction of these specific variables for all generated splits, bypassing\n            proportional sampling. Requires `override_split_dates` to also be set for typical use cases (like\n            inference).\n        override_split_dates : list[datetime], optional\n            If provided, uses these specific dates instead of discovering and sampling dates based on LoTs\n            (useful for inference scenarios).\n\n        Returns\n        -------\n        list[DataSplitterForecastingGroup] or tuple\n            If `include_metadata` is False:\n                A list of `DataSplitterForecastingGroup` objects. Each group corresponds to one selected split date\n                (one per LoT typically) and contains `nr_samples_per_split` `DataSplitterForecastingOption` items,\n                where each dictionary represents a full forecasting task sample\n                (e.g., {'events_until_split': df, 'target_events_after_split': df, ...}).\n                Returns `[[None]]` if no valid splits are found.\n            If `include_metadata` is True:\n                A tuple: (`ret_splits`, `all_possible_split_dates_return`)\n                - `ret_splits`: The list of lists of split dictionaries as described above.\n                - `all_possible_split_dates_return`: A DataFrame containing the actual split\n                  dates and their associated LoT dates that were successfully used to\n                  generate the samples in `ret_splits`. Columns: ['date', 'lot_date'].\n        \"\"\"\n\n        # Setup basics\n        events = patient_data[\"events\"]\n\n        # Do some quick sanity checks\n        if self.config.warning_for_splitters_patient_without_splits:\n            lot_events = events[events[self.config.event_category_col] == self.config.event_category_lot]\n            if lot_events.shape[0] == 0:\n                logging.warning(\n                    \"Patient \"\n                    + str(patient_data[\"constant\"][self.config.patient_id_col].iloc[0])\n                    + \" has no split events. Forecasting splits may be invalid.\"\n                    \"To disable this warning, set warning_for_splitters_patient_without_splits to False in config.\"\n                )\n\n        if override_categories_to_predict is not None:\n            logging.info(\n                \"Including all variables from categories: \" + \", \".join(map(str, override_categories_to_predict))\n            )\n            category_mask = events[self.config.event_category_col].isin(override_categories_to_predict)\n            variables_from_categories = events[category_mask][self.config.event_name_col].unique().tolist()\n            if override_variables_to_predict is None:\n                override_variables_to_predict = variables_from_categories\n            else:\n                # Combine the lists and ensure uniqueness\n                combined_vars = override_variables_to_predict + variables_from_categories\n                override_variables_to_predict = list(dict.fromkeys(combined_vars))\n\n        if override_split_dates is None:\n            #: get all possible splits via _get_all_possible_splits, randomly selecting one split date per LoT\n            all_possible_split_dates, all_possible_split_dates_no_vars = self._get_all_possible_splits(\n                patient_data,\n                min_nr_variable_seen_previously=self.min_nr_variable_seen_previously,\n                min_nr_variable_seen_after=self.min_nr_variable_seen_after,\n                list_of_valid_categories=self.list_of_valid_categories,\n                subselect_random_within_lot=True,\n                max_num_splits_per_split_event=max_num_splits_per_split_event,\n            )\n\n            if all_possible_split_dates.shape[0] == 0:\n                logging.info(\n                    \"No possible forecasting splits found for patient: \"\n                    + str(patient_data[\"constant\"][self.config.patient_id_col].iloc[0])\n                )\n                ret = [None], None if include_metadata else None\n                return ret\n\n        else:\n            assert override_variables_to_predict is not None, (\n                \"If you override split dates, you must also override variables to predict\"\n            )\n\n            #: create all_possible_split_dates, with override_split_dates for date and nr of rows\n            #: then for each row, we add: None for LoT date, and override_variables_to_predict for variables\n\n            all_possible_split_dates = []\n            for split_date in override_split_dates:\n                for variable_to_predict in override_variables_to_predict:\n                    all_possible_split_dates.append(\n                        {\n                            self.config.date_col: split_date,\n                            self.config.event_name_col: variable_to_predict,\n                            self.config.split_date_col: \"override\",\n                        }\n                    )\n            all_possible_split_dates = pd.DataFrame(all_possible_split_dates)\n            all_possible_split_dates_no_vars = all_possible_split_dates.copy()\n            all_possible_split_dates_no_vars = all_possible_split_dates_no_vars[\n                [self.config.date_col, self.config.split_date_col]\n            ].drop_duplicates()\n\n        #: loop through 1 to nr_samples_per_split\n        all_lots_dates = all_possible_split_dates_no_vars[[self.config.date_col, self.config.split_date_col]]\n\n        ret_splits = []\n        ret_split_dates = []\n\n        for lot_date in all_lots_dates[self.config.split_date_col].unique():\n            all_dates_in_lot = all_lots_dates[all_lots_dates[self.config.split_date_col] == lot_date][\n                self.config.date_col\n            ]\n\n            for curr_date in all_dates_in_lot:\n                date_splits = []\n\n                # Try generating date splits for current date\n                (date_splits, valid_date, date_split_meta, all_possible_split_dates) = (\n                    self._generate_variable_splits_for_date(\n                        curr_date,\n                        nr_samples_per_split,\n                        override_variables_to_predict,\n                        events,\n                        all_possible_split_dates,\n                        filter_outliers,\n                        override_split_dates,\n                        patient_data,\n                        lot_date,\n                    )\n                )\n\n                # In case we didn't add any splits, due to issues with the timeline (and not invalid date),\n                # then try with another date in the current lot\n                # A bit hacky, and slow, but should work in case there is an option\n                if len(date_splits) == 0 and valid_date:\n                    # Try earlier dates (since often those have more success due to future LoTs blocking)\n                    other_dates_in_lot = (\n                        pd.Series(all_dates_in_lot[all_dates_in_lot != curr_date]).sort_values().unique()\n                    )\n\n                    for other_date in other_dates_in_lot:\n                        # Generate data from another date\n                        (date_splits, valid_date, date_split_meta, all_possible_split_dates) = (\n                            self._generate_variable_splits_for_date(\n                                other_date,\n                                nr_samples_per_split,\n                                override_variables_to_predict,\n                                events,\n                                all_possible_split_dates,\n                                filter_outliers,\n                                override_split_dates,\n                                patient_data,\n                                lot_date,\n                            )\n                        )\n\n                        if len(date_splits) &gt; 0:\n                            break\n\n                    # If nothing found, then append empty list and meta\n                    if len(date_splits) == 0:\n                        date_splits = DataSplitterForecastingGroup()\n                        date_split_meta = [\n                            {\n                                self.config.date_col: curr_date,\n                                self.config.split_date_col: lot_date,\n                            }\n                        ]\n                        date_split_meta = pd.DataFrame(date_split_meta)\n\n                #: append to return_splits, so to randomly subselect which variables to use\n                ret_splits.append(date_splits)\n                ret_split_dates.append(date_split_meta)\n\n        #: return list\n        if include_metadata:\n            #: get all possible split dates from what was actually used\n            all_possible_split_dates_return = pd.concat(ret_split_dates, axis=0, ignore_index=True)\n\n            # Return\n            return ret_splits, all_possible_split_dates_return\n        else:\n            return ret_splits\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting.DataSplitterForecasting-functions","title":"Functions","text":""},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting.DataSplitterForecasting.__init__","title":"__init__","text":"<pre><code>__init__(\n    config,\n    data_manager,\n    max_split_length_after_split_event=pd.Timedelta(\n        days=90\n    ),\n    max_lookback_time_for_value=pd.Timedelta(days=90),\n    max_forecast_time_for_value=pd.Timedelta(days=90),\n    min_num_samples_for_statistics=10,\n    sampling_score_to_use=\"score_log_nrmse_n_samples\",\n    min_nr_variable_seen_previously=1,\n    min_nr_variable_seen_after=1,\n    list_of_valid_categories=None,\n    save_path_for_variable_stats=None,\n    min_nr_variables_to_sample=1,\n    max_nr_variables_to_sample=3,\n    filtering_strategy=\"3-sigma\",\n    sampling_strategy=\"proportional\",\n)\n</code></pre> <p>Initializes the DataSplitterForecasting instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object containing shared settings like column names.</p> required <code>data_manager</code> <code>DataManager</code> <p>Provides access to patient data for a single indication.</p> required <code>max_split_length_after_split_event</code> <code>Timedelta</code> <p>Max days after LoT start to consider for split dates. Defaults to 90.</p> <code>Timedelta(days=90)</code> <code>max_lookback_time_for_value</code> <code>Timedelta</code> <p>Max days before a split date to look for past variable occurrences. Defaults to 90.</p> <code>Timedelta(days=90)</code> <code>max_forecast_time_for_value</code> <code>Timedelta</code> <p>Max days after a split date to look for future variable occurrences (target data). Defaults to 90.</p> <code>Timedelta(days=90)</code> <code>min_num_samples_for_statistics</code> <code>int</code> <p>Minimum total occurrences of a variable across the training set needed to calculate statistics. Defaults to 50.</p> <code>10</code> <code>sampling_score_to_use</code> <code>str</code> <p>Column name in the computed statistics table used for weighted sampling of variables. Defaults to 'score_log_nrmse_n_samples'.</p> <code>'score_log_nrmse_n_samples'</code> <code>min_nr_variable_seen_previously</code> <code>int</code> <p>Min occurrences of a variable required in the lookback window for a split to be valid for that variable. Defaults to 1.</p> <code>1</code> <code>min_nr_variable_seen_after</code> <code>int</code> <p>Min occurrences of a variable required in the forecast window for a split to be valid for that variable. Defaults to 1.</p> <code>1</code> <code>list_of_valid_categories</code> <code>list</code> <p>List of event categories to consider for forecasting (e.g., ['LABS']). Defaults to <code>config.event_category_forecast</code>.</p> <code>None</code> <code>save_path_for_variable_stats</code> <code>str</code> <p>Optional file path to save the calculated variable statistics CSV. Defaults to None.</p> <code>None</code> <code>min_nr_variables_to_sample</code> <code>int</code> <p>The minimum number of distinct variables to attempt to sample for each forecasting task. Defaults to 3.</p> <code>1</code> <code>max_nr_variables_to_sample</code> <code>int</code> <p>The maximum number of distinct variables to attempt to sample for each forecasting task. Defaults to 3.</p> <code>3</code> <code>filtering_strategy</code> <code>str</code> <p>The strategy for handling outliers in target variable values ('3-sigma'). Defaults to '3-sigma'.</p> <code>'3-sigma'</code> <code>sampling_strategy</code> <code>str</code> <p>The strategy for sampling variables ('proportional' or 'uniform'). Defaults to 'proportional'.</p> <code>'proportional'</code> Source code in <code>twinweaver/instruction/data_splitter_forecasting.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n    data_manager: DataManager,\n    max_split_length_after_split_event: pd.Timedelta = pd.Timedelta(days=90),\n    max_lookback_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n    max_forecast_time_for_value: pd.Timedelta = pd.Timedelta(days=90),\n    min_num_samples_for_statistics: int = 10,\n    sampling_score_to_use: str = \"score_log_nrmse_n_samples\",\n    min_nr_variable_seen_previously: int = 1,\n    min_nr_variable_seen_after: int = 1,\n    list_of_valid_categories: list = None,\n    save_path_for_variable_stats: str = None,\n    min_nr_variables_to_sample: int = 1,\n    max_nr_variables_to_sample: int = 3,\n    filtering_strategy: str = \"3-sigma\",\n    sampling_strategy: str = \"proportional\",\n):\n    \"\"\"\n    Initializes the DataSplitterForecasting instance.\n\n    Parameters\n    ----------\n    config : Config\n        Configuration object containing shared settings like column names.\n    data_manager : DataManager\n        Provides access to patient data for a single indication.\n    max_split_length_after_split_event : pd.Timedelta\n        Max days after LoT start to consider for split dates. Defaults to 90.\n    max_lookback_time_for_value : pd.Timedelta\n        Max days before a split date to look for past variable occurrences.\n        Defaults to 90.\n    max_forecast_time_for_value : pd.Timedelta\n        Max days after a split date to look for future variable occurrences (target\n        data). Defaults to 90.\n    min_num_samples_for_statistics : int\n        Minimum total occurrences of a variable across the training set\n        needed to calculate statistics. Defaults to 50.\n    sampling_score_to_use : str\n        Column name in the computed statistics table used for weighted sampling of variables.\n        Defaults to 'score_log_nrmse_n_samples'.\n    min_nr_variable_seen_previously : int\n        Min occurrences of a variable required in the lookback window for a split\n        to be valid for that variable. Defaults to 1.\n    min_nr_variable_seen_after : int\n        Min occurrences of a variable required in the forecast window for a split to be\n        valid for that variable. Defaults to 1.\n    list_of_valid_categories : list\n        List of event categories to consider for forecasting (e.g., ['LABS']). Defaults\n        to `config.event_category_forecast`.\n    save_path_for_variable_stats : str, optional\n        Optional file path to save the calculated variable statistics CSV. Defaults to\n        None.\n    min_nr_variables_to_sample : int\n        The minimum number of distinct variables to attempt to sample for each\n        forecasting task. Defaults to 3.\n    max_nr_variables_to_sample : int\n        The maximum number of distinct variables to attempt to sample for each\n        forecasting task. Defaults to 3.\n    filtering_strategy : str\n        The strategy for handling outliers in target variable values ('3-sigma').\n        Defaults to '3-sigma'.\n    sampling_strategy : str\n        The strategy for sampling variables ('proportional' or 'uniform').\n        Defaults to 'proportional'.\n    \"\"\"\n    super().__init__(\n        data_manager,\n        config,\n        max_split_length_after_split_event,\n        max_lookback_time_for_value,\n        max_forecast_time_for_value,\n    )\n\n    assert self.config.event_category_forecast is not None or list_of_valid_categories is not None, (\n        \"event_category_forecast must be set in Config for DataSplitterForecasting.\"\n        \"For example: ['lab']\"\n        \" Alternatively, provide list_of_valid_categories directly.\"\n    )\n\n    self.variable_stats = None\n    self.variable_type = {}  # event_name -&gt; \"numeric\" / \"categorical\"\n    self.min_num_samples_for_statistics = min_num_samples_for_statistics\n    self.sampling_score_to_use = sampling_score_to_use\n\n    self.min_nr_variable_seen_previously = min_nr_variable_seen_previously\n    self.min_nr_variable_seen_after = min_nr_variable_seen_after\n    self.list_of_valid_categories = (\n        list_of_valid_categories if list_of_valid_categories is not None else self.config.event_category_forecast\n    )\n    self.save_path_for_variable_stats = save_path_for_variable_stats\n    self.min_nr_variables_to_sample = min_nr_variables_to_sample\n    self.max_nr_variables_to_sample = max_nr_variables_to_sample\n    self.filtering_strategy = filtering_strategy\n    self.sampling_strategy = sampling_strategy\n\n    self._filtering_methods = {\"3-sigma\": self._filter_3_sigma}\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting.DataSplitterForecasting.get_splits_from_patient","title":"get_splits_from_patient","text":"<pre><code>get_splits_from_patient(\n    patient_data,\n    nr_samples_per_split,\n    max_num_splits_per_split_event=1,\n    include_metadata=False,\n    filter_outliers=True,\n    override_categories_to_predict=None,\n    override_variables_to_predict=None,\n    override_split_dates=None,\n)\n</code></pre> <p>Generates multiple forecasting splits for a patient.</p> <p>This is the main method for creating forecasting tasks for a single patient. It first identifies potential split dates, typically by randomly selecting up to <code>max_num_splits_per_split_event</code> valid dates associated with each Line of Therapy (LoT) using <code>_get_all_possible_splits</code>. Alternatively, specific dates can be provided via <code>override_split_dates</code>.</p> <p>For each selected date, it calls <code>_generate_variable_splits_for_date</code> to generate <code>nr_samples_per_split</code> distinct forecasting tasks by sampling different sets of variables to predict (unless <code>override_variables_to_predict</code> is set). Filtering of target values can be applied.</p> <p>Parameters:</p> Name Type Description Default <code>patient_data</code> <code>dict</code> <p>Dictionary containing the patient's 'events' and 'constant' data.</p> required <code>nr_samples_per_split</code> <code>int</code> <p>The number of variable sets to sample per selected split date.</p> required <code>max_num_splits_per_split_event</code> <code>int</code> <p>The maximum number of dates to randomly sample per LoT when <code>override_split_dates</code> is None. Defaults to 1.</p> <code>1</code> <code>include_metadata</code> <code>bool</code> <p>If True, returns both the generated splits and a DataFrame of the split dates used. Defaults to False. Useful for alignment with other splitters, such as DataSplitterEvents.</p> <code>False</code> <code>filter_outliers</code> <code>bool</code> <p>If True, applies filtering (e.g., 3-sigma) to the target event values.</p> <code>True</code> <code>override_categories_to_predict</code> <code>list[str]</code> <p>If provided, forces prediction of all variables present in these categories for all generated splits, bypassing proportional sampling.</p> <code>None</code> <code>override_variables_to_predict</code> <code>list[str]</code> <p>If provided, forces prediction of these specific variables for all generated splits, bypassing proportional sampling. Requires <code>override_split_dates</code> to also be set for typical use cases (like inference).</p> <code>None</code> <code>override_split_dates</code> <code>list[datetime]</code> <p>If provided, uses these specific dates instead of discovering and sampling dates based on LoTs (useful for inference scenarios).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[DataSplitterForecastingGroup] or tuple</code> <p>If <code>include_metadata</code> is False:     A list of <code>DataSplitterForecastingGroup</code> objects. Each group corresponds to one selected split date     (one per LoT typically) and contains <code>nr_samples_per_split</code> <code>DataSplitterForecastingOption</code> items,     where each dictionary represents a full forecasting task sample     (e.g., {'events_until_split': df, 'target_events_after_split': df, ...}).     Returns <code>[[None]]</code> if no valid splits are found. If <code>include_metadata</code> is True:     A tuple: (<code>ret_splits</code>, <code>all_possible_split_dates_return</code>)     - <code>ret_splits</code>: The list of lists of split dictionaries as described above.     - <code>all_possible_split_dates_return</code>: A DataFrame containing the actual split       dates and their associated LoT dates that were successfully used to       generate the samples in <code>ret_splits</code>. Columns: ['date', 'lot_date'].</p> Source code in <code>twinweaver/instruction/data_splitter_forecasting.py</code> <pre><code>def get_splits_from_patient(\n    self,\n    patient_data: dict,\n    nr_samples_per_split: int,\n    max_num_splits_per_split_event: int = 1,\n    include_metadata: bool = False,\n    filter_outliers: bool = True,\n    override_categories_to_predict: list[str] = None,\n    override_variables_to_predict: list[str] = None,\n    override_split_dates: list[datetime] = None,\n) -&gt; list[DataSplitterForecastingGroup]:\n    \"\"\"\n    Generates multiple forecasting splits for a patient.\n\n    This is the main method for creating forecasting tasks for a single patient.\n    It first identifies potential split dates, typically by randomly selecting\n    up to `max_num_splits_per_split_event` valid dates associated with each Line of\n    Therapy (LoT) using `_get_all_possible_splits`. Alternatively, specific\n    dates can be provided via `override_split_dates`.\n\n    For each selected date, it calls `_generate_variable_splits_for_date` to\n    generate `nr_samples_per_split` distinct forecasting tasks by sampling different\n    sets of variables to predict (unless `override_variables_to_predict` is set).\n    Filtering of target values can be applied.\n\n    Parameters\n    ----------\n    patient_data : dict\n        Dictionary containing the patient's 'events' and 'constant' data.\n    nr_samples_per_split : int\n        The number of variable sets to sample per selected split date.\n    max_num_splits_per_split_event : int, optional\n        The maximum number of dates to randomly sample per LoT when `override_split_dates` is None. Defaults to 1.\n    include_metadata : bool, optional\n        If True, returns both the generated splits and a DataFrame of the split dates used. Defaults to False.\n        Useful for alignment with other splitters, such as DataSplitterEvents.\n    filter_outliers : bool, optional\n        If True, applies filtering (e.g., 3-sigma) to the target event values.\n    override_categories_to_predict : list[str], optional\n        If provided, forces prediction of all variables present in these categories for all generated splits,\n        bypassing proportional sampling.\n    override_variables_to_predict : list[str], optional\n        If provided, forces prediction of these specific variables for all generated splits, bypassing\n        proportional sampling. Requires `override_split_dates` to also be set for typical use cases (like\n        inference).\n    override_split_dates : list[datetime], optional\n        If provided, uses these specific dates instead of discovering and sampling dates based on LoTs\n        (useful for inference scenarios).\n\n    Returns\n    -------\n    list[DataSplitterForecastingGroup] or tuple\n        If `include_metadata` is False:\n            A list of `DataSplitterForecastingGroup` objects. Each group corresponds to one selected split date\n            (one per LoT typically) and contains `nr_samples_per_split` `DataSplitterForecastingOption` items,\n            where each dictionary represents a full forecasting task sample\n            (e.g., {'events_until_split': df, 'target_events_after_split': df, ...}).\n            Returns `[[None]]` if no valid splits are found.\n        If `include_metadata` is True:\n            A tuple: (`ret_splits`, `all_possible_split_dates_return`)\n            - `ret_splits`: The list of lists of split dictionaries as described above.\n            - `all_possible_split_dates_return`: A DataFrame containing the actual split\n              dates and their associated LoT dates that were successfully used to\n              generate the samples in `ret_splits`. Columns: ['date', 'lot_date'].\n    \"\"\"\n\n    # Setup basics\n    events = patient_data[\"events\"]\n\n    # Do some quick sanity checks\n    if self.config.warning_for_splitters_patient_without_splits:\n        lot_events = events[events[self.config.event_category_col] == self.config.event_category_lot]\n        if lot_events.shape[0] == 0:\n            logging.warning(\n                \"Patient \"\n                + str(patient_data[\"constant\"][self.config.patient_id_col].iloc[0])\n                + \" has no split events. Forecasting splits may be invalid.\"\n                \"To disable this warning, set warning_for_splitters_patient_without_splits to False in config.\"\n            )\n\n    if override_categories_to_predict is not None:\n        logging.info(\n            \"Including all variables from categories: \" + \", \".join(map(str, override_categories_to_predict))\n        )\n        category_mask = events[self.config.event_category_col].isin(override_categories_to_predict)\n        variables_from_categories = events[category_mask][self.config.event_name_col].unique().tolist()\n        if override_variables_to_predict is None:\n            override_variables_to_predict = variables_from_categories\n        else:\n            # Combine the lists and ensure uniqueness\n            combined_vars = override_variables_to_predict + variables_from_categories\n            override_variables_to_predict = list(dict.fromkeys(combined_vars))\n\n    if override_split_dates is None:\n        #: get all possible splits via _get_all_possible_splits, randomly selecting one split date per LoT\n        all_possible_split_dates, all_possible_split_dates_no_vars = self._get_all_possible_splits(\n            patient_data,\n            min_nr_variable_seen_previously=self.min_nr_variable_seen_previously,\n            min_nr_variable_seen_after=self.min_nr_variable_seen_after,\n            list_of_valid_categories=self.list_of_valid_categories,\n            subselect_random_within_lot=True,\n            max_num_splits_per_split_event=max_num_splits_per_split_event,\n        )\n\n        if all_possible_split_dates.shape[0] == 0:\n            logging.info(\n                \"No possible forecasting splits found for patient: \"\n                + str(patient_data[\"constant\"][self.config.patient_id_col].iloc[0])\n            )\n            ret = [None], None if include_metadata else None\n            return ret\n\n    else:\n        assert override_variables_to_predict is not None, (\n            \"If you override split dates, you must also override variables to predict\"\n        )\n\n        #: create all_possible_split_dates, with override_split_dates for date and nr of rows\n        #: then for each row, we add: None for LoT date, and override_variables_to_predict for variables\n\n        all_possible_split_dates = []\n        for split_date in override_split_dates:\n            for variable_to_predict in override_variables_to_predict:\n                all_possible_split_dates.append(\n                    {\n                        self.config.date_col: split_date,\n                        self.config.event_name_col: variable_to_predict,\n                        self.config.split_date_col: \"override\",\n                    }\n                )\n        all_possible_split_dates = pd.DataFrame(all_possible_split_dates)\n        all_possible_split_dates_no_vars = all_possible_split_dates.copy()\n        all_possible_split_dates_no_vars = all_possible_split_dates_no_vars[\n            [self.config.date_col, self.config.split_date_col]\n        ].drop_duplicates()\n\n    #: loop through 1 to nr_samples_per_split\n    all_lots_dates = all_possible_split_dates_no_vars[[self.config.date_col, self.config.split_date_col]]\n\n    ret_splits = []\n    ret_split_dates = []\n\n    for lot_date in all_lots_dates[self.config.split_date_col].unique():\n        all_dates_in_lot = all_lots_dates[all_lots_dates[self.config.split_date_col] == lot_date][\n            self.config.date_col\n        ]\n\n        for curr_date in all_dates_in_lot:\n            date_splits = []\n\n            # Try generating date splits for current date\n            (date_splits, valid_date, date_split_meta, all_possible_split_dates) = (\n                self._generate_variable_splits_for_date(\n                    curr_date,\n                    nr_samples_per_split,\n                    override_variables_to_predict,\n                    events,\n                    all_possible_split_dates,\n                    filter_outliers,\n                    override_split_dates,\n                    patient_data,\n                    lot_date,\n                )\n            )\n\n            # In case we didn't add any splits, due to issues with the timeline (and not invalid date),\n            # then try with another date in the current lot\n            # A bit hacky, and slow, but should work in case there is an option\n            if len(date_splits) == 0 and valid_date:\n                # Try earlier dates (since often those have more success due to future LoTs blocking)\n                other_dates_in_lot = (\n                    pd.Series(all_dates_in_lot[all_dates_in_lot != curr_date]).sort_values().unique()\n                )\n\n                for other_date in other_dates_in_lot:\n                    # Generate data from another date\n                    (date_splits, valid_date, date_split_meta, all_possible_split_dates) = (\n                        self._generate_variable_splits_for_date(\n                            other_date,\n                            nr_samples_per_split,\n                            override_variables_to_predict,\n                            events,\n                            all_possible_split_dates,\n                            filter_outliers,\n                            override_split_dates,\n                            patient_data,\n                            lot_date,\n                        )\n                    )\n\n                    if len(date_splits) &gt; 0:\n                        break\n\n                # If nothing found, then append empty list and meta\n                if len(date_splits) == 0:\n                    date_splits = DataSplitterForecastingGroup()\n                    date_split_meta = [\n                        {\n                            self.config.date_col: curr_date,\n                            self.config.split_date_col: lot_date,\n                        }\n                    ]\n                    date_split_meta = pd.DataFrame(date_split_meta)\n\n            #: append to return_splits, so to randomly subselect which variables to use\n            ret_splits.append(date_splits)\n            ret_split_dates.append(date_split_meta)\n\n    #: return list\n    if include_metadata:\n        #: get all possible split dates from what was actually used\n        all_possible_split_dates_return = pd.concat(ret_split_dates, axis=0, ignore_index=True)\n\n        # Return\n        return ret_splits, all_possible_split_dates_return\n    else:\n        return ret_splits\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting.DataSplitterForecasting.setup_statistics","title":"setup_statistics","text":"<pre><code>setup_statistics(train_patientids=None)\n</code></pre> <p>Calculates baseline performance statistics for variables.</p> <p>Iterates through all patients in the training set and potential split dates within specified ranges around Lines of Therapy (LoTs). For each numeric variable (typically labs), it calculates metrics like R\u00b2, NRMSE, MAPE, mean, standard deviation, etc., based on a simple \"copy forward\" prediction baseline (predicting the next value as the previous one). These statistics quantify the inherent predictability/variability of each variable and are stored in the <code>self.variable_stats</code> DataFrame. This dataframe is used later for filtering variables and weighted sampling during split generation.</p> Source code in <code>twinweaver/instruction/data_splitter_forecasting.py</code> <pre><code>def setup_statistics(self, train_patientids: list = None):\n    \"\"\"\n    Calculates baseline performance statistics for variables.\n\n    Iterates through all patients in the training set and potential split\n    dates within specified ranges around Lines of Therapy (LoTs). For each\n    numeric variable (typically labs), it calculates metrics like R\u00b2, NRMSE,\n    MAPE, mean, standard deviation, etc., based on a simple \"copy forward\"\n    prediction baseline (predicting the next value as the previous one).\n    These statistics quantify the inherent predictability/variability of each\n    variable and are stored in the `self.variable_stats` DataFrame. This\n    dataframe is used later for filtering variables and weighted sampling\n    during split generation.\n    \"\"\"\n\n    events = self.dm.data_frames[self.config.event_table_name]\n\n    #: Assert that no event values are NaN\n    assert events[self.config.event_value_col].notna().all(), (\n        \"There are NaN values in event_value column of events table\"\n    )\n\n    #: setup forecasting variables\n    mask_cat = events[self.config.event_category_col].isin(self.config.event_category_forecast)\n    all_vars = events[mask_cat][self.config.event_name_col].unique()\n\n    #: setup all possible split dates by looping through all patients\n    all_possible_split_dates = []\n    relevant_events = events[\n        [\n            self.config.patient_id_col,\n            self.config.date_col,\n            self.config.event_category_col,\n            self.config.event_name_col,\n        ]\n    ].copy()\n    relevant_events = relevant_events.sort_values([self.config.patient_id_col, self.config.date_col])\n    grouped_events = relevant_events.groupby(self.config.patient_id_col)\n\n    for idx, (patientid, event_data) in enumerate(grouped_events):\n        if idx % 1000 == 0:\n            logging.info(f\"Processing patient ({idx + 1}/{len(self.dm.all_patientids)})\")\n        temp_patient_data = {\"events\": event_data}\n\n        temp_splits = self._get_all_dates_within_range_of_split_event(\n            temp_patient_data,\n            time_before_lot_start=self.max_lookback_time_for_value,\n            max_split_length_after_split_event=self.max_forecast_time_for_value,\n        )\n        temp_splits[self.config.patient_id_col] = patientid\n        temp_splits = temp_splits[[self.config.date_col, self.config.patient_id_col]]\n        all_possible_split_dates.append(temp_splits)\n        del temp_patient_data\n\n    all_possible_split_dates = pd.concat(all_possible_split_dates, axis=0, ignore_index=True)\n    all_possible_split_dates = all_possible_split_dates.drop_duplicates()\n\n    #: filter to only train patients\n    if train_patientids is not None:\n        all_train_patientids = train_patientids\n    else:\n        all_train_patientids = self.dm.get_all_patientids_in_split(self.config.train_split_name)\n    rows_to_select = all_possible_split_dates[self.config.patient_id_col].isin(all_train_patientids)\n    all_possible_split_dates = all_possible_split_dates[rows_to_select]\n\n    # Setup status\n    self.variable_stats = {}\n\n    #: loop through every forecasting variable\n    for idx, fore_var in enumerate(all_vars):\n        if idx % 20 == 0:\n            logging.info(f\"Processing forecasting variable {fore_var} ({idx + 1}/{len(all_vars)})\")\n\n        # Get corresponding events, sorted by date and patientid\n        curr_events = (\n            events[events[self.config.event_name_col] == fore_var]\n            .copy()\n            .sort_values([self.config.patient_id_col, self.config.date_col])\n        )\n        descriptive_name = curr_events[self.config.event_descriptive_name_col].iloc[0]\n        curr_events = curr_events[\n            [\n                self.config.patient_id_col,\n                self.config.date_col,\n                self.config.event_value_col,\n            ]\n        ].drop_duplicates()\n\n        # : extract only those dates which are given in self._get_all_possible_split_dates\n        # by doing inner join with all_possible_split_dates\n        curr_events = curr_events.merge(\n            all_possible_split_dates,\n            on=[self.config.patient_id_col, self.config.date_col],\n            how=\"inner\",\n        )\n\n        if self.dm.variable_types.get(fore_var) == \"numeric\":\n            # Numeric path: try to parse as numeric\n            curr_events[self.config.event_value_col] = pd.to_numeric(\n                curr_events[self.config.event_value_col], errors=\"coerce\"\n            )\n            # Shift values by one for copy forward\n            curr_events[\"predicted_value\"] = curr_events.groupby(self.config.patient_id_col)[\n                self.config.event_value_col\n            ].shift(1)\n\n            # Drop rows where either true value or predicted_value is NaN (first value for each patient)\n            valid_events = curr_events.dropna(subset=[self.config.event_value_col, \"predicted_value\"])\n\n            # Need at least 2 samples for R^2, else, we should ignore the variable anyway\n            if valid_events.shape[0] &gt;= self.min_num_samples_for_statistics:\n                # Calculate R\u00b2 across all\n                r2 = r2_score(\n                    valid_events[self.config.event_value_col],\n                    valid_events[\"predicted_value\"],\n                )\n\n                # Calculate NRMSE\n                mse = np.mean((valid_events[self.config.event_value_col] - valid_events[\"predicted_value\"]) ** 2)\n                rmse = np.sqrt(mse)\n                nrmse = rmse / (valid_events[self.config.event_value_col].std())\n\n                # Calculate mape\n                mape = (\n                    np.mean(\n                        np.abs(\n                            (valid_events[self.config.event_value_col] - valid_events[\"predicted_value\"])\n                            / valid_events[self.config.event_value_col]\n                        )\n                    )\n                    * 100\n                )\n\n                # Calculate score\n                score_nrmse_n_samples = nrmse * valid_events.shape[0]\n                score_log_nrmse_n_samples = np.log2(score_nrmse_n_samples)\n\n                # Calculate buckets\n                _, bin_5_edges = pd.qcut(\n                    valid_events[self.config.event_value_col],\n                    q=5,\n                    retbins=True,\n                    labels=False,\n                    duplicates=\"drop\",\n                )\n\n                # Calculate mean and std after removing over 3 standard deviations\n                mean = valid_events[self.config.event_value_col].mean()\n                std = valid_events[self.config.event_value_col].std()\n                valid_events = valid_events.copy()\n\n                valid_events[self.config.event_value_col + \"_cleaned\"] = valid_events[\n                    self.config.event_value_col\n                ].apply(lambda x: x if (x &gt; mean - 3 * std) and (x &lt; mean + 3 * std) else np.nan)\n\n                mean_without_outliers = np.nanmean(valid_events[self.config.event_value_col + \"_cleaned\"].values)\n                std_without_outliers = np.nanstd(valid_events[self.config.event_value_col + \"_cleaned\"].values)\n\n                # Record\n                self.variable_stats[fore_var] = {\n                    \"event_descriptive_name\": descriptive_name,\n                    \"is_numeric\": True,\n                    \"r2\": r2,\n                    \"nrmse\": nrmse,\n                    \"mape\": mape,\n                    \"score_nrmse_n_samples\": score_nrmse_n_samples,\n                    \"score_log_nrmse_n_samples\": score_log_nrmse_n_samples,\n                    \"std\": std,\n                    \"mean\": mean,\n                    \"range\": valid_events[self.config.event_value_col].max()\n                    - valid_events[self.config.event_value_col].min(),\n                    \"num_samples\": valid_events.shape[0],\n                    \"5_equal_sized_bins\": bin_5_edges.tolist(),\n                    \"mean_without_outliers\": mean_without_outliers,\n                    \"std_without_outliers\": std_without_outliers,\n                }\n        else:\n            # Categorical path: keep as strings\n            curr_events[self.config.event_value_col] = curr_events[self.config.event_value_col].astype(str)\n\n            # Remove rows where event_value is missing\n            valid_events = curr_events.dropna(subset=[self.config.event_value_col])\n\n            # Build placeholder stats, no real metrics calculated\n            self.variable_stats[fore_var] = {\n                \"event_descriptive_name\": descriptive_name,\n                \"is_numeric\": False,\n                # Keep any numeric-score columns unused for cats\n                \"r2\": np.nan,\n                \"nrmse\": np.nan,\n                \"mape\": np.nan,\n                \"score_nrmse_n_samples\": np.nan,\n                \"score_log_nrmse_n_samples\": np.nan,\n                \"std\": np.nan,\n                \"mean\": np.nan,\n                \"range\": np.nan,\n                \"num_samples\": valid_events.shape[0],\n                \"5_equal_sized_bins\": [],\n                \"mean_without_outliers\": np.nan,\n                \"std_without_outliers\": np.nan,\n            }\n\n    #: turn into a pandas dataframe\n    self.variable_stats = pd.DataFrame(self.variable_stats).T\n    self.variable_stats = self.variable_stats.reset_index(drop=False, names=self.config.event_name_col)\n\n    # Print some statistics\n    logging.info(f\"Number of variables included in selection: {self.variable_stats.shape[0]}\")\n    logging.info(f\"Mean of score used for sampling: {self.variable_stats[self.sampling_score_to_use].mean()}\")\n    logging.info(f\"Std of score used for sampling: {self.variable_stats[self.sampling_score_to_use].std()}\")\n    logging.info(f\"Min of score used for sampling: {self.variable_stats[self.sampling_score_to_use].min()}\")\n    logging.info(f\"Max of score used for sampling: {self.variable_stats[self.sampling_score_to_use].max()}\")\n\n    assert self.variable_stats.shape[0] &gt; 0, (\n        \"Error - for some reason no variables have been included in the statistics table. Check your data &amp; setup.\"\n    )\n\n    # Save if requested\n    if self.save_path_for_variable_stats is not None:\n        self.variable_stats.to_csv(self.save_path_for_variable_stats)\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting.DataSplitterForecastingGroup","title":"DataSplitterForecastingGroup","text":"<p>Groups multiple forecasting split options together. Represents a collection of forecasting tasks for a patient, which can be e.g. randomly selected in converter_manual_instruction.</p> Source code in <code>twinweaver/instruction/data_splitter_forecasting.py</code> <pre><code>class DataSplitterForecastingGroup:\n    \"\"\"\n    Groups multiple forecasting split options together.\n    Represents a collection of forecasting tasks for a patient, which\n    can be e.g. randomly selected in converter_manual_instruction.\n    \"\"\"\n\n    def __init__(\n        self,\n        forecasting_options: list[DataSplitterForecastingOption] = None,\n    ):\n        if forecasting_options is None:\n            forecasting_options = []\n        self.forecasting_options = forecasting_options\n\n    def append(self, option: DataSplitterForecastingOption):\n        self.forecasting_options.append(option)\n\n    def __len__(self):\n        return len(self.forecasting_options)\n\n    def __getitem__(self, index):\n        return self.forecasting_options[index]\n</code></pre>"},{"location":"reference/instruction/data_splitters/#twinweaver.instruction.data_splitter_forecasting.DataSplitterForecastingOption","title":"DataSplitterForecastingOption","text":"<p>Represents a single forecasting split option with associated data and parameters.</p> <p>Attributes:</p> Name Type Description <code>events_until_split</code> <code>list</code> <p>Events occurring until the split point.</p> <code>target_events_after_split</code> <code>list</code> <p>Target events occurring after the split point.</p> <code>constant_data</code> <code>dict</code> <p>Constant data related to the patient or context.</p> <code>split_date_included_in_input</code> <code>datetime</code> <p>The date included in the input split.</p> <code>sampled_variables</code> <code>list</code> <p>Variables sampled for forecasting.</p> <code>lot_date</code> <code>datetime</code> <p>The lot date associated with the split.</p> Source code in <code>twinweaver/instruction/data_splitter_forecasting.py</code> <pre><code>class DataSplitterForecastingOption:\n    \"\"\"\n    Represents a single forecasting split option with associated data and parameters.\n\n    Attributes\n    ----------\n    events_until_split : list\n        Events occurring until the split point.\n    target_events_after_split : list\n        Target events occurring after the split point.\n    constant_data : dict\n        Constant data related to the patient or context.\n    split_date_included_in_input : datetime\n        The date included in the input split.\n    sampled_variables : list\n        Variables sampled for forecasting.\n    lot_date : datetime\n        The lot date associated with the split.\n    \"\"\"\n\n    def __init__(\n        self,\n        events_until_split,\n        target_events_after_split,\n        constant_data,\n        split_date_included_in_input,\n        sampled_variables,\n        lot_date,\n    ):\n        self.events_until_split = events_until_split\n        self.target_events_after_split = target_events_after_split\n        self.constant_data = constant_data\n        self.split_date_included_in_input = split_date_included_in_input\n        self.sampled_variables = sampled_variables\n        self.lot_date = lot_date\n</code></pre>"},{"location":"reference/pretrain/converter_pretrain/","title":"Converter Pretrain","text":""},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain","title":"twinweaver.pretrain.converter_pretrain","text":""},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain-classes","title":"Classes","text":""},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain.ConverterPretrain","title":"ConverterPretrain","text":"<p>               Bases: <code>ConverterBase</code></p> <p>Implements bidirectional conversion between structured patient data and a textual representation.</p> <p>This class provides the core logic for transforming pandas DataFrames containing patient events and constant information into a human-readable text format (<code>forward_conversion</code>) format (<code>forward_conversion</code>) and parsing this text format back into DataFrames (<code>reverse_conversion</code>). It inherits base functionalities and configuration handling from <code>ConverterBase</code>.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Config</code> <p>Configuration object storing settings like column names, date formats, etc.</p> <code>preamble_text</code> <code>str</code> <p>Inherited/configured text added at the beginning of the output string.</p> <code>constant_text</code> <code>str</code> <p>Inherited/configured text marking the start of the constant data section.</p> <code>first_day_text</code> <code>str</code> <p>Inherited/configured text marking the start of the event data section.</p> <code>constant_description</code> <code>DataFrame</code> <p>DataFrame describing the columns in the constant DataFrame.</p> Source code in <code>twinweaver/pretrain/converter_pretrain.py</code> <pre><code>class ConverterPretrain(ConverterBase):\n    \"\"\"\n    Implements bidirectional conversion between structured patient data and a textual representation.\n\n    This class provides the core logic for transforming pandas DataFrames containing patient\n    events and constant information into a human-readable text format (`forward_conversion`)\n    format (`forward_conversion`) and parsing this text format back into DataFrames (`reverse_conversion`). It inherits\n    base functionalities and configuration handling from `ConverterBase`.\n\n    Attributes\n    ----------\n    config : Config\n        Configuration object storing settings like column names, date formats, etc.\n    preamble_text : str\n        Inherited/configured text added at the beginning of the output string.\n    constant_text : str\n        Inherited/configured text marking the start of the constant data section.\n    first_day_text : str\n        Inherited/configured text marking the start of the event data section.\n    constant_description : pd.DataFrame\n        DataFrame describing the columns in the constant DataFrame.\n        # Other attributes related to formatting and separators inherited from base class.\n    \"\"\"\n\n    def __init__(self, config: Config, dm: DataManager) -&gt; None:\n        \"\"\"\n        Initializes the ConverterPretrain instance.\n\n        Sets up the converter using the provided configuration object, primarily by\n        calling the initializer of the base class `ConverterBase`.\n\n        Parameters\n        ----------\n        config : Config\n            A configuration object containing necessary settings for data processing,\n            such as standard column names, text separators, and formatting details.\n        dm : DataManager\n            A DataManager object containing the data frames, e.g. constant_description.\n        \"\"\"\n        super().__init__(config)\n        self.constant_description = dm.data_frames[\"constant_description\"]\n\n    def forward_conversion(self, events: pd.DataFrame, constant: pd.DataFrame) -&gt; dict:\n        \"\"\"\n        Converts structured patient data (events, constant info) into a textual representation.\n\n        This method takes patient data as DataFrames, preprocesses them (e.g., calculating age\n        from birthdate), formats the constant information and time-series events into predefined\n        textual structures, and combines them into a single string. It returns the generated text\n        along with metadata containing both the original and processed DataFrames.\n\n        Parameters\n        ----------\n        events : pd.DataFrame\n            DataFrame containing the time-series event data for the patient. Expected columns\n            are defined in the `config` object (e.g., date, category, name, value).\n        constant : pd.DataFrame\n            DataFrame containing constant (non-time-varying) information for the patient.\n            Expected to have a single row. Columns represent different attributes.\n\n        Returns\n        -------\n        dict\n            A dictionary containing the conversion results:\n            {\n                \"text\": str,  # The full textual representation of the patient's data.\n                \"meta\": {\n                    \"raw_constant\": pd.DataFrame,  # Original input constant DataFrame.\n                    \"processed_constant\": pd.DataFrame,  # Constant DataFrame after preprocessing.\n                    \"raw_events\": pd.DataFrame,  # Original input events DataFrame.\n                    \"events\": pd.DataFrame,  # Events DataFrame after preprocessing.\n                    \"constant_description\": pd.DataFrame  # The constant description used.\n                }\n            }\n        \"\"\"\n        # Run assertions on input data\n        self._run_input_assertions(events, constant, self.constant_description)\n\n        # NOTE: most functions here are coming from the base class\n        #: preprocess constant data into appropriate format (e.g. birthyear to age)\n        raw_constant = constant.copy()\n        raw_events = events.copy()\n        constant, constant_description = self._preprocess_constant_date(events, constant, self.constant_description)\n\n        #: add in preamble\n        master_string = self.preamble_text\n\n        #: add in constant\n        constant_string = self._get_constant_string(constant, constant_description)\n        master_string += constant_string\n\n        # preprocess events\n        events = self._preprocess_events(events)\n\n        #: Convert events\n        events_string = self._get_event_string(events)\n        master_string += events_string\n\n        #: setup return dictionary with meta data\n        ret_dict = {\n            \"text\": master_string,\n            \"meta\": {\n                \"raw_constant\": raw_constant,\n                \"processed_constant\": constant.copy(),\n                \"raw_events\": raw_events,\n                \"events\": events.copy(),\n                \"constant_description\": constant_description.copy(),\n            },\n        }\n\n        #: return\n        return ret_dict\n\n    def reverse_conversion(self, text: str, \n                           data_manager : DataManager,\n                           init_date: datetime) -&gt; dict:\n        \"\"\"\n        Converts a textual representation of patient data back into structured DataFrames.\n\n        Parses the input text string, attempting to extract constant patient information and\n        time-series events based on the formatting conventions used by `forward_conversion`.\n        Uses helper methods (`_extract_constant_data`, `_extract_event_data`) and metadata\n        (like constant descriptions and original event structure hints) to reconstruct the\n        DataFrames.\n\n        Parameters\n        ----------\n        text : str\n            The textual representation of patient data, as generated by `forward_conversion`.\n        data_manager : DataManager\n            DataManager object containing necessary metadata for reconstruction,\n            including `constant_description` and `unique_events`.\n        init_date : datetime\n            The initial date to use as a reference point for reconstructing event dates.\n        Returns\n        -------\n        dict\n            A dictionary containing the reconstructed data:\n            {\n                \"constant\": pd.DataFrame,  # DataFrame of the extracted constant information.\n                \"events\": pd.DataFrame     # DataFrame of the extracted event data.\n            }\n        \"\"\"\n        # Extract constant data\n        constant_data = self._extract_constant_data(text, data_manager.data_frames[\"constant_description\"])\n\n        # Extract event data\n        event_data = self._extract_event_data(text=text, unique_events=data_manager.unique_events, init_date=init_date)\n\n        # Combine constant and event data\n        ret_dict = {\"constant\": constant_data, \"events\": event_data}\n\n        return ret_dict\n\n    def _run_input_assertions(\n        self,\n        events: pd.DataFrame,\n        constant: pd.DataFrame,\n        constant_description: pd.DataFrame,\n    ) -&gt; None:\n        \"\"\"\n        Validates the types and basic structure of input DataFrames for `forward_conversion`.\n\n        Checks if the inputs `events`, `constant`, and `constant_description` are pandas\n        DataFrames. Verifies that `constant` has exactly one row. Checks if `events`\n        (if not empty) and `constant_description` contain essential columns as expected\n        (based on `self.config` for events, and fixed names like 'variable', 'comment'\n        for `constant_description`). Raises an AssertionError if any validation fails.\n\n        Parameters\n        ----------\n        events : pd.DataFrame\n            The events DataFrame to validate.\n        constant : pd.DataFrame\n            The constant DataFrame to validate.\n        constant_description : pd.DataFrame\n            The constant description DataFrame to validate.\n\n        Raises\n        ------\n        AssertionError\n            If any of the input validation checks fail.\n        \"\"\"\n\n        assert isinstance(events, pd.DataFrame), f\"Input 'events' must be a pandas DataFrame, but got {type(events)}\"\n        assert isinstance(constant, pd.DataFrame), (\n            f\"Input 'constant' must be a pandas DataFrame, but got {type(constant)}\"\n        )\n        assert isinstance(constant_description, pd.DataFrame), (\n            f\"Input 'constant_description' must be a pandas DataFrame, but got {type(constant_description)}\"\n        )\n\n        # Check constant DataFrame structure (should represent one patient)\n        assert constant.shape[0] == 1, (\n            f\"Input 'constant' DataFrame should have exactly one row, but found {constant.shape[0]} rows.\"\n        )\n        # Optional: Check for a patient ID column using config\n        # assert self.config.patient_id_col in constant.columns, f\"Input 'constant' DataFrame must contain the patient\n        # ID column: '{self.config.patient_id_col}'.\"\n\n        # Check events DataFrame structure using self.config (allow empty events, but check columns if not empty)\n        required_event_cols = [\n            self.config.date_col,\n            self.config.event_category_col,\n            self.config.event_name_col,\n            self.config.event_value_col,\n        ]  # Modify if other columns from config are essential\n        if not events.empty:\n            missing_event_cols = [col for col in required_event_cols if col not in events.columns]\n            assert not missing_event_cols, (\n                f\"Input 'events' DataFrame is missing required columns defined in config: {missing_event_cols}\"\n            )\n        # Optional: Check if date column is datetime type\n        # if not events.empty:\n        #    assert pd.api.types.is_datetime64_any_dtype(events[self.config.date_col]),\n        # f\"Column '{self.config.date_col}' in 'events' should be a datetime type.\"\n\n        # Check constant_description DataFrame structure (columns 'variable' and 'comment' are likely fixed structural\n        # names, not from config)\n        required_const_desc_cols = [\n            \"variable\",\n            \"comment\",\n        ]  # These describe the constant data structure itself\n        missing_const_desc_cols = [col for col in required_const_desc_cols if col not in constant_description.columns]\n        assert not missing_const_desc_cols, (\n            f\"Input 'constant_description' DataFrame is missing required columns: {missing_const_desc_cols}\"\n        )\n        assert not constant_description.empty, \"Input 'constant_description' DataFrame cannot be empty.\"\n        # --- End Input Assertions ---\n\n    def _extract_constant_data(self, text: str, constant_description: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Extracts constant patient data from its textual representation.\n\n        Uses regular expressions to locate the section of the text containing constant\n        information (between `self.constant_text` and `self.first_day_text`). It then\n        iterates through lines in this section, matching them against the descriptions\n        in `constant_description` to identify variable names and extract their values.\n\n        Parameters\n        ----------\n        text : str\n            The full textual representation of the patient data.\n        constant_description : pd.DataFrame\n            DataFrame describing the constant variables, mapping descriptions ('comment')\n            back to variable names ('variable').\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the extracted constant data, with one row and columns\n            corresponding to the identified variables. Returns an empty DataFrame or one\n            with missing values if extraction fails for some variables.\n        \"\"\"\n\n        constant_data = {}\n\n        # Extract constant section\n        constant_section = re.search(\n            re.escape(self.constant_text) + r\"(.*?)\" + re.escape(self.first_day_text),\n            text,\n            re.DOTALL,\n        )\n        if constant_section:\n            constant_section = constant_section.group(1).strip()\n\n            # Split by lines and process each line\n            for line in constant_section.split(\",\\n\"):\n                line = line.strip()\n                if line:\n                    for _, row in constant_description.iterrows():\n                        if row[\"comment\"] in line:\n                            value = line.split(\" is \")[1].strip()\n                            constant_data[row[\"variable\"]] = value\n                            break\n\n        # Convert to DataFrame\n        constant_df = pd.DataFrame([constant_data])\n\n        return constant_df\n</code></pre>"},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain.ConverterPretrain--other-attributes-related-to-formatting-and-separators-inherited-from-base-class","title":"Other attributes related to formatting and separators inherited from base class.","text":""},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain.ConverterPretrain-functions","title":"Functions","text":""},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain.ConverterPretrain.__init__","title":"__init__","text":"<pre><code>__init__(config, dm)\n</code></pre> <p>Initializes the ConverterPretrain instance.</p> <p>Sets up the converter using the provided configuration object, primarily by calling the initializer of the base class <code>ConverterBase</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>A configuration object containing necessary settings for data processing, such as standard column names, text separators, and formatting details.</p> required <code>dm</code> <code>DataManager</code> <p>A DataManager object containing the data frames, e.g. constant_description.</p> required Source code in <code>twinweaver/pretrain/converter_pretrain.py</code> <pre><code>def __init__(self, config: Config, dm: DataManager) -&gt; None:\n    \"\"\"\n    Initializes the ConverterPretrain instance.\n\n    Sets up the converter using the provided configuration object, primarily by\n    calling the initializer of the base class `ConverterBase`.\n\n    Parameters\n    ----------\n    config : Config\n        A configuration object containing necessary settings for data processing,\n        such as standard column names, text separators, and formatting details.\n    dm : DataManager\n        A DataManager object containing the data frames, e.g. constant_description.\n    \"\"\"\n    super().__init__(config)\n    self.constant_description = dm.data_frames[\"constant_description\"]\n</code></pre>"},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain.ConverterPretrain.forward_conversion","title":"forward_conversion","text":"<pre><code>forward_conversion(events, constant)\n</code></pre> <p>Converts structured patient data (events, constant info) into a textual representation.</p> <p>This method takes patient data as DataFrames, preprocesses them (e.g., calculating age from birthdate), formats the constant information and time-series events into predefined textual structures, and combines them into a single string. It returns the generated text along with metadata containing both the original and processed DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>DataFrame containing the time-series event data for the patient. Expected columns are defined in the <code>config</code> object (e.g., date, category, name, value).</p> required <code>constant</code> <code>DataFrame</code> <p>DataFrame containing constant (non-time-varying) information for the patient. Expected to have a single row. Columns represent different attributes.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the conversion results: {     \"text\": str,  # The full textual representation of the patient's data.     \"meta\": {         \"raw_constant\": pd.DataFrame,  # Original input constant DataFrame.         \"processed_constant\": pd.DataFrame,  # Constant DataFrame after preprocessing.         \"raw_events\": pd.DataFrame,  # Original input events DataFrame.         \"events\": pd.DataFrame,  # Events DataFrame after preprocessing.         \"constant_description\": pd.DataFrame  # The constant description used.     } }</p> Source code in <code>twinweaver/pretrain/converter_pretrain.py</code> <pre><code>def forward_conversion(self, events: pd.DataFrame, constant: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Converts structured patient data (events, constant info) into a textual representation.\n\n    This method takes patient data as DataFrames, preprocesses them (e.g., calculating age\n    from birthdate), formats the constant information and time-series events into predefined\n    textual structures, and combines them into a single string. It returns the generated text\n    along with metadata containing both the original and processed DataFrames.\n\n    Parameters\n    ----------\n    events : pd.DataFrame\n        DataFrame containing the time-series event data for the patient. Expected columns\n        are defined in the `config` object (e.g., date, category, name, value).\n    constant : pd.DataFrame\n        DataFrame containing constant (non-time-varying) information for the patient.\n        Expected to have a single row. Columns represent different attributes.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the conversion results:\n        {\n            \"text\": str,  # The full textual representation of the patient's data.\n            \"meta\": {\n                \"raw_constant\": pd.DataFrame,  # Original input constant DataFrame.\n                \"processed_constant\": pd.DataFrame,  # Constant DataFrame after preprocessing.\n                \"raw_events\": pd.DataFrame,  # Original input events DataFrame.\n                \"events\": pd.DataFrame,  # Events DataFrame after preprocessing.\n                \"constant_description\": pd.DataFrame  # The constant description used.\n            }\n        }\n    \"\"\"\n    # Run assertions on input data\n    self._run_input_assertions(events, constant, self.constant_description)\n\n    # NOTE: most functions here are coming from the base class\n    #: preprocess constant data into appropriate format (e.g. birthyear to age)\n    raw_constant = constant.copy()\n    raw_events = events.copy()\n    constant, constant_description = self._preprocess_constant_date(events, constant, self.constant_description)\n\n    #: add in preamble\n    master_string = self.preamble_text\n\n    #: add in constant\n    constant_string = self._get_constant_string(constant, constant_description)\n    master_string += constant_string\n\n    # preprocess events\n    events = self._preprocess_events(events)\n\n    #: Convert events\n    events_string = self._get_event_string(events)\n    master_string += events_string\n\n    #: setup return dictionary with meta data\n    ret_dict = {\n        \"text\": master_string,\n        \"meta\": {\n            \"raw_constant\": raw_constant,\n            \"processed_constant\": constant.copy(),\n            \"raw_events\": raw_events,\n            \"events\": events.copy(),\n            \"constant_description\": constant_description.copy(),\n        },\n    }\n\n    #: return\n    return ret_dict\n</code></pre>"},{"location":"reference/pretrain/converter_pretrain/#twinweaver.pretrain.converter_pretrain.ConverterPretrain.reverse_conversion","title":"reverse_conversion","text":"<pre><code>reverse_conversion(text, data_manager, init_date)\n</code></pre> <p>Converts a textual representation of patient data back into structured DataFrames.</p> <p>Parses the input text string, attempting to extract constant patient information and time-series events based on the formatting conventions used by <code>forward_conversion</code>. Uses helper methods (<code>_extract_constant_data</code>, <code>_extract_event_data</code>) and metadata (like constant descriptions and original event structure hints) to reconstruct the DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The textual representation of patient data, as generated by <code>forward_conversion</code>.</p> required <code>data_manager</code> <code>DataManager</code> <p>DataManager object containing necessary metadata for reconstruction, including <code>constant_description</code> and <code>unique_events</code>.</p> required <code>init_date</code> <code>datetime</code> <p>The initial date to use as a reference point for reconstructing event dates.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the reconstructed data: {     \"constant\": pd.DataFrame,  # DataFrame of the extracted constant information.     \"events\": pd.DataFrame     # DataFrame of the extracted event data. }</p> Source code in <code>twinweaver/pretrain/converter_pretrain.py</code> <pre><code>def reverse_conversion(self, text: str, \n                       data_manager : DataManager,\n                       init_date: datetime) -&gt; dict:\n    \"\"\"\n    Converts a textual representation of patient data back into structured DataFrames.\n\n    Parses the input text string, attempting to extract constant patient information and\n    time-series events based on the formatting conventions used by `forward_conversion`.\n    Uses helper methods (`_extract_constant_data`, `_extract_event_data`) and metadata\n    (like constant descriptions and original event structure hints) to reconstruct the\n    DataFrames.\n\n    Parameters\n    ----------\n    text : str\n        The textual representation of patient data, as generated by `forward_conversion`.\n    data_manager : DataManager\n        DataManager object containing necessary metadata for reconstruction,\n        including `constant_description` and `unique_events`.\n    init_date : datetime\n        The initial date to use as a reference point for reconstructing event dates.\n    Returns\n    -------\n    dict\n        A dictionary containing the reconstructed data:\n        {\n            \"constant\": pd.DataFrame,  # DataFrame of the extracted constant information.\n            \"events\": pd.DataFrame     # DataFrame of the extracted event data.\n        }\n    \"\"\"\n    # Extract constant data\n    constant_data = self._extract_constant_data(text, data_manager.data_frames[\"constant_description\"])\n\n    # Extract event data\n    event_data = self._extract_event_data(text=text, unique_events=data_manager.unique_events, init_date=init_date)\n\n    # Combine constant and event data\n    ret_dict = {\"constant\": constant_data, \"events\": event_data}\n\n    return ret_dict\n</code></pre>"},{"location":"reference/utils/meds_importer/","title":"MEDS Importer","text":""},{"location":"reference/utils/meds_importer/#twinweaver.utils.meds_importer","title":"twinweaver.utils.meds_importer","text":""},{"location":"reference/utils/meds_importer/#twinweaver.utils.meds_importer-functions","title":"Functions","text":""},{"location":"reference/utils/meds_importer/#twinweaver.utils.meds_importer.convert_meds_to_dtc","title":"convert_meds_to_dtc","text":"<pre><code>convert_meds_to_dtc(\n    df_codes,\n    df_data,\n    df_split,\n    prefer_text_value_over_numeric=True,\n    no_value_default=\"occurred\",\n    event_category_mapping=None,\n    default_category=\"generic_event\",\n)\n</code></pre> <p>Converts raw medical data into a format compatible with the digital_twin_converter package.</p> <p>This function takes multiple DataFrames containing patient data, medical codes, and patient splits, and transforms them into three distinct DataFrames: one for static (constant) patient data, one for descriptions of that static data, and one for time-series event data.</p> <p>The function separates events with timestamps from those without. Events without timestamps are treated as static patient characteristics and are pivoted into a wide-format DataFrame (<code>converted_constant</code>). Events with timestamps are formatted into a long-format DataFrame (<code>converted_events</code>).</p> <p>Parameters:</p> Name Type Description Default <code>df_codes</code> <code>DataFrame</code> <p>DataFrame containing the mapping from medical codes to their descriptions. Must contain 'code' and 'description' columns.</p> required <code>df_data</code> <code>DataFrame</code> <p>The primary DataFrame with event data for each patient. Must include 'subject_id', 'code', 'time', 'text_value', and 'numeric_value' columns.</p> required <code>df_split</code> <code>DataFrame</code> <p>DataFrame containing static patient information, such as train/test split assignments. Must include a 'subject_id' column.</p> required <code>prefer_text_value_over_numeric</code> <code>bool</code> <p>If True, when an event has both a text and a numeric value, the text value is used. If False, the numeric value is prioritized. Defaults to True.</p> <code>True</code> <code>no_value_default</code> <code>str</code> <p>The default value to assign to an event's 'event_value' if it has a timestamp but no associated text or numeric value. Defaults to \"occurred\".</p> <code>'occurred'</code> <code>event_category_mapping</code> <code>dict</code> <p>A dictionary to map event codes ('event_name') to event categories. If None, no specific category mapping is applied. Defaults to None.</p> <code>None</code> <code>default_category</code> <code>str</code> <p>The category to assign to events that are not found in the <code>event_category_mapping</code>. Defaults to \"generic_event\".</p> <code>'generic_event'</code> <p>Returns:</p> Name Type Description <code>converted_constant</code> <code>DataFrame</code> <p>A DataFrame with one row per patient, containing static features. This includes data from <code>df_split</code> and pivoted data from <code>df_data</code> for events that had no timestamp.</p> <code>converted_constant_description</code> <code>DataFrame</code> <p>A DataFrame providing human-readable descriptions for each column in the <code>converted_constant</code> DataFrame.</p> <code>converted_events</code> <code>DataFrame</code> <p>A long-format DataFrame containing all time-stamped events, structured for time-series analysis. Includes columns like 'patientid', 'date', 'event_name', 'event_value', and 'event_category'.</p> <p>Warns:</p> Type Description <code>Warning</code> <p>Prints a warning to the console if duplicate rows are detected in the final <code>converted_events</code> DataFrame. Duplicates are checked based on the combination of \"patientid\", \"date\", \"event_name\", and \"event_value\".</p> Source code in <code>twinweaver/utils/meds_importer.py</code> <pre><code>def convert_meds_to_dtc(\n    df_codes,\n    df_data,\n    df_split,\n    prefer_text_value_over_numeric=True,\n    no_value_default=\"occurred\",\n    event_category_mapping=None,\n    default_category=\"generic_event\",\n):\n    \"\"\"Converts raw medical data into a format compatible with the digital_twin_converter package.\n\n    This function takes multiple DataFrames containing patient data, medical codes,\n    and patient splits, and transforms them into three distinct DataFrames:\n    one for static (constant) patient data, one for descriptions of that static\n    data, and one for time-series event data.\n\n    The function separates events with timestamps from those without. Events\n    without timestamps are treated as static patient characteristics and are\n    pivoted into a wide-format DataFrame (`converted_constant`). Events with\n    timestamps are formatted into a long-format DataFrame (`converted_events`).\n\n    Parameters\n    ----------\n    df_codes : pd.DataFrame\n        DataFrame containing the mapping from medical codes to their descriptions.\n        Must contain 'code' and 'description' columns.\n    df_data : pd.DataFrame\n        The primary DataFrame with event data for each patient. Must include\n        'subject_id', 'code', 'time', 'text_value', and 'numeric_value' columns.\n    df_split : pd.DataFrame\n        DataFrame containing static patient information, such as train/test\n        split assignments. Must include a 'subject_id' column.\n    prefer_text_value_over_numeric : bool, optional\n        If True, when an event has both a text and a numeric value, the text\n        value is used. If False, the numeric value is prioritized.\n        Defaults to True.\n    no_value_default : str, optional\n        The default value to assign to an event's 'event_value' if it has\n        a timestamp but no associated text or numeric value. Defaults to \"occurred\".\n    event_category_mapping : dict, optional\n        A dictionary to map event codes ('event_name') to event categories.\n        If None, no specific category mapping is applied. Defaults to None.\n    default_category : str, optional\n        The category to assign to events that are not found in the\n        `event_category_mapping`. Defaults to \"generic_event\".\n\n    Returns\n    -------\n    converted_constant : pd.DataFrame\n        A DataFrame with one row per patient, containing static features. This\n        includes data from `df_split` and pivoted data from `df_data` for\n        events that had no timestamp.\n    converted_constant_description : pd.DataFrame\n        A DataFrame providing human-readable descriptions for each column in\n        the `converted_constant` DataFrame.\n    converted_events : pd.DataFrame\n        A long-format DataFrame containing all time-stamped events, structured\n        for time-series analysis. Includes columns like 'patientid', 'date',\n        'event_name', 'event_value', and 'event_category'.\n\n    Warns\n    -----\n    Warning\n        Prints a warning to the console if duplicate rows are detected in the\n        final `converted_events` DataFrame. Duplicates are checked based on\n        the combination of \"patientid\", \"date\", \"event_name\", and \"event_value\".\n\n    \"\"\"\n\n    #: Set all subject_id to strings\n    df_data[\"subject_id\"] = df_data[\"subject_id\"].astype(str)\n    df_split[\"subject_id\"] = df_split[\"subject_id\"].astype(str)\n\n    #: assert all actually used codes in df_data are in df_codes and have a non-NA description\n    all_used_codes = df_data[\"code\"]\n    assert all_used_codes.isin(df_codes[\"code\"]).all(), \"Not all used codes are present in df_codes\"\n    assert df_codes[df_codes[\"code\"].isin(all_used_codes)][\"description\"].notna().all(), (\n        \"Not all used codes have a description in df_codes\"\n    )\n\n    #: put all events in df_data with no time into constant\n    no_time_events = df_data[df_data[\"time\"].isna()].copy()\n    if prefer_text_value_over_numeric:\n        no_time_events[\"event_value\"] = no_time_events[\"text_value\"]\n        no_time_events[\"event_value\"] = no_time_events[\"event_value\"].fillna(no_time_events[\"numeric_value\"])\n    else:\n        no_time_events[\"event_value\"] = no_time_events[\"numeric_value\"]\n        no_time_events[\"event_value\"] = no_time_events[\"event_value\"].fillna(no_time_events[\"text_value\"])\n\n    no_time_events = no_time_events[[\"subject_id\", \"code\", \"event_value\"]].drop_duplicates()\n    no_time_events = no_time_events.pivot(index=\"subject_id\", columns=\"code\", values=\"event_value\")\n    converted_constant = no_time_events.reset_index()\n\n    #: put split data into constant\n    converted_constant = converted_constant.merge(df_split, on=\"subject_id\", how=\"left\")\n    converted_constant = converted_constant.rename(columns={\"subject_id\": \"patientid\"})\n\n    #: generate corresponding constant_description_mapping file, which maps\n    # for every column in converted_constant the description of the code\n    converted_constant_description = []\n    for column in converted_constant.columns:\n        if column in df_codes[\"code\"].values:\n            description = df_codes[df_codes[\"code\"] == column][\"description\"].values[0]\n            converted_constant_description.append({\"variable\": column, \"comment\": description})\n        else:\n            converted_constant_description.append({\"variable\": column, \"comment\": \"No description available\"})\n    converted_constant_description = pd.DataFrame(converted_constant_description)\n\n    #: create first general events file\n    converted_events = df_data[df_data[\"time\"].notna()].copy()\n    converted_events = converted_events.rename(\n        columns={\"subject_id\": \"patientid\", \"time\": \"date\", \"code\": \"event_name\"}\n    )\n    converted_events[\"event_value\"] = None  # Placeholder for event_value\n\n    #: based on prefer_text_value_over_numeric, assign event_value\n    if prefer_text_value_over_numeric:\n        converted_events[\"event_value\"] = converted_events[\"text_value\"]\n        converted_events[\"event_value\"] = converted_events[\"event_value\"].fillna(converted_events[\"numeric_value\"])\n    else:\n        converted_events[\"event_value\"] = converted_events[\"numeric_value\"]\n        converted_events[\"event_value\"] = converted_events[\"event_value\"].fillna(converted_events[\"text_value\"])\n    converted_events = converted_events.drop(columns=[\"text_value\", \"numeric_value\"])\n\n    #: fill in event_value with no_value_default if no value is present\n    converted_events[\"event_value\"] = converted_events[\"event_value\"].fillna(no_value_default)\n\n    #: apply event_category_mapping to df_data if provided\n    if event_category_mapping is not None:\n        converted_events[\"event_category\"] = converted_events[\"event_name\"].map(event_category_mapping)\n    else:\n        converted_events[\"event_category\"] = pd.NA\n\n    #: apply default_category to all events without a category\n    converted_events[\"event_category\"] = converted_events[\"event_category\"].fillna(default_category)\n\n    #: add in event_descriptive_name as a merge of event_name and df_codes\n    converted_events = converted_events.merge(\n        df_codes[[\"code\", \"description\"]],\n        left_on=\"event_name\",\n        right_on=\"code\",\n        how=\"left\",\n    )\n    converted_events = converted_events.rename(columns={\"description\": \"event_descriptive_name\"})\n    converted_events = converted_events.drop(columns=[\"code\"])\n\n    #: add in empty meta_data column\n    converted_events[\"meta_data\"] = pd.NA\n\n    # Concert events to string\n    converted_events[\"event_value\"] = converted_events[\"event_value\"].astype(str)\n\n    #: issue warning if duplicates are present\n    all_duplicate_rows = converted_events[\n        converted_events.duplicated(subset=[\"patientid\", \"date\", \"event_name\", \"event_value\"], keep=False)\n    ]\n    if not all_duplicate_rows.empty:\n        print(\"Warning: Duplicates found in converted events data. Please make sure to handle them appropriately!\")\n        print(all_duplicate_rows)\n\n    #: retrun constant, constant_description, events\n    return converted_constant, converted_constant_description, converted_events\n</code></pre>"},{"location":"reference/utils/preprocessing_helpers/","title":"Preprocessing Helpers","text":""},{"location":"reference/utils/preprocessing_helpers/#twinweaver.utils.preprocessing_helpers","title":"twinweaver.utils.preprocessing_helpers","text":""},{"location":"reference/utils/preprocessing_helpers/#twinweaver.utils.preprocessing_helpers-functions","title":"Functions","text":""},{"location":"reference/utils/preprocessing_helpers/#twinweaver.utils.preprocessing_helpers.aggregate_events_to_weeks","title":"aggregate_events_to_weeks","text":"<pre><code>aggregate_events_to_weeks(\n    df,\n    patientid_column=\"patientid\",\n    date_column=\"date\",\n    event_name_column=\"event_name\",\n    event_value_column=\"event_value\",\n    random_state=None,\n)\n</code></pre> <p>Aggregates a long-format events DataFrame to rounded weeks relative to each patient's first visit.</p> <p>This function rounds event dates to the nearest week (relative to each patient's first visit date), then aggregates multiple events that fall within the same week. For identical events (same <code>event_name</code>) within the same week: - Numerical values are averaged. - Categorical values use the mode (most frequent value), with random selection   as a tiebreaker if multiple modes exist.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing longitudinal patient events in TwinWeaver format. Expected columns: patientid, date, event_category, event_name, event_value, event_descriptive_name, meta_data, source.</p> required <code>patientid_column</code> <code>str</code> <p>The name of the column containing patient identifiers.</p> <code>\"patientid\"</code> <code>date_column</code> <code>str</code> <p>The name of the column containing date/time information.</p> <code>\"date\"</code> <code>event_name_column</code> <code>str</code> <p>The name of the column containing event names (used to identify identical events).</p> <code>\"event_name\"</code> <code>event_value_column</code> <code>str</code> <p>The name of the column containing event values to aggregate.</p> <code>\"event_value\"</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility when breaking ties in mode selection. If None, results may vary for tied modes.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with dates rounded to weeks and events aggregated. The date column will contain dates representing the start of each week relative to the patient's first visit.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = {\n...     'patientid': ['p1', 'p1', 'p1', 'p1', 'p2', 'p2'],\n...     'date': ['2024-01-01', '2024-01-03', '2024-01-08', '2024-01-09',\n...              '2024-02-01', '2024-02-05'],\n...     'event_category': ['lab', 'lab', 'lab', 'lab', 'lab', 'lab'],\n...     'event_name': ['glucose', 'glucose', 'glucose', 'glucose', 'glucose', 'glucose'],\n...     'event_value': [100, 110, 120, 130, 90, 95],\n...     'event_descriptive_name': ['Glucose', 'Glucose', 'Glucose', 'Glucose',\n...                                 'Glucose', 'Glucose'],\n...     'meta_data': [None] * 6,\n...     'source': ['events'] * 6,\n... }\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; df['date'] = pd.to_datetime(df['date'])\n&gt;&gt;&gt; result = aggregate_events_to_weeks(df)\n&gt;&gt;&gt; # p1: Jan 1 and Jan 3 are in week 0 -&gt; averaged to 105\n&gt;&gt;&gt; #     Jan 8 and Jan 9 are in week 1 -&gt; averaged to 125\n&gt;&gt;&gt; # p2: Feb 1 and Feb 5 are in week 0 -&gt; averaged to 92.5\n</code></pre> Notes <ul> <li>Weeks are calculated as 7-day intervals from each patient's first visit.</li> <li>A date is assigned to week N if it falls within [first_visit + N7 days,   first_visit + (N+1)7 days).</li> <li>The output date for each week is the first day of that week interval.</li> <li>Non-grouping columns (like event_descriptive_name, meta_data, source) take   the first value within each aggregation group.</li> <li>Empty DataFrames are returned as-is.</li> </ul> Source code in <code>twinweaver/utils/preprocessing_helpers.py</code> <pre><code>def aggregate_events_to_weeks(\n    df: pd.DataFrame,\n    patientid_column: str = \"patientid\",\n    date_column: str = \"date\",\n    event_name_column: str = \"event_name\",\n    event_value_column: str = \"event_value\",\n    random_state: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregates a long-format events DataFrame to rounded weeks relative to each patient's first visit.\n\n    This function rounds event dates to the nearest week (relative to each patient's\n    first visit date), then aggregates multiple events that fall within the same week.\n    For identical events (same `event_name`) within the same week:\n    - Numerical values are averaged.\n    - Categorical values use the mode (most frequent value), with random selection\n      as a tiebreaker if multiple modes exist.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input DataFrame containing longitudinal patient events in TwinWeaver format.\n        Expected columns: patientid, date, event_category, event_name, event_value,\n        event_descriptive_name, meta_data, source.\n    patientid_column : str, default \"patientid\"\n        The name of the column containing patient identifiers.\n    date_column : str, default \"date\"\n        The name of the column containing date/time information.\n    event_name_column : str, default \"event_name\"\n        The name of the column containing event names (used to identify identical events).\n    event_value_column : str, default \"event_value\"\n        The name of the column containing event values to aggregate.\n    random_state : int, optional\n        Random seed for reproducibility when breaking ties in mode selection.\n        If None, results may vary for tied modes.\n\n    Returns\n    -------\n    pd.DataFrame\n        A new DataFrame with dates rounded to weeks and events aggregated.\n        The date column will contain dates representing the start of each week\n        relative to the patient's first visit.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = {\n    ...     'patientid': ['p1', 'p1', 'p1', 'p1', 'p2', 'p2'],\n    ...     'date': ['2024-01-01', '2024-01-03', '2024-01-08', '2024-01-09',\n    ...              '2024-02-01', '2024-02-05'],\n    ...     'event_category': ['lab', 'lab', 'lab', 'lab', 'lab', 'lab'],\n    ...     'event_name': ['glucose', 'glucose', 'glucose', 'glucose', 'glucose', 'glucose'],\n    ...     'event_value': [100, 110, 120, 130, 90, 95],\n    ...     'event_descriptive_name': ['Glucose', 'Glucose', 'Glucose', 'Glucose',\n    ...                                 'Glucose', 'Glucose'],\n    ...     'meta_data': [None] * 6,\n    ...     'source': ['events'] * 6,\n    ... }\n    &gt;&gt;&gt; df = pd.DataFrame(data)\n    &gt;&gt;&gt; df['date'] = pd.to_datetime(df['date'])\n    &gt;&gt;&gt; result = aggregate_events_to_weeks(df)\n    &gt;&gt;&gt; # p1: Jan 1 and Jan 3 are in week 0 -&gt; averaged to 105\n    &gt;&gt;&gt; #     Jan 8 and Jan 9 are in week 1 -&gt; averaged to 125\n    &gt;&gt;&gt; # p2: Feb 1 and Feb 5 are in week 0 -&gt; averaged to 92.5\n\n    Notes\n    -----\n    - Weeks are calculated as 7-day intervals from each patient's first visit.\n    - A date is assigned to week N if it falls within [first_visit + N*7 days,\n      first_visit + (N+1)*7 days).\n    - The output date for each week is the first day of that week interval.\n    - Non-grouping columns (like event_descriptive_name, meta_data, source) take\n      the first value within each aggregation group.\n    - Empty DataFrames are returned as-is.\n\n    \"\"\"\n    if df.empty:\n        return df.copy()\n\n    # Validate input columns exist\n    required_columns = [patientid_column, date_column, event_name_column, event_value_column]\n    for col in required_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Required column '{col}' not found in DataFrame\")\n\n    # Set random seed if provided\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    # Make a copy to avoid modifying the original\n    df = df.copy()\n\n    # Ensure date column is datetime\n    df[date_column] = pd.to_datetime(df[date_column])\n\n    # Calculate the first visit date for each patient\n    first_visits = df.groupby(patientid_column)[date_column].min().reset_index()\n    first_visits.columns = [patientid_column, \"_first_visit_date\"]\n\n    # Merge first visit dates\n    df = df.merge(first_visits, on=patientid_column, how=\"left\")\n\n    # Calculate days since first visit and round to weeks\n    df[\"_days_since_first\"] = (df[date_column] - df[\"_first_visit_date\"]).dt.days\n    df[\"_week_number\"] = (df[\"_days_since_first\"] / 7).apply(np.floor).astype(int)\n\n    # Calculate the rounded date (start of the week)\n    df[\"_rounded_date\"] = df[\"_first_visit_date\"] + pd.to_timedelta(df[\"_week_number\"] * 7, unit=\"D\")\n\n    # Identify grouping columns\n    grouping_cols = [patientid_column, \"_rounded_date\", event_name_column]\n\n    # Identify other columns in the dataframe\n    all_columns = df.columns.tolist()\n    temp_columns = [\"_first_visit_date\", \"_days_since_first\", \"_week_number\", \"_rounded_date\"]\n    other_columns = [\n        col for col in all_columns if col not in grouping_cols + [date_column, event_value_column] + temp_columns\n    ]\n\n    def aggregate_values(group: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Aggregate event values within a group.\"\"\"\n        values = group[event_value_column]\n\n        # Try to convert to numeric\n        numeric_values = pd.to_numeric(values, errors=\"coerce\")\n\n        if numeric_values.notna().all() and len(numeric_values) &gt; 0:\n            # All values are numeric - use mean\n            aggregated_value = numeric_values.mean()\n        else:\n            # Categorical - use mode\n            mode_result = values.mode()\n            if len(mode_result) == 0:\n                # All NaN\n                aggregated_value = values.iloc[0] if len(values) &gt; 0 else np.nan\n            elif len(mode_result) == 1:\n                aggregated_value = mode_result.iloc[0]\n            else:\n                # Multiple modes - random selection\n                aggregated_value = np.random.choice(mode_result.values)\n\n        # Build result series - grouping columns are available in the group\n        result = {\n            event_value_column: aggregated_value,\n        }\n\n        # Take first value for other columns (non-grouping, non-date, non-temp columns)\n        for col in other_columns:\n            if col in group.columns:\n                result[col] = group[col].iloc[0]\n\n        return pd.Series(result)\n\n    # Group and aggregate\n    aggregated = df.groupby(grouping_cols, as_index=False).apply(aggregate_values, include_groups=False)\n\n    # The groupby with as_index=False returns the grouping columns, rename _rounded_date to date_column\n    aggregated = aggregated.rename(columns={\"_rounded_date\": date_column})\n\n    # Ensure proper column order (original order where possible)\n    original_cols = [col for col in df.columns if col not in temp_columns]\n    final_cols = [col for col in original_cols if col in aggregated.columns]\n    aggregated = aggregated[final_cols]\n\n    # Sort by patient and date\n    aggregated = aggregated.sort_values([patientid_column, date_column]).reset_index(drop=True)\n\n    return aggregated\n</code></pre>"},{"location":"reference/utils/preprocessing_helpers/#twinweaver.utils.preprocessing_helpers.identify_constant_and_changing_columns","title":"identify_constant_and_changing_columns","text":"<pre><code>identify_constant_and_changing_columns(\n    df, date_column, patientid_column\n)\n</code></pre> <p>Identifies which columns remain constant and which change over time for each patient.</p> <p>This function analyzes a DataFrame to determine which columns have values that stay constant across all time points for each patient, and which columns have values that change over time for at least one patient.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing patient data with multiple time points.</p> required <code>date_column</code> <code>str</code> <p>The name of the column containing date/time information.</p> required <code>patientid_column</code> <code>str</code> <p>The name of the column containing patient identifiers.</p> required <p>Returns:</p> Name Type Description <code>constant_columns</code> <code>list of str</code> <p>A list of column names that remain constant across all time points for every patient.</p> <code>changing_columns</code> <code>list of str</code> <p>A list of column names that change over time for at least one patient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = {\n...     'patient_id': [1, 1, 2, 2],\n...     'date': ['2024-01-01', '2024-02-01', '2024-01-01', '2024-02-01'],\n...     'age': [30, 30, 45, 45],\n...     'weight': [70, 72, 80, 80],\n...     'gender': ['M', 'M', 'F', 'F']\n... }\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; constant, changing = identify_constant_and_changing_columns(\n...     df, date_column='date', patientid_column='patient_id'\n... )\n&gt;&gt;&gt; print(constant)\n['age', 'gender']\n&gt;&gt;&gt; print(changing)\n['weight']\n</code></pre> Notes <ul> <li>The date_column and patientid_column are excluded from the analysis.</li> <li>A column is considered constant if all values for a patient are identical   (including NaN values, which are treated as equal to each other).</li> <li>A column is considered changing if at least one patient has different   values across their time points.</li> </ul> Source code in <code>twinweaver/utils/preprocessing_helpers.py</code> <pre><code>def identify_constant_and_changing_columns(\n    df: pd.DataFrame,\n    date_column: str,\n    patientid_column: str,\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Identifies which columns remain constant and which change over time for each patient.\n\n    This function analyzes a DataFrame to determine which columns have values that\n    stay constant across all time points for each patient, and which columns have\n    values that change over time for at least one patient.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input DataFrame containing patient data with multiple time points.\n    date_column : str\n        The name of the column containing date/time information.\n    patientid_column : str\n        The name of the column containing patient identifiers.\n\n    Returns\n    -------\n    constant_columns : list of str\n        A list of column names that remain constant across all time points\n        for every patient.\n    changing_columns : list of str\n        A list of column names that change over time for at least one patient.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = {\n    ...     'patient_id': [1, 1, 2, 2],\n    ...     'date': ['2024-01-01', '2024-02-01', '2024-01-01', '2024-02-01'],\n    ...     'age': [30, 30, 45, 45],\n    ...     'weight': [70, 72, 80, 80],\n    ...     'gender': ['M', 'M', 'F', 'F']\n    ... }\n    &gt;&gt;&gt; df = pd.DataFrame(data)\n    &gt;&gt;&gt; constant, changing = identify_constant_and_changing_columns(\n    ...     df, date_column='date', patientid_column='patient_id'\n    ... )\n    &gt;&gt;&gt; print(constant)\n    ['age', 'gender']\n    &gt;&gt;&gt; print(changing)\n    ['weight']\n\n    Notes\n    -----\n    - The date_column and patientid_column are excluded from the analysis.\n    - A column is considered constant if all values for a patient are identical\n      (including NaN values, which are treated as equal to each other).\n    - A column is considered changing if at least one patient has different\n      values across their time points.\n\n    \"\"\"\n    # Validate input columns exist\n    if date_column not in df.columns:\n        raise ValueError(f\"Date column '{date_column}' not found in DataFrame\")\n    if patientid_column not in df.columns:\n        raise ValueError(f\"Patient ID column '{patientid_column}' not found in DataFrame\")\n\n    # Get columns to analyze (exclude date and patientid columns)\n    columns_to_analyze = [col for col in df.columns if col not in [date_column, patientid_column]]\n\n    constant_columns = []\n    changing_columns = []\n\n    for col in columns_to_analyze:\n        # Group by patient and check if values are constant within each patient\n        # Use nunique with dropna=False to count NaN as a distinct value\n        unique_values_per_patient = df.groupby(patientid_column)[col].nunique(dropna=False)\n\n        # A column changes if any patient has more than one unique value\n        if (unique_values_per_patient &gt; 1).any():\n            changing_columns.append(col)\n        else:\n            constant_columns.append(col)\n\n    return constant_columns, changing_columns\n</code></pre>"}]}